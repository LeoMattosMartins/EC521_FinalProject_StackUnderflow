[
  {
    "url": "https://stackoverflow.com/questions/65686318/sharing-python-objects-across-multiple-workers",
    "body": "# app_cache.py\nimport os\nfrom aiocache import Cache\nfrom fastapi import FastAPI, status\napp = FastAPI()\ncache = Cache(Cache.REDIS, endpoint=\"localhost\", port=6379, namespace=\"main\")\nclass Meta:\n    def __init__(self):\n        pass\n    async def get_count(self) -> int:\n        return await cache.get(\"count\", default=0)\n    async def set_count(self, value: int) -> None:\n        await cache.set(\"count\", value)\n    async def increment_count(self) -> None:\n        await cache.increment(\"count\", 1)\nmeta = Meta()\n# increases the count variable in the meta object by 1\n@app.post(\"/increment\")\nasync def increment():\n    await meta.increment_count()\n    return status.HTTP_200_OK\n# returns a json containing the current count from the meta object\n@app.get(\"/report\")\nasync def report():\n    count = await meta.get_count()\n    return {'count': count, \"current_process_id\": os.getpid()}\n# resets the count in the meta object to 0\n@app.post(\"/reset\")\nasync def reset():\n    await meta.set_count(0)\n    return status.HTTP_200_OK"
  },
  {
    "url": "https://stackoverflow.com/questions/65686318/sharing-python-objects-across-multiple-workers",
    "body": "# app_db.py\nfrom fastapi import FastAPI, status\nfrom tortoise import Model, fields\nfrom tortoise.contrib.fastapi import register_tortoise\nclass MetaModel(Model):\n    count = fields.IntField(default=0)\napp = FastAPI()\n# increases the count variable in the meta object by 1\n@app.post(\"/increment\")\nasync def increment():\n    meta, is_created = await MetaModel.get_or_create(id=1)\n    meta.count += 1  # it's better do it in transaction\n    await meta.save()\n    return status.HTTP_200_OK\n# returns a json containing the current count from the meta object\n@app.get(\"/report\")\nasync def report():\n    meta, is_created = await MetaModel.get_or_create(id=1)\n    return {'count': meta.count}\n# resets the count in the meta object to 0\n@app.post(\"/reset\")\nasync def reset():\n    meta, is_created = await MetaModel.get_or_create(id=1)\n    meta.count = 0\n    await meta.save()\n    return status.HTTP_200_OK\nregister_tortoise(\n    app,\n    db_url=\"postgres://test_user:test_pass@localhost:5432/test_db\",  # Don't expose login/pass in src, use environment variables\n    modules={\"models\": [\"app_db\"]},\n    generate_schemas=True,\n    add_exception_handlers=True,\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/60455830/can-you-have-an-async-handler-in-lambda-python-3-6",
    "body": "import asyncio\nimport aioboto3\n# To reduce execution time for subsequent invocations,\n#   open a reusable resource in a global scope\ndynamodb = aioboto3.Session().resource('dynamodb')\nasync def async_handler(event, context):\n    # Put your asynchronous code here\n    table = await dynamodb.Table('test')\n    await table.put_item(\n        Item={'pk': 'test1', 'col1': 'some_data'},\n    )\n    return {'statusCode': 200, 'body': '{\"ok\": true}'}\n# Point to this function as a handler in the Lambda configuration\ndef lambda_handler(event, context):\n    loop = asyncio.get_event_loop()\n    # DynamoDB resource defined above is attached to this loop:\n    #   if you use asyncio.run instead\n    #   you will encounter \"Event loop closed\" exception\n    return loop.run_until_complete(async_handler(event, context))"
  },
  {
    "url": "https://stackoverflow.com/questions/4183208/how-do-i-rotate-an-image-around-its-center-using-pygame",
    "body": "import pygame\npygame.init()\nscreen = pygame.display.set_mode((300, 300))\nclock = pygame.time.Clock()\ndef blitRotate(surf, image, pos, originPos, angle):\n    # offset from pivot to center\n    image_rect = image.get_rect(topleft = (pos[0] - originPos[0], pos[1]-originPos[1]))\n    offset_center_to_pivot = pygame.math.Vector2(pos) - image_rect.center\n\n    # roatated offset from pivot to center\n    rotated_offset = offset_center_to_pivot.rotate(-angle)\n    # roatetd image center\n    rotated_image_center = (pos[0] - rotated_offset.x, pos[1] - rotated_offset.y)\n    # get a rotated image\n    rotated_image = pygame.transform.rotate(image, angle)\n    rotated_image_rect = rotated_image.get_rect(center = rotated_image_center)\n    # rotate and blit the image\n    surf.blit(rotated_image, rotated_image_rect)\n\n    # draw rectangle around the image\n    pygame.draw.rect(surf, (255, 0, 0), (*rotated_image_rect.topleft, *rotated_image.get_size()),2)\ndef blitRotate2(surf, image, topleft, angle):\n    rotated_image = pygame.transform.rotate(image, angle)\n    new_rect = rotated_image.get_rect(center = image.get_rect(topleft = topleft).center)\n    surf.blit(rotated_image, new_rect.topleft)\n    pygame.draw.rect(surf, (255, 0, 0), new_rect, 2)\ntry:\n    image = pygame.image.load('AirPlaneFront.png')\nexcept:\n    text = pygame.font.SysFont('Times New Roman', 50).render('image', False, (255, 255, 0))\n    image = pygame.Surface((text.get_width()+1, text.get_height()+1))\n    pygame.draw.rect(image, (0, 0, 255), (1, 1, *text.get_size()))\n    image.blit(text, (1, 1))\nw, h = image.get_size()\nangle = 0\ndone = False\nwhile not done:\n    clock.tick(60)\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            done = True\n    pos = (screen.get_width()/2, screen.get_height()/2)\n\n    screen.fill(0)\n    blitRotate(screen, image, pos, (w/2, h/2), angle)\n    #blitRotate2(screen, image, pos, angle)\n    angle += 1\n\n    pygame.draw.line(screen, (0, 255, 0), (pos[0]-20, pos[1]), (pos[0]+20, pos[1]), 3)\n    pygame.draw.line(screen, (0, 255, 0), (pos[0], pos[1]-20), (pos[0], pos[1]+20), 3)\n    pygame.draw.circle(screen, (0, 255, 0), pos, 7, 0)\n    pygame.display.flip()\n\npygame.quit()\nexit()"
  },
  {
    "url": "https://stackoverflow.com/questions/34469060/python-native-coroutines-and-send",
    "body": "from queue import Queue\n# ------------------------------------------------------------\n#                       === Tasks ===\n# ------------------------------------------------------------\nclass Task:\n    taskid = 0\n    def __init__(self,target):\n        Task.taskid += 1\n        self.tid = Task.taskid   # Task ID\n        self.target = target        # Target coroutine\n        self.sendval = None          # Value to send\n    # Run a task until it hits the next yield statement\n    def run(self):\n        return self.target.send(self.sendval)\n# ------------------------------------------------------------\n#                      === Scheduler ===\n# ------------------------------------------------------------\nclass Scheduler:\n    def __init__(self):\n        self.ready = Queue()\n        self.taskmap = {}\n    def new(self,target):\n        newtask = Task(target)\n        self.taskmap[newtask.tid] = newtask\n        self.schedule(newtask)\n        return newtask.tid\n    def exit(self,task):\n        print(\"Task %d terminated\" % task.tid)\n        del self.taskmap[task.tid]\n    def schedule(self,task):\n        self.ready.put(task)\n    def mainloop(self):\n         while self.taskmap:\n            task = self.ready.get()\n            try:\n                result = task.run()\n                if isinstance(result,SystemCall):\n                    result.task  = task\n                    result.sched = self\n                    result.handle()\n                    continue\n            except StopIteration:\n                self.exit(task)\n                continue\n            self.schedule(task)\n# ------------------------------------------------------------\n#                   === System Calls ===\n# ------------------------------------------------------------\nclass SystemCall:\n    def handle(self):\n        pass\n    def __await__(self):\n        return (yield self)\n# Return a task's ID number\nclass GetTid(SystemCall):\n    def handle(self):\n        self.task.sendval = self.task.tid\n        self.sched.schedule(self.task)\nclass YieldControl(SystemCall):\n    def handle(self):\n        self.task.sendval = None   # setting sendval is optional\n        self.sched.schedule(self.task)\n# ------------------------------------------------------------\n#                      === Example ===\n# ------------------------------------------------------------\nif __name__ == '__main__':\n    async def foo():\n        mytid = await GetTid()\n        for i in range(3):\n            print(\"I'm foo\", mytid)\n            await YieldControl()\n    async def bar():\n        mytid = await GetTid()\n        for i in range(5):\n            print(\"I'm bar\", mytid)\n            await YieldControl()\n    sched = Scheduler()\n    sched.new(foo())\n    sched.new(bar())\n    sched.mainloop()"
  },
  {
    "url": "https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api",
    "body": "import pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.feature_extraction.text import _VectorizerMixin\nfrom sklearn.feature_selection._base import SelectorMixin\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_extraction.text import CountVectorizer\ntrain = pd.DataFrame({'age': [23,12, 12, np.nan],\n                      'Gender': ['M','F', np.nan, 'F'],\n                      'income': ['high','low','low','medium'],\n                      'sales': [10000, 100020, 110000, 100],\n                      'foo' : [1,0,0,1],\n                      'text': ['I will test this',\n                               'need to write more sentence',\n                               'want to keep it simple',\n                               'hope you got that these sentences are junk'],\n                      'y': [0,1,1,1]})\nnumeric_columns = ['age']\ncat_columns     = ['Gender','income']\nnumeric_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\ncat_pipeline     = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder())\ntext_pipeline = make_pipeline(CountVectorizer(), SelectKBest(k=5))\ntransformers = [\n    ('num', numeric_pipeline, numeric_columns),\n    ('cat', cat_pipeline, cat_columns),\n    ('text', text_pipeline, 'text'),\n    ('simple_transformer', MinMaxScaler(), ['sales']),\n]\ncombined_pipe = ColumnTransformer(\n    transformers, remainder='passthrough')\ntransformed_data = combined_pipe.fit_transform(\n    train.drop('y',1), train['y'])"
  },
  {
    "url": "https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api",
    "body": "def get_feature_out(estimator, feature_in):\n    if hasattr(estimator,'get_feature_names'):\n        if isinstance(estimator, _VectorizerMixin):\n            # handling all vectorizers\n            return [f'vec_{f}' \\\n                for f in estimator.get_feature_names()]\n        else:\n            return estimator.get_feature_names(feature_in)\n    elif isinstance(estimator, SelectorMixin):\n        return np.array(feature_in)[estimator.get_support()]\n    else:\n        return feature_in\ndef get_ct_feature_names(ct):\n    # handles all estimators, pipelines inside ColumnTransfomer\n    # doesn't work when remainder =='passthrough'\n    # which requires the input column names.\n    output_features = []\n    for name, estimator, features in ct.transformers_:\n        if name!='remainder':\n            if isinstance(estimator, Pipeline):\n                current_features = features\n                for step in estimator:\n                    current_features = get_feature_out(step, current_features)\n                features_out = current_features\n            else:\n                features_out = get_feature_out(estimator, features)\n            output_features.extend(features_out)\n        elif estimator=='passthrough':\n            output_features.extend(ct._feature_names_in[features])\n\n    return output_features\npd.DataFrame(transformed_data,\n             columns=get_ct_feature_names(combined_pipe))"
  },
  {
    "url": "https://stackoverflow.com/questions/67255653/how-to-set-up-and-tear-down-a-database-between-tests-in-fastapi",
    "body": "import pytest\nfrom fastapi.testclient import TestClient\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom main import app, get_db\nfrom database import Base\nSQLALCHEMY_DATABASE_URL = \"sqlite:///./test.db\"\nengine = create_engine(\n    SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n)\nTestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\ndef override_get_db():\n    try:\n        db = TestingSessionLocal()\n        yield db\n    finally:\n        db.close()\n@pytest.fixture()\ndef test_db():\n    Base.metadata.create_all(bind=engine)\n    yield\n    Base.metadata.drop_all(bind=engine)\napp.dependency_overrides[get_db] = override_get_db\nclient = TestClient(app)\ndef test_get_todos(test_db):\n    response = client.post(\"/todos/\", json={\"text\": \"some new todo\"})\n    data1 = response.json()\n    response = client.post(\"/todos/\", json={\"text\": \"some even newer todo\"})\n    data2 = response.json()\n    assert data1[\"user_id\"] == data2[\"user_id\"]\n    response = client.get(\"/todos/\")\n    assert response.status_code == 200\n    assert response.json() == [\n        {\"id\": data1[\"id\"], \"user_id\": data1[\"user_id\"], \"text\": data1[\"text\"]},\n        {\"id\": data2[\"id\"], \"user_id\": data2[\"user_id\"], \"text\": data2[\"text\"]},\n    ]\ndef test_get_empty_todos_list(test_db):\n    response = client.get(\"/todos/\")\n    assert response.status_code == 200\n    assert response.json() == []"
  },
  {
    "url": "https://stackoverflow.com/questions/67255653/how-to-set-up-and-tear-down-a-database-between-tests-in-fastapi",
    "body": "import pytest\nimport sqlalchemy as sa\nfrom fastapi.testclient import TestClient\nfrom sqlalchemy.orm import sessionmaker\nfrom database import Base\nfrom main import app, get_db\nSQLALCHEMY_DATABASE_URL = \"sqlite:///./test.db\"\nengine = sa.create_engine(\n    SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n)\nTestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n# Set up the database once\nBase.metadata.drop_all(bind=engine)\nBase.metadata.create_all(bind=engine)\n# These two event listeners are only needed for sqlite for proper\n# SAVEPOINT / nested transaction support. Other databases like postgres\n# don't need them.\n# From: https://docs.sqlalchemy.org/en/14/dialects/sqlite.html#serializable-isolation-savepoints-transactional-ddl\n@sa.event.listens_for(engine, \"connect\")\ndef do_connect(dbapi_connection, connection_record):\n    # disable pysqlite's emitting of the BEGIN statement entirely.\n    # also stops it from emitting COMMIT before any DDL.\n    dbapi_connection.isolation_level = None\n@sa.event.listens_for(engine, \"begin\")\ndef do_begin(conn):\n    # emit our own BEGIN\n    conn.exec_driver_sql(\"BEGIN\")\n# This fixture is the main difference to before. It creates a nested\n# transaction, recreates it when the application code calls session.commit\n# and rolls it back at the end.\n# Based on: https://docs.sqlalchemy.org/en/14/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites\n@pytest.fixture()\ndef session():\n    connection = engine.connect()\n    transaction = connection.begin()\n    session = TestingSessionLocal(bind=connection)\n    # Begin a nested transaction (using SAVEPOINT).\n    nested = connection.begin_nested()\n    # If the application code calls session.commit, it will end the nested\n    # transaction. Need to start a new one when that happens.\n    @sa.event.listens_for(session, \"after_transaction_end\")\n    def end_savepoint(session, transaction):\n        nonlocal nested\n        if not nested.is_active:\n            nested = connection.begin_nested()\n    yield session\n    # Rollback the overall transaction, restoring the state before the test ran.\n    session.close()\n    transaction.rollback()\n    connection.close()\n# A fixture for the fastapi test client which depends on the\n# previous session fixture. Instead of creating a new session in the\n# dependency override as before, it uses the one provided by the\n# session fixture.\n@pytest.fixture()\ndef client(session):\n    def override_get_db():\n        yield session\n    app.dependency_overrides[get_db] = override_get_db\n    yield TestClient(app)\n    del app.dependency_overrides[get_db]\ndef test_get_empty_todos_list(client):\n    response = client.get(\"/todos/\")\n    assert response.status_code == 200\n    assert response.json() == []"
  },
  {
    "url": "https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work",
    "body": "from typing import List\nimport math\ndef kernels(ind,outd) -> List:\n    \"\"\"Returns a List [(kernel_offset_start,kernel_length)] defining all the pooling kernels for a 1-D adaptive pooling layer that takes an input of dimension `ind` and yields an output of dimension `outd`\"\"\"\n    def start_index(a,b,c):\n        return math.floor((float(a) * float(c)) / b)\n    def end_index(a,b,c):\n        return math.ceil((float(a + 1) * float(c)) / b)\n    results = []\n    for ow in range(outd):\n        start = start_index(ow,outd,ind)\n        end = end_index(ow,outd,ind)\n        sz = end - start\n        results.append((start,sz))\n    return results\ndef kernel_indexes(ind,out) -> List:\n    \"\"\"Returns a List [[*ind]] containing the indexes of the pooling kernels\"\"\"\n    startsLengths = kernels(ind,out)\n    return [list(range(start,start+length)) for (start,length) in startsLengths]"
  },
  {
    "url": "https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work",
    "body": "import torch\nimport torch.nn as nn\ndef compare1DAdaptivity(ind,outd,inputpattern):\n    c = 1\n    padding = 0\n    input = torch.Tensor(inputpattern).view(1,c,ind)\n    stride = ind // outd\n    kernel_size = (ind - (outd-1)*stride)\n    avg_pool = nn.AvgPool1d(stride=stride,kernel_size=kernel_size,padding=padding)\n    avg_out = avg_pool(input)\n    adap_avg_pool = torch.nn.AdaptiveAvgPool1d(outd)\n    adap_avg_out = adap_avg_pool(input)\n\n    try:\n        equal_output = torch.allclose(avg_out,adap_avg_out)\n    except:\n        equal_output = False\n    print(\"input.shape: {}\".format(input.shape))\n    print(\"in_dims: {}\".format(ind))\n    print(\"out_dims: {}\".format(outd))\n    print(\"\")\n    print(\"AAL strides: {}\".format(stride))\n    print(\"AAL kernel_sizes: {}\".format(kernel_size))\n    print(\"AAL pad: {}\".format(padding))\n    print(\"\")\n    print(\"outputs equal: {}\".format(equal_output))\n    print(\"\")\n    print(\"AAL input -> output: {} -> {}\".format(input,avg_out))\n    print(\"adap input -> output: {} -> {}\".format(input,adap_avg_out))\n    return equal_output"
  },
  {
    "url": "https://stackoverflow.com/questions/63580229/how-to-save-uploadfile-in-fastapi",
    "body": "`\nimport shutil\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom typing import Callable\nfrom fastapi import UploadFile\ndef save_upload_file(upload_file: UploadFile, destination: Path) -> None:\n    try:\n        with destination.open(\"wb\") as buffer:\n            shutil.copyfileobj(upload_file.file, buffer)\n    finally:\n        upload_file.file.close()\ndef save_upload_file_tmp(upload_file: UploadFile) -> Path:\n    try:\n        suffix = Path(upload_file.filename).suffix\n        with NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n            shutil.copyfileobj(upload_file.file, tmp)\n            tmp_path = Path(tmp.name)\n    finally:\n        upload_file.file.close()\n    return tmp_path\ndef handle_upload_file(\n    upload_file: UploadFile, handler: Callable[[Path], None]\n) -> None:\n    tmp_path = save_upload_file_tmp(upload_file)\n    try:\n        handler(tmp_path)  # Do something with the saved temp file\n    finally:\n        tmp_path.unlink()  # Delete the temp file"
  },
  {
    "url": "https://stackoverflow.com/questions/29640685/how-do-i-detect-collision-in-pygame",
    "body": "  import pygame\n  pygame.init()\n  window = pygame.display.set_mode((250, 250))\n  rect = pygame.Rect(*window.get_rect().center, 0, 0).inflate(100, 100)\n  run = True\n  while run:\n      for event in pygame.event.get():\n          if event.type == pygame.QUIT:\n              run = False\n      point = pygame.mouse.get_pos()\n      collide = rect.collidepoint(point)\n      color = (255, 0, 0) if collide else (255, 255, 255)\n      window.fill(0)\n      pygame.draw.rect(window, color, rect)\n      pygame.display.flip()\n  pygame.quit()\n  exit()"
  },
  {
    "url": "https://stackoverflow.com/questions/29640685/how-do-i-detect-collision-in-pygame",
    "body": "  import pygame\n  pygame.init()\n  window = pygame.display.set_mode((250, 250))\n  rect1 = pygame.Rect(*window.get_rect().center, 0, 0).inflate(75, 75)\n  rect2 = pygame.Rect(0, 0, 75, 75)\n  run = True\n  while run:\n      for event in pygame.event.get():\n          if event.type == pygame.QUIT:\n              run = False\n      rect2.center = pygame.mouse.get_pos()\n      collide = rect1.colliderect(rect2)\n      color = (255, 0, 0) if collide else (255, 255, 255)\n      window.fill(0)\n      pygame.draw.rect(window, color, rect1)\n      pygame.draw.rect(window, (0, 255, 0), rect2, 6, 1)\n      pygame.display.flip()\n  pygame.quit()\n  exit()"
  },
  {
    "url": "https://stackoverflow.com/questions/29640685/how-do-i-detect-collision-in-pygame",
    "body": "  import pygame\n  pygame.init()\n  window = pygame.display.set_mode((250, 250))\n  sprite1 = pygame.sprite.Sprite()\n  sprite1.image = pygame.Surface((75, 75))\n  sprite1.image.fill((255, 0, 0))\n  sprite1.rect = pygame.Rect(*window.get_rect().center, 0, 0).inflate(75, 75)\n  sprite2 = pygame.sprite.Sprite()\n  sprite2.image = pygame.Surface((75, 75))\n  sprite2.image.fill((0, 255, 0))\n  sprite2.rect = pygame.Rect(*window.get_rect().center, 0, 0).inflate(75, 75)\n  all_group = pygame.sprite.Group([sprite2, sprite1])\n  test_group = pygame.sprite.Group(sprite2)\n  run = True\n  while run:\n      for event in pygame.event.get():\n          if event.type == pygame.QUIT:\n              run = False\n      sprite1.rect.center = pygame.mouse.get_pos()\n      collide = pygame.sprite.spritecollide(sprite1, test_group, False)\n      window.fill(0)\n      all_group.draw(window)\n      for s in collide:\n          pygame.draw.rect(window, (255, 255, 255), s.rect, 5, 1)\n      pygame.display.flip()\n  pygame.quit()\n  exit()"
  },
  {
    "url": "https://stackoverflow.com/questions/29640685/how-do-i-detect-collision-in-pygame",
    "body": "  import pygame\n  pygame.init()\n  window = pygame.display.set_mode((250, 250))\n  sprite1 = pygame.sprite.Sprite()\n  sprite1.image = pygame.Surface((80, 80), pygame.SRCALPHA)\n  pygame.draw.circle(sprite1.image, (255, 0, 0), (40, 40), 40)\n  sprite1.rect = pygame.Rect(*window.get_rect().center, 0, 0).inflate(80, 80)\n  sprite1.radius = 40\n  sprite2 = pygame.sprite.Sprite()\n  sprite2.image = pygame.Surface((80, 89), pygame.SRCALPHA)\n  pygame.draw.circle(sprite2.image, (0, 255, 0), (40, 40), 40)\n  sprite2.rect = pygame.Rect(*window.get_rect().center, 0, 0).inflate(80, 80)\n  sprite2.radius = 40\n  all_group = pygame.sprite.Group([sprite2, sprite1])\n  test_group = pygame.sprite.Group(sprite2)\n  run = True\n  while run:\n      for event in pygame.event.get():\n          if event.type == pygame.QUIT:\n              run = False\n      sprite1.rect.center = pygame.mouse.get_pos()\n      collide = pygame.sprite.spritecollide(sprite1, test_group, False, pygame.sprite.collide_circle)\n      window.fill(0)\n      all_group.draw(window)\n      for s in collide:\n          pygame.draw.circle(window, (255, 255, 255), s.rect.center, s.rect.width // 2, 5)\n      pygame.display.flip()\n  pygame.quit()\n  exit()"
  },
  {
    "url": "https://stackoverflow.com/questions/29640685/how-do-i-detect-collision-in-pygame",
    "body": "#Define the sprite class\nclass Sprite:\n    def __init__(self, x, y, name):\n        self.image = pygame.image.load(name)\n        self.rect = self.image.get_rect(topleft = (x, y))\n    def render(self):\n        window.blit(self.image, self.rect)\n# Define the bullet class to create bullets\nclass Bullet:\n    def __init__(self, x, y):\n        self.bullet = pygame.image.load(\"user_bullet.BMP\")\n        self.rect = self.bullet.get_rect(topleft = (x + 23, y))\n    def render(self):\n        window.blit(self.bullet, self.rect)"
  },
  {
    "url": "https://stackoverflow.com/questions/35736598/cannot-pip-install-cryptography-in-docker-alpine-linux-3-3-with-openssl-1-0-2g",
    "body": "writing manifest file 'src/cryptography.egg-info/SOURCES.txt'\nrunning build_ext\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_padding.c'\ncreating build/temp.linux-x86_64-2.7\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_constant_time.c'\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_openssl.c'\nbuilding '_openssl' extension\ncreating build/temp.linux-x86_64-2.7/build\ncreating build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7\ngcc -fno-strict-aliasing -Os -fomit-frame-pointer -g -DNDEBUG -Os -fomit-frame-pointer -g -DTHREAD_STACK_SIZE=0x100000 -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_openssl.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o -Wconversion -Wno-error=sign-conversion\nbuild/temp.linux-x86_64-2.7/_openssl.c:493:30: fatal error: openssl/opensslv.h: No such file or directory\n #include <openssl/opensslv.h>\n                              ^\ncompilation terminated.\nerror: command 'gcc' failed with exit status 1"
  },
  {
    "url": "https://stackoverflow.com/questions/50792316/what-does-1-of-view-mean-in-pytorch",
    "body": "import torch\nx = torch.arange(6)\nprint(x.view(3, -1))      # inferred size will be 2 as 6 / 3 = 2\n# tensor([[ 0.,  1.],\n#         [ 2.,  3.],\n#         [ 4.,  5.]])\nprint(x.view(-1, 6))      # inferred size will be 1 as 6 / 6 = 1\n# tensor([[ 0.,  1.,  2.,  3.,  4.,  5.]])\nprint(x.view(1, -1, 2))   # inferred size will be 3 as 6 / (1 * 2) = 3\n# tensor([[[ 0.,  1.],\n#          [ 2.,  3.],\n#          [ 4.,  5.]]])\n# print(x.view(-1, 5))    # throw error as there's no int N so that 5 * N = 6\n# RuntimeError: invalid argument 2: size '[-1 x 5]' is invalid for input with 6 elements\nprint(x.view(-1, -1, 3))  # throw error as only one dimension can be inferred\n# RuntimeError: invalid argument 1: only one dimension can be inferred"
  },
  {
    "url": "https://stackoverflow.com/questions/41403458/how-do-i-send-html-formatted-emails-through-the-gmail-api-for-python",
    "body": "def create_message(sender, to, cc, subject, message_text):\n    \"\"\"Create a message for an email.\n    Args:\n    sender: Email address of the sender.\n    to: Email address of the receiver.\n    subject: The subject of the email message.\n    message_text: The text of the email message.\n    Returns:\n    An object containing a base64url encoded email object.\n    \"\"\"\n    print(sender + ', ' + to + ', ' + subject + ', ' + message_text)\n    message = MIMEText(message_text,'html')\n    message['to'] = to\n    message['from'] = sender\n    message['subject'] = subject\n    message['cc'] = cc\n    pprint(message)\n    return {'raw': base64.urlsafe_b64encode(message.as_string())}"
  },
  {
    "url": "https://stackoverflow.com/questions/14982836/rendering-and-saving-images-through-blender-python",
    "body": "def rotate_and_render(output_dir, output_file_pattern_string = 'render%d.jpg', rotation_steps = 32, rotation_angle = 360.0, subject = bpy.context.object):\n  import os\n  original_rotation = subject.rotation_euler\n  for step in range(0, rotation_steps):\n    subject.rotation_euler[2] = radians(step * (rotation_angle / rotation_steps))\n    bpy.context.scene.render.filepath = os.path.join(output_dir, (output_file_pattern_string % step))\n    bpy.ops.render.render(write_still = True)\n  subject.rotation_euler = original_rotation\nrotate_and_render('/Users/myusername/Pictures/VR', 'render%d.jpg')"
  },
  {
    "url": "https://stackoverflow.com/questions/47402435/pytest-fixture-of-fixture-not-found",
    "body": "======================================= test session starts ========================================\nplatform darwin -- Python 3.6.8, pytest-4.3.0\ncollected 1 item\ntest2.py E                                                                                   [100%]\n============================================== ERRORS ==============================================\n__________________________________ ERROR at setup of test_foo_bar __________________________________\nfile .../test_foo_bar.py, line 3\n  def test_foo_bar(bar):\n.../test.py, line 7\n  @pytest.fixture\n  def bar(foo):\nE       fixture 'foo' not found\n>       available fixtures: TIMEOUT, bar, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, doctest_namespace, monkeypatch, no_cover, once_without_docker, pytestconfig, record_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n.../test.py:7\n===================================== 1 error in 0.03 seconds ======================================"
  },
  {
    "url": "https://stackoverflow.com/questions/18159221/remove-namespace-and-prefix-from-xml-in-python-using-lxml",
    "body": "from lxml import etree\ninput_xml = \"\"\"\n<package xmlns=\"http://apple.com/itunes/importer\">\n  <provider>some data</provider>\n  <language>en-GB</language>\n  <!-- some comment -->\n  <?xml-some-processing-instruction ?>\n</package>\n\"\"\"\nroot = etree.fromstring(input_xml)\n# Iterate through all XML elements\nfor elem in root.getiterator():\n    # Skip comments and processing instructions,\n    # because they do not have names\n    if not (\n        isinstance(elem, etree._Comment)\n        or isinstance(elem, etree._ProcessingInstruction)\n    ):\n        # Remove a namespace URI in the element's name\n        elem.tag = etree.QName(elem).localname\n# Remove unused namespace declarations\netree.cleanup_namespaces(root)\nprint(etree.tostring(root).decode())"
  },
  {
    "url": "https://stackoverflow.com/questions/67807596/pyenv-install-3-x-build-failed-ubuntu-20-04-using-python-build-20180424",
    "body": "/tmp/python-build.20210602162502.2268/Python-3.9.4/Modules/_ctypes/_ctypes.c:107:10: fatal error: ffi.h: No such file or directory\n  107 | #include <ffi.h>\n      |          ^~~~~~~\ncompilation terminated.\nPython build finished successfully!\nThe necessary bits to build these optional modules were not found:\n_bz2                  _curses_panel         _dbm\n_gdbm                 _lzma                 _sqlite3\n_tkinter              _uuid                 zlib\nTo find the necessary bits, look in setup.py in detect_modules() for the module's name.\nThe following modules found by detect_modules() in setup.py, have been\nbuilt by the Makefile instead, as configured by the Setup files:\n_abc                  atexit                pwd\ntime\nFailed to build these modules:\n_ctypes               _curses"
  },
  {
    "url": "https://stackoverflow.com/questions/57983431/whats-the-most-space-efficient-way-to-compress-serialized-python-data",
    "body": "import bz2\nimport gzip\nimport lzma\nimport pickle\nimport brotli\nclass SomeObject():\n    a = 'some data'\n    b = 123\n    c = 'more data'\n    def __init__(self, i):\n        self.i = i\ndata = [SomeObject(i) for i in range(1, 1000000)]\nwith open('no_compression.pickle', 'wb') as f:\n    pickle.dump(data, f)\nwith gzip.open(\"gzip_test.gz\", \"wb\") as f:\n    pickle.dump(data, f)\nwith bz2.BZ2File('bz2_test.pbz2', 'wb') as f:\n    pickle.dump(data, f)\nwith lzma.open(\"lzma_test.xz\", \"wb\") as f:\n    pickle.dump(data, f)\nwith open('no_compression.pickle', 'rb') as f:\n    pdata = f.read()\n    with open('brotli_test.bt', 'wb') as b:\n        b.write(brotli.compress(pdata))"
  },
  {
    "url": "https://stackoverflow.com/questions/36162414/how-to-add-bold-annotated-text-to-a-plot",
    "body": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nswing = pd.read_csv('https://assets.datacamp.com/production/repositories/469/datasets/e079fddb581197780e1a7b7af2aeeff7242535f0/2008_swing_states.csv')\n\nplt.figure(figsize=(10, 10))\nsns.scatterplot(x='total_votes', y='dem_share', data=swing, hue='state')\nplt.xlabel('total votes')\nplt.ylabel('% of vote for Obama')\nplt.xticks(range(0, 1000000, 100000), rotation=40)\nplt.yticks(range(0, 100, 10))\n\n# Create a Rectangle patch\nplt.gca().add_patch(Rectangle((400000, 52), 500000, 34, linewidth=1, edgecolor='b', facecolor='none'))\n\nplt.gca().add_patch(Rectangle((0, 5), 50000, 45, linewidth=1, edgecolor='r', facecolor='none'))"
  },
  {
    "url": "https://stackoverflow.com/questions/36162414/how-to-add-bold-annotated-text-to-a-plot",
    "body": "fig, ax = plt.subplots(figsize=(10, 10))\nsns.scatterplot(x='total_votes', y='dem_share', data=swing, hue='state', ax=ax)\nax.set_xlabel('total votes')\nax.set_ylabel('% of vote for Obama')\nax.set_xticks(range(0, 1000000, 100000), range(0, 1000000, 100000), rotation=40)\nax.set_yticks(range(0, 100, 10))\n\n# Create a Rectangle patch\nax.add_patch(Rectangle((400000, 52), 500000, 34, linewidth=1, edgecolor='b', facecolor='none'))\n\nax.add_patch(Rectangle((0, 5), 50000, 45, linewidth=1, edgecolor='r', facecolor='none'))\nax.annotate('12 largest counties; most vote for Obama', xy=(650000, 52),\n             xytext=(400000, 35), fontsize=10, arrowprops=dict(arrowstyle=\"->\", color='b'))\nax.annotate('small counties; most vote for McCain', xy=(50000, 20), weight='bold',\n             xytext=(150000, 7), fontsize=10, arrowprops=dict(arrowstyle=\"->\", color='r'))\nax.text(600000, 20, '2008 Swing-State Counties', weight='bold')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/36162414/how-to-add-bold-annotated-text-to-a-plot",
    "body": "g = sns.relplot(x='total_votes', y='dem_share', data=swing, hue='state', height=10)\n# iterate through each axes of the figure-level plot\nfor ax in g.axes.flat:\n    ax.set_xlabel('total votes')\n    ax.set_ylabel('% of vote for Obama')\n    ax.set_xticks(range(0, 1000000, 100000), range(0, 1000000, 100000), rotation=40)\n    ax.set_yticks(range(0, 100, 10))\n    # Create a Rectangle patch\n    ax.add_patch(Rectangle((400000, 52), 500000, 34, linewidth=1, edgecolor='b', facecolor='none'))\n    ax.add_patch(Rectangle((0, 5), 50000, 45, linewidth=1, edgecolor='r', facecolor='none'))\n    ax.annotate('12 largest counties; most vote for Obama', xy=(650000, 52),\n                 xytext=(400000, 35), fontsize=10, arrowprops=dict(arrowstyle=\"->\", color='b'))\n    ax.annotate('small counties; most vote for McCain', xy=(50000, 20), weight='bold',\n                 xytext=(150000, 7), fontsize=10, arrowprops=dict(arrowstyle=\"->\", color='r'))\n    ax.text(600000, 20, '2008 Swing-State Counties', weight='bold')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/36162414/how-to-add-bold-annotated-text-to-a-plot",
    "body": "state,county,total_votes,dem_votes,rep_votes,dem_share\nPA,Erie County,127691,75775,50351,60.08\nPA,Bradford County,25787,10306,15057,40.64\nPA,Tioga County,17984,6390,11326,36.07\nPA,McKean County,15947,6465,9224,41.21\nPA,Potter County,7507,2300,5109,31.04\nPA,Wayne County,22835,9892,12702,43.78\nPA,Susquehanna County,19286,8381,10633,44.08\nPA,Warren County,18517,8537,9685,46.85\nOH,Ashtabula County,44874,25027,18949,56.94\nOH,Lake County,121335,60155,59142,50.46\nPA,Crawford County,38134,16780,20750,44.71\nOH,Lucas County,219830,142852,73706,65.99\nOH,Fulton County,21973,9900,11689,45.88\nOH,Geauga County,51102,21250,29096,42.23\nOH,Williams County,18397,8174,9880,45.26\nPA,Wyoming County,13138,5985,6983,46.15\nPA,Lackawanna County,107876,67520,39488,63.1\nPA,Elk County,14271,7290,6676,52.2\nPA,Forest County,2444,1038,1366,43.18\nPA,Venango County,23307,9238,13718,40.24\nOH,Erie County,41229,23148,17432,57.01\nOH,Wood County,65022,34285,29648,53.61\nPA,Cameron County,2245,879,1323,39.92\nPA,Pike County,24284,11493,12518,47.87\nPA,Lycoming County,49237,18381,30280,37.77\nPA,Sullivan County,3120,1233,1841,40.11\nOH,Lorain County,146859,85276,59068,59.1\nOH,Trumbull County,106911,64145,40164,61.48\nPA,Mercer County,53821,26411,26565,49.85\nOH,Henry County,14840,6320,8239,43.43\nPA,Clinton County,14791,7097,7504,48.61\nPA,Clarion County,17766,6756,10737,38.62\nPA,Luzerne County,135175,72492,61127,54.25\nOH,Defiance County,19195,8399,10407,44.69\nPA,Jefferson County,18802,6447,12057,34.84\nOH,Portage County,78206,41856,34822,54.59\nPA,Columbia County,28063,13230,14477,47.75\nOH,Huron County,25582,12076,12884,48.36\nOH,Medina County,90451,40924,48189,45.89\nOH,Seneca County,27449,13087,13823,48.62\nPA,Clearfield County,33813,14555,18662,43.82\nPA,Centre County,75763,41950,32992,55.97\nPA,Monroe County,68443,39453,28293,58.23\nOH,Paulding County,9769,4165,5317,43.92\nPA,Northumberland County,33939,14329,19018,42.97\nPA,Montour County,8023,3364,4574,42.38\nPA,Butler County,90425,32260,57074,36.11\nPA,Armstrong County,30081,11138,18542,37.53\nOH,Hancock County,36981,13870,22420,38.23\nOH,Putnam County,18680,5281,13072,28.79\nPA,Union County,17400,7333,9859,42.65\nOH,Mahoning County,127032,79173,45319,63.57\nPA,Carbon County,26923,13464,12957,50.96\nPA,Lawrence County,42103,19711,21851,47.43\nOH,Ashland County,25168,9300,15158,38.07\nOH,Crawford County,21173,8288,12316,40.18\nOH,Richland County,61122,25727,34034,43.05\nOH,Wyandot County,10977,4461,6270,41.56\nOH,Wayne County,52142,21712,29342,42.49\nOH,Van Wert County,14652,5178,9168,36.06\nOH,Stark County,187545,96990,86743,52.76\nPA,Northampton County,135587,75255,58551,56.24\nPA,Schuylkill County,63057,28300,33767,45.6\nOH,Columbiana County,48487,21882,25585,46.07\nOH,Allen County,50263,19522,29940,39.43\nPA,Indiana County,37302,17065,19727,46.39\nPA,Snyder County,15479,5382,9900,35.22\nPA,Beaver County,84488,40499,42895,48.56\nPA,Mifflin County,16502,5375,10929,32.97\nOH,Hardin County,13114,5013,7749,39.26\nPA,Lehigh County,152473,87089,63382,57.88\nPA,Huntingdon County,18632,6621,11745,36.05\nPA,Blair County,53102,19813,32708,37.72\nOH,Carroll County,13953,6423,7097,47.47\nOH,Mercer County,21271,5853,15100,27.92\nPA,Cambria County,65670,32451,31995,50.36\nOH,Morrow County,16643,6177,10067,38.01\nOH,Marion County,29017,12870,15454,45.45\nPA,Juniata County,9711,3068,6484,32.12\nOH,Auglaize County,23486,6727,16395,29.07\nPA,Westmoreland County,176873,72721,102294,41.55\nPA,Berks County,180000,97047,80513,54.66\nPA,Allegheny County,651436,373153,272347,57.81\nOH,Holmes County,11113,3141,7720,28.94\nOH,Tuscarawas County,42950,21498,20454,51.28\nPA,Dauphin County,129529,69975,58238,54.58\nPA,Perry County,19745,6396,13058,32.88\nPA,Bucks County,332924,179031,150248,54.37\nOH,Jefferson County,35939,17635,17559,50.1\nOH,Knox County,28231,11014,16640,39.84\nPA,Lebanon County,58297,23310,34314,40.45\nOH,Logan County,22217,7936,13848,36.43\nOH,Union County,24928,8761,15744,35.71\nOH,Shelby County,23668,7317,15924,31.47\nPA,Washington County,98047,46122,50752,47.61\nOH,Coshocton County,16863,7689,8675,47.01\nPA,Montgomery County,422419,253393,165552,60.49\nOH,Delaware County,92416,36653,54778,40.1\nOH,Harrison County,7787,3683,3872,48.76\nOH,Darke County,25793,7964,17290,31.56\nPA,Cumberland County,113304,48306,63739,43.11\nPA,Bedford County,22443,6059,16124,27.32\nPA,Lancaster County,228137,99586,126568,44.03\nPA,Franklin County,63641,21169,41906,33.56\nPA,Somerset County,35168,12878,21686,37.26\nOH,Champaign County,18887,7385,11141,39.86\nPA,Chester County,254354,137833,114421,54.64\nPA,York County,194210,82839,109268,43.12\nOH,Guernsey County,17325,7625,9197,45.31\nOH,Miami County,52807,18372,33417,35.47\nOH,Belmont County,32411,16302,15422,51.38\nOH,Muskingum County,39071,17730,20549,46.33\nPA,Fulton County,6306,1576,4642,25.34\nPA,Fayette County,52560,25866,26081,49.79\nPA,Philadelphia County,717329,595980,117221,83.56\nPA,Adams County,44491,17633,26349,40.09\nPA,Delaware County,297004,178870,115273,60.81\nOH,Clark County,66770,31958,33634,48.73\nPA,Greene County,15976,7829,7889,49.81\nOH,Noble County,6172,2474,3450,41.77\nOH,Fairfield County,71945,29250,41580,41.32\nOH,Perry County,15404,7261,7721,48.46\nOH,Montgomery County,278511,145997,128679,53.14\nOH,Preble County,21002,6999,13562,34.01\nOH,Monroe County,6982,3705,3066,54.74\nOH,Greene County,83589,33540,48936,40.67\nOH,Pickaway County,23726,9077,14228,38.96\nOH,Morgan County,6608,2966,3440,46.29\nOH,Fayette County,11694,4401,7102,38.25\nOH,Washington County,18802,1238,17019,6.8\nOH,Warren County,106216,33398,71691,31.75\nOH,Ross County,31840,14455,16759,46.33\nOH,Vinton County,5646,2463,3021,44.9\nOH,Clermont County,95480,31611,62559,33.57\nOH,Brown County,20113,7503,12192,38.1\nOH,Jackson County,13993,5397,8219,39.67\nOH,Meigs County,10354,4094,6015,40.47\nOH,Pike County,12506,6033,6162,49.44\nOH,Adams County,11388,4170,6914,37.62\nOH,Gallia County,13318,4777,8247,36.71\nOH,Scioto County,32571,14926,16994,46.73\nOH,Lawrence County,27194,11262,15415,42.2\nFL,Jackson County,21565,7671,13717,35.86\nFL,Escambia County,154447,61572,91411,40.25\nFL,Santa Rosa County,76185,19470,55972,25.81\nFL,Okaloosa County,95529,25872,68789,27.33\nFL,Holmes County,8589,1446,7033,17.06\nFL,Walton County,27046,7174,19561,26.84\nFL,Washington County,11131,2863,8178,25.93\nFL,Nassau County,38304,10618,27403,27.93\nFL,Gadsden County,22510,15582,6811,69.58\nFL,Leon County,148608,91747,55705,62.23\nFL,Jefferson County,7957,4088,3797,51.85\nFL,Madison County,8907,4270,4544,48.44\nFL,Hamilton County,5587,2364,3179,42.65\nFL,Calhoun County,6244,1821,4345,29.53\nFL,Liberty County,3278,895,2339,27.67\nFL,Columbia County,28128,9171,18670,32.94\nFL,Duval County,415761,202618,210537,49.04\nFL,Baker County,11059,2327,8672,21.15\nFL,Bay County,81127,23653,56683,29.45\nFL,Suwannee County,17662,4916,12534,28.17\nFL,Taylor County,9366,2803,6457,30.27\nFL,Wakulla County,14376,5311,8877,37.43\nFL,Lafayette County,3359,642,2679,19.33\nFL,Saint Johns County,105844,35791,69222,34.08\nFL,Gulf County,7205,2149,4980,30.15\nFL,Clay County,94577,26697,67203,28.43\nFL,Bradford County,11676,3430,8136,29.66\nFL,Union County,5293,1300,3940,24.81\nFL,Franklin County,6029,2134,3818,35.86\nFL,Alachua County,125519,75565,48513,60.9\nFL,Gilchrist County,7819,1996,5656,26.09\nFL,Putnam County,33171,13236,19637,40.26\nFL,Dixie County,7264,1925,5194,27.04\nFL,Flagler County,49031,24726,23951,50.8\nFL,Levy County,18725,6711,11754,36.35\nFL,Marion County,162022,70839,89628,44.14\nFL,Volusia County,243824,127795,113938,52.86\nFL,Lake County,146926,62948,82802,43.19\nFL,Citrus County,76158,31460,43706,41.85\nFL,Sumter County,48868,17655,30866,36.39\nFL,Seminole County,205895,99335,105070,48.6\nFL,Brevard County,287859,127620,157589,44.74\nFL,Orange County,462711,273009,186832,59.37\nFL,Hernando County,87901,41886,45021,48.19\nFL,Pasco County,214866,102417,110104,48.2\nFL,Polk County,244833,113865,128878,46.91\nFL,Osceola County,100670,59962,40086,59.93\nFL,Pinellas County,463282,248299,210066,54.17\nFL,Hillsborough County,513312,272963,236355,53.59\nFL,Indian River County,70591,29710,40176,42.52\nFL,Highlands County,44783,18135,26221,40.89\nFL,Hardee County,7412,2568,4763,35.03\nFL,Manatee County,151994,70034,80721,46.46\nFL,Okeechobee County,12786,5108,7561,40.32\nFL,Saint Lucie County,120579,67125,52512,56.11\nFL,Sarasota County,207353,102686,102897,49.95\nFL,DeSoto County,10131,4383,5632,43.76\nFL,Martin County,78294,33508,44143,43.15\nFL,Glades County,3358,1381,1938,41.61\nFL,Charlotte County,85158,39031,45205,46.34\nFL,Palm Beach County,590500,361271,226037,61.51\nFL,Hendry County,10879,4998,5780,46.37\nFL,Lee County,269276,119701,147608,44.78\nFL,Collier County,141988,54450,86379,38.66\nFL,Broward County,733899,492640,237729,67.45\nFL,Miami-Dade County,863486,499831,360551,58.09\nFL,Monroe County,40272,20907,18933,52.48\nOH,Ottawa County,23069,12049,10618,53.16\nOH,Sandusky County,30373,15601,14190,52.4\nOH,Summit County,269059,155105,110499,58.36\nOH,Athens County,31098,20722,9742,68.02\nOH,Butler County,173777,66030,105341,38.53\nOH,Clinton County,19305,6558,12409,34.58\nOH,Cuyahoga County,665352,458422,199880,69.64\nOH,Franklin County,560325,334709,218486,60.5\nOH,Hamilton County,425086,225213,195530,53.53\nOH,Highland County,19186,6856,11907,36.54\nOH,Hocking County,12961,6259,6364,49.58\nOH,Licking County,82356,33932,46918,41.97\nOH,Madison County,17454,6532,10606,38.11"
  },
  {
    "url": "https://stackoverflow.com/questions/78427983/why-does-dictid-1-id-2-sometimes-raise-keyerror-id-instead-of-a",
    "body": "In [1]: import dis\nIn [2]: dis.dis(\"dict(id=1, **{'id': 2})\")\n  1           0 LOAD_NAME                0 (dict)\n              2 LOAD_CONST               3 (())\n              4 LOAD_CONST               0 ('id')\n              6 LOAD_CONST               1 (1)\n              8 BUILD_MAP                1\n             10 LOAD_CONST               0 ('id')\n             12 LOAD_CONST               2 (2)\n             14 BUILD_MAP                1\n             16 DICT_MERGE               1\n             18 CALL_FUNCTION_EX         1\n             20 RETURN_VALUE"
  },
  {
    "url": "https://stackoverflow.com/questions/9505898/conditional-command-line-arguments-in-python-using-argparse",
    "body": "$ python ap.py tmp.txt upload\nusage: ap.py file upload [-h] {amazon,imgur}\nap.py file upload: error: too few arguments\n$ python ap.py tmp.txt upload amazo\nusage: ap.py file upload [-h] {amazon,imgur}\nap.py file upload: error: argument server: invalid choice: 'amazo' (choose from 'amazon', 'imgur')\n$ python ap.py tmp.txt upload amazon\nNamespace(file='tmp.txt', server='amazon', subcommand='upload')\nI will now upload \"tmp.txt\" to amazon\n$ python ap.py tmp.txt upload imgur\nNamespace(file='tmp.txt', server='imgur', subcommand='upload')\nI will now upload \"tmp.txt\" to imgur"
  },
  {
    "url": "https://stackoverflow.com/questions/72005302/completely-uninstall-python-3-on-mac",
    "body": "# Remove the application\nsudo rm -rf /Applications/Python\\ 3.x\n# Check for framework files\nls -la /Library/Frameworks/Python.framework/Versions/\n# If found, remove the specific version\nsudo rm -rf /Library/Frameworks/Python.framework/Versions/3.x\n# Check for and remove symbolic links\n# For Intel Macs:\ncd /usr/local/bin\nls -l | grep '../Library/Frameworks/Python.framework/Versions/3.x' | awk '{print $9}' | xargs rm -f\n# For Apple Silicon:\ncd /opt/homebrew/bin\nls -l | grep '../Library/Frameworks/Python.framework/Versions/3.x' | awk '{print $9}' | xargs rm -f\n# Check for user-specific packages\nls -l ~/Library/Python/\n# Remove if needed\nrm -rf ~/Library/Python/3.x/"
  },
  {
    "url": "https://stackoverflow.com/questions/72005302/completely-uninstall-python-3-on-mac",
    "body": "# List all installed Python versions\npyenv versions\n# Show the current active version\npyenv version\n# Uninstall a specific version\npyenv uninstall 3.x.y\n# Check pyenv installation location\nls -la ~/.pyenv/versions/\n# Remove pyenv shims and rehash\npyenv rehash\n# If you want to completely remove pyenv itself\n# For Homebrew installation:\nbrew uninstall pyenv\n# For manual installation:\nrm -rf ~/.pyenv\n# Don't forget to remove pyenv initialization from your shell profile\n# (.bash_profile, .zshrc, etc.)"
  },
  {
    "url": "https://stackoverflow.com/questions/12788217/how-can-i-extract-a-single-value-from-a-nested-data-structure-such-as-from-pars",
    "body": ">>> print(json.dumps(my_json, indent=4))\n{\n    \"name\": \"ns1:timeSeriesResponseType\",\n    \"declaredType\": \"org.cuahsi.waterml.TimeSeriesResponseType\",\n    \"scope\": \"javax.xml.bind.JAXBElement$GlobalScope\",\n    \"value\": {\n        \"queryInfo\": {\n            \"creationTime\": 1349724919000,\n            \"queryURL\": \"http://waterservices.usgs.gov/nwis/iv/\",\n            \"criteria\": {\n                \"locationParam\": \"[ALL:103232434]\",\n                \"variableParam\": \"[00060, 00065]\"\n            },\n            \"note\": [\n                {\n                    \"value\": \"[ALL:103232434]\",\n                    \"title\": \"filter:sites\"\n                },\n                {\n                    \"value\": \"[mode=LATEST, modifiedSince=null]\",\n                    \"title\": \"filter:timeRange\"\n                },\n                {\n                    \"value\": \"sdas01\",\n                    \"title\": \"server\"\n                }\n            ]\n        }\n    },\n    \"nil\": false,\n    \"globalScope\": true,\n    \"typeSubstituted\": false\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/21182725/how-to-use-query-strings-for-filtering-querysets-in-django",
    "body": "class PassengerList(generics.ListCreateAPIView):\n    model = Passenger\n    serializer_class = PassengerSerializer\n    # Show all of the PASSENGERS in particular WORKSPACE\n    # or all of the PASSENGERS in particular AIRLINE\n    def get_queryset(self):\n        queryset = Passenger.objects.all()\n        workspace = self.request.query_params.get('workspace')\n        airline = self.request.query_params.get('airline')\n        if workspace:\n            queryset = queryset.filter(workspace_id=workspace)\n        elif airline:\n            queryset = queryset.filter(workspace__airline_id=airline)\n        return queryset"
  },
  {
    "url": "https://stackoverflow.com/questions/45022566/create-python-cli-with-select-interface",
    "body": "from whaaaaat import prompt, print_json, Separator\nquestions = [\n    {\n        \"type\": \"list\",\n        \"name\": \"theme\",\n        \"message\": \"What do you want to do?\",\n        \"choices\": [\n            \"Order a pizza\",\n            \"Make a reservation\",\n            Separator(),\n            \"Ask for opening hours\",\n            {\"name\": \"Contact support\", \"disabled\": \"Unavailable at this time\"},\n            \"Talk to the receptionist\",\n        ],\n    },\n    {\n        \"type\": \"list\",\n        \"name\": \"size\",\n        \"message\": \"What size do you need?\",\n        \"choices\": [\"Jumbo\", \"Large\", \"Standard\", \"Medium\", \"Small\", \"Micro\"],\n        \"filter\": lambda val: val.lower(),\n    },\n]\nanswers = prompt(questions)\nprint_json(answers)"
  },
  {
    "url": "https://stackoverflow.com/questions/75556221/why-is-np-dot-so-much-faster-than-np-sum",
    "body": "# L3 cache of 9 MiB\n# 2 x 22.9 = 45.8 MiB\na = np.ones(3_000_000)\nb = np.ones(3_000_000)\n%timeit -n 100 np.dot(a, a)   #  494 µs => read from RAM\n%timeit -n 100 np.dot(a, b)   # 1007 µs => read from RAM\n# 2 x 7.6 = 15.2 MiB\na = np.ones(1_000_000)\nb = np.ones(1_000_000)\n%timeit -n 100 np.dot(a, a)   #  90 µs => read from the L3 cache\n%timeit -n 100 np.dot(a, b)   # 283 µs => read from RAM\n# 2 x 1.9 = 3.8 MiB\na = np.ones(250_000)\nb = np.ones(250_000)\n%timeit -n 100 np.dot(a, a)   # 40 µs => read from the L3 cache (quite compute-bound)\n%timeit -n 100 np.dot(a, b)   # 46 µs => read from the L3 cache too (quite memory-bound)"
  },
  {
    "url": "https://stackoverflow.com/questions/75556221/why-is-np-dot-so-much-faster-than-np-sum",
    "body": ".LBB0_7:\n        vaddpd  (%r9,%rdx,8), %ymm0, %ymm0\n        vaddpd  32(%r9,%rdx,8), %ymm1, %ymm1\n        vaddpd  64(%r9,%rdx,8), %ymm2, %ymm2\n        vaddpd  96(%r9,%rdx,8), %ymm3, %ymm3\n        vaddpd  128(%r9,%rdx,8), %ymm0, %ymm0\n        vaddpd  160(%r9,%rdx,8), %ymm1, %ymm1\n        vaddpd  192(%r9,%rdx,8), %ymm2, %ymm2\n        vaddpd  224(%r9,%rdx,8), %ymm3, %ymm3\n        vaddpd  256(%r9,%rdx,8), %ymm0, %ymm0\n        vaddpd  288(%r9,%rdx,8), %ymm1, %ymm1\n        vaddpd  320(%r9,%rdx,8), %ymm2, %ymm2\n        vaddpd  352(%r9,%rdx,8), %ymm3, %ymm3\n        vaddpd  384(%r9,%rdx,8), %ymm0, %ymm0\n        vaddpd  416(%r9,%rdx,8), %ymm1, %ymm1\n        vaddpd  448(%r9,%rdx,8), %ymm2, %ymm2\n        vaddpd  480(%r9,%rdx,8), %ymm3, %ymm3\n        addq    $64, %rdx\n        addq    $-4, %r11\n        jne     .LBB0_7"
  },
  {
    "url": "https://stackoverflow.com/questions/35538882/how-to-remove-the-duplicate-legend-when-overlaying-boxplot-and-stripplot",
    "body": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ntips = sns.load_dataset(\"tips\")\nsns.stripplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", data=tips, jitter=True, palette=\"Set2\", dodge=True, linewidth=1, edgecolor='gray')\n# Get the ax object to use later.\nax = sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", data=tips, palette=\"Set2\", fliersize=0)\n# Get the handles and labels. For this example it'll be 2 tuples\n# of length 4 each.\nhandles, labels = ax.get_legend_handles_labels()\n# When creating the legend, only use the first two elements\n# to effectively remove the last two.\nl = plt.legend(handles[0:2], labels[0:2], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
  },
  {
    "url": "https://stackoverflow.com/questions/36243538/how-often-do-i-have-to-commit-to-ensure-sqlite-execution-statements-all-get-comm",
    "body": "import os\nimport sqlite3\n_DBPATH = \"./q6996603.sqlite\"\ndef fresh_db():\n    if os.path.isfile(_DBPATH):\n        os.remove(_DBPATH)\n    with sqlite3.connect(_DBPATH) as conn:\n        cur = conn.cursor().executescript(\"\"\"\n            CREATE TABLE \"mytable\" (\n                \"id\" INTEGER PRIMARY KEY AUTOINCREMENT, -- rowid\n                \"data\" INTEGER\n            );\n            \"\"\")\n    print \"created %s\" % _DBPATH\n# functions are syntactic sugar only and use global conn, cur, rowid\ndef select():\n    sql = 'select * from \"mytable\"'\n    rows = cur.execute(sql).fetchall()\n    print \"   same connection sees\", rows\n    # simulate another script accessing tha database concurrently\n    with sqlite3.connect(_DBPATH) as conn2:\n        rows = conn2.cursor().execute(sql).fetchall()\n    print \"   other connection sees\", rows\ndef count():\n    print \"counting up\"\n    cur.execute('update \"mytable\" set data = data + 1 where \"id\" = ?', (rowid,))\ndef commit():\n    print \"commit\"\n    conn.commit()\n# now the script\nfresh_db()\nwith sqlite3.connect(_DBPATH) as conn:\n    print \"--- prepare test case\"\n    sql = 'insert into \"mytable\"(data) values(17)'\n    print sql\n    cur = conn.cursor().execute(sql)\n    rowid = cur.lastrowid\n    print \"rowid =\", rowid\n    commit()\n    select()\n    print \"--- two consecutive w/o commit\"\n    count()\n    select()\n    count()\n    select()\n    commit()\n    select()\n    print \"--- two consecutive with commit\"\n    count()\n    select()\n    commit()\n    select()\n    count()\n    select()\n    commit()\n    select()"
  },
  {
    "url": "https://stackoverflow.com/questions/36243538/how-often-do-i-have-to-commit-to-ensure-sqlite-execution-statements-all-get-comm",
    "body": "$ python try.py\ncreated ./q6996603.sqlite\n--- prepare test case\ninsert into \"mytable\"(data) values(17)\nrowid = 1\ncommit\n   same connection sees [(1, 17)]\n   other connection sees [(1, 17)]\n--- two consecutive w/o commit\ncounting up\n   same connection sees [(1, 18)]\n   other connection sees [(1, 17)]\ncounting up\n   same connection sees [(1, 19)]\n   other connection sees [(1, 17)]\ncommit\n   same connection sees [(1, 19)]\n   other connection sees [(1, 19)]\n--- two consecutive with commit\ncounting up\n   same connection sees [(1, 20)]\n   other connection sees [(1, 19)]\ncommit\n   same connection sees [(1, 20)]\n   other connection sees [(1, 20)]\ncounting up\n   same connection sees [(1, 21)]\n   other connection sees [(1, 20)]\ncommit\n   same connection sees [(1, 21)]\n   other connection sees [(1, 21)]\n$"
  },
  {
    "url": "https://stackoverflow.com/questions/38649501/labeling-boxplot-in-seaborn-with-median-value",
    "body": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as path_effects\ndef add_median_labels(ax: plt.Axes, fmt: str = \".1f\") -> None:\n    \"\"\"Add text labels to the median lines of a seaborn boxplot.\n    Args:\n        ax: plt.Axes, e.g. the return value of sns.boxplot()\n        fmt: format string for the median value\n    \"\"\"\n    lines = ax.get_lines()\n    boxes = [c for c in ax.get_children() if \"Patch\" in str(c)]\n    start = 4\n    if not boxes:  # seaborn v0.13 => fill=False => no patches => +1 line\n        boxes = [c for c in ax.get_lines() if len(c.get_xdata()) == 5]\n        start += 1\n    lines_per_box = len(lines) // len(boxes)\n    for median in lines[start::lines_per_box]:\n        x, y = (data.mean() for data in median.get_data())\n        # choose value depending on horizontal or vertical plot orientation\n        value = x if len(set(median.get_xdata())) == 1 else y\n        text = ax.text(x, y, f'{value:{fmt}}', ha='center', va='center',\n                       fontweight='bold', color='white')\n        # create median-colored border around white text for contrast\n        text.set_path_effects([\n            path_effects.Stroke(linewidth=3, foreground=median.get_color()),\n            path_effects.Normal(),\n        ])\ntips = sns.load_dataset(\"tips\")\nax = sns.boxplot(data=tips, x='day', y='total_bill', hue=\"sex\")\nadd_median_labels(ax)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/48619517/call-a-click-command-from-code",
    "body": "import click\n@click.command()\n@click.option('-w', '--width', type=int, default=0)\n@click.option('--option2')\n@click.argument('argument')\ndef app(width, option2, argument):\n    click.echo(\"params: {} {} {}\".format(width, option2, argument))\n    assert width == 3\n    assert option2 == '4'\n    assert argument == 'arg'\napp([\"arg\", \"--option2\", \"4\", \"-w\", 3], standalone_mode=False)\napp([\"arg\", \"-w\", 3, \"--option2\", \"4\" ], standalone_mode=False)\napp([\"-w\", 3, \"--option2\", \"4\", \"arg\"], standalone_mode=False)"
  },
  {
    "url": "https://stackoverflow.com/questions/47094949/labeling-edges-in-networkx",
    "body": "import matplotlib.pyplot as plt\nimport networkx as nx\nedges = [['A', 'B'], ['B', 'C'], ['B', 'D']]\nG = nx.Graph()\nG.add_edges_from(edges)\npos = nx.spring_layout(G)\nplt.figure()\nnx.draw(\n    G, pos, edge_color='black', width=1, linewidths=1,\n    node_size=500, node_color='pink', alpha=0.9,\n    labels={node: node for node in G.nodes()}\n)\nnx.draw_networkx_edge_labels(\n    G, pos,\n    edge_labels={('A', 'B'): 'AB',\n                 ('B', 'C'): 'BC',\n                 ('B', 'D'): 'BD'},\n    font_color='red'\n)\nplt.axis('off')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/56637973/how-to-fix-selenium-devtoolsactiveport-file-doesnt-exist-exception-in-python",
    "body": "chromeOptions = webdriver.ChromeOptions()\nchromeOptions.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\nchromeOptions.add_argument(\"--no-sandbox\")\nchromeOptions.add_argument(\"--disable-setuid-sandbox\")\nchromeOptions.add_argument(\"--remote-debugging-port=9222\")  # this\nchromeOptions.add_argument(\"--disable-dev-shm-using\")\nchromeOptions.add_argument(\"--disable-extensions\")\nchromeOptions.add_argument(\"--disable-gpu\")\nchromeOptions.add_argument(\"start-maximized\")\nchromeOptions.add_argument(\"disable-infobars\")\nchromeOptions.add_argument(r\"user-data-dir=.\\cookies\\\\test\")\nb = webdriver.Chrome(chrome_options=chromeOptions)\nb.get(\"https://google.com/\")\nb.quit()"
  },
  {
    "url": "https://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function",
    "body": "/* For numeric types, the hash of a number x is based on the reduction\n   of x modulo the prime P = 2**_PyHASH_BITS - 1.  It's designed so that\n   hash(x) == hash(y) whenever x and y are numerically equal, even if\n   x and y have different types.\n   A quick summary of the hashing strategy:\n   (1) First define the 'reduction of x modulo P' for any rational\n   number x; this is a standard extension of the usual notion of\n   reduction modulo P for integers.  If x == p/q (written in lowest\n   terms), the reduction is interpreted as the reduction of p times\n   the inverse of the reduction of q, all modulo P; if q is exactly\n   divisible by P then define the reduction to be infinity.  So we've\n   got a well-defined map\n      reduce : { rational numbers } -> { 0, 1, 2, ..., P-1, infinity }.\n   (2) Now for a rational number x, define hash(x) by:\n      reduce(x)   if x >= 0\n      -reduce(-x) if x < 0\n   If the result of the reduction is infinity (this is impossible for\n   integers, floats and Decimals) then use the predefined hash value\n   _PyHASH_INF for x >= 0, or -_PyHASH_INF for x < 0, instead.\n   _PyHASH_INF and -_PyHASH_INF are also used for the\n   hashes of float and Decimal infinities.\n   NaNs hash with a pointer hash.  Having distinct hash values prevents\n   catastrophic pileups from distinct NaN instances which used to always\n   have the same hash value but would compare unequal.\n   A selling point for the above strategy is that it makes it possible\n   to compute hashes of decimal and binary floating-point numbers\n   efficiently, even if the exponent of the binary or decimal number\n   is large.  The key point is that\n      reduce(x * y) == reduce(x) * reduce(y) (modulo _PyHASH_MODULUS)\n   provided that {reduce(x), reduce(y)} != {0, infinity}.  The reduction of a\n   binary or decimal float is never infinity, since the denominator is a power\n   of 2 (for binary) or a divisor of a power of 10 (for decimal).  So we have,\n   for nonnegative x,\n      reduce(x * 2**e) == reduce(x) * reduce(2**e) % _PyHASH_MODULUS\n      reduce(x * 10**e) == reduce(x) * reduce(10**e) % _PyHASH_MODULUS\n   and reduce(10**e) can be computed efficiently by the usual modular\n   exponentiation algorithm.  For reduce(2**e) it's even better: since\n   P is of the form 2**n-1, reduce(2**e) is 2**(e mod n), and multiplication\n   by 2**(e mod n) modulo 2**n-1 just amounts to a rotation of bits.\n   */"
  },
  {
    "url": "https://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function",
    "body": "Py_hash_t\n_Py_HashDouble(double v)\n{\n    int e, sign;\n    double m;\n    Py_uhash_t x, y;\n    if (!Py_IS_FINITE(v)) {\n        if (Py_IS_INFINITY(v))\n            return v > 0 ? _PyHASH_INF : -_PyHASH_INF;\n        else\n            return _PyHASH_NAN;\n    }\n    m = frexp(v, &e);\n    sign = 1;\n    if (m < 0) {\n        sign = -1;\n        m = -m;\n    }\n    /* process 28 bits at a time;  this should work well both for binary\n       and hexadecimal floating point. */\n    x = 0;\n    while (m) {\n        x = ((x << 28) & _PyHASH_MODULUS) | x >> (_PyHASH_BITS - 28);\n        m *= 268435456.0;  /* 2**28 */\n        e -= 28;\n        y = (Py_uhash_t)m;  /* pull out integer part */\n        m -= y;\n        x += y;\n        if (x >= _PyHASH_MODULUS)\n            x -= _PyHASH_MODULUS;\n    }\n    /* adjust for the exponent;  first reduce it modulo _PyHASH_BITS */\n    e = e >= 0 ? e % _PyHASH_BITS : _PyHASH_BITS-1-((-1-e) % _PyHASH_BITS);\n    x = ((x << e) & _PyHASH_MODULUS) | x >> (_PyHASH_BITS - e);\n    x = x * sign;\n    if (x == (Py_uhash_t)-1)\n        x = (Py_uhash_t)-2;\n    return (Py_hash_t)x;\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function",
    "body": "Py_hash_t\n_Py_HashBytes(const void *src, Py_ssize_t len)\n{\n    Py_hash_t x;\n    /*\n      We make the hash of the empty string be 0, rather than using\n      (prefix ^ suffix), since this slightly obfuscates the hash secret\n    */\n    if (len == 0) {\n        return 0;\n    }\n#ifdef Py_HASH_STATS\n    hashstats[(len <= Py_HASH_STATS_MAX) ? len : 0]++;\n#endif\n#if Py_HASH_CUTOFF > 0\n    if (len < Py_HASH_CUTOFF) {\n        /* Optimize hashing of very small strings with inline DJBX33A. */\n        Py_uhash_t hash;\n        const unsigned char *p = src;\n        hash = 5381; /* DJBX33A starts with 5381 */\n        switch(len) {\n            /* ((hash << 5) + hash) + *p == hash * 33 + *p */\n            case 7: hash = ((hash << 5) + hash) + *p++; /* fallthrough */\n            case 6: hash = ((hash << 5) + hash) + *p++; /* fallthrough */\n            case 5: hash = ((hash << 5) + hash) + *p++; /* fallthrough */\n            case 4: hash = ((hash << 5) + hash) + *p++; /* fallthrough */\n            case 3: hash = ((hash << 5) + hash) + *p++; /* fallthrough */\n            case 2: hash = ((hash << 5) + hash) + *p++; /* fallthrough */\n            case 1: hash = ((hash << 5) + hash) + *p++; break;\n            default:\n                Py_UNREACHABLE();\n        }\n        hash ^= len;\n        hash ^= (Py_uhash_t) _Py_HashSecret.djbx33a.suffix;\n        x = (Py_hash_t)hash;\n    }\n    else\n#endif /* Py_HASH_CUTOFF */\n        x = PyHash_Func.hash(src, len);\n    if (x == -1)\n        return -2;\n    return x;\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function",
    "body": "#if Py_HASH_ALGORITHM == Py_HASH_FNV\n/* **************************************************************************\n * Modified Fowler-Noll-Vo (FNV) hash function\n */\nstatic Py_hash_t\nfnv(const void *src, Py_ssize_t len)\n{\n    const unsigned char *p = src;\n    Py_uhash_t x;\n    Py_ssize_t remainder, blocks;\n    union {\n        Py_uhash_t value;\n        unsigned char bytes[SIZEOF_PY_UHASH_T];\n    } block;\n#ifdef Py_DEBUG\n    assert(_Py_HashSecret_Initialized);\n#endif\n    remainder = len % SIZEOF_PY_UHASH_T;\n    if (remainder == 0) {\n        /* Process at least one block byte by byte to reduce hash collisions\n         * for strings with common prefixes. */\n        remainder = SIZEOF_PY_UHASH_T;\n    }\n    blocks = (len - remainder) / SIZEOF_PY_UHASH_T;\n    x = (Py_uhash_t) _Py_HashSecret.fnv.prefix;\n    x ^= (Py_uhash_t) *p << 7;\n    while (blocks--) {\n        PY_UHASH_CPY(block.bytes, p);\n        x = (_PyHASH_MULTIPLIER * x) ^ block.value;\n        p += SIZEOF_PY_UHASH_T;\n    }\n    /* add remainder */\n    for (; remainder > 0; remainder--)\n        x = (_PyHASH_MULTIPLIER * x) ^ (Py_uhash_t) *p++;\n    x ^= (Py_uhash_t) len;\n    x ^= (Py_uhash_t) _Py_HashSecret.fnv.suffix;\n    if (x == -1) {\n        x = -2;\n    }\n    return x;\n}\nstatic PyHash_FuncDef PyHash_Func = {fnv, \"fnv\", 8 * SIZEOF_PY_HASH_T,\n                                     16 * SIZEOF_PY_HASH_T};\n#endif /* Py_HASH_ALGORITHM == Py_HASH_FNV */"
  },
  {
    "url": "https://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function",
    "body": "/* byte swap little endian to host endian\n * Endian conversion not only ensures that the hash function returns the same\n * value on all platforms. It is also required to for a good dispersion of\n * the hash values' least significant bits.\n */\n#if PY_LITTLE_ENDIAN\n#  define _le64toh(x) ((uint64_t)(x))\n#elif defined(__APPLE__)\n#  define _le64toh(x) OSSwapLittleToHostInt64(x)\n#elif defined(HAVE_LETOH64)\n#  define _le64toh(x) le64toh(x)\n#else\n#  define _le64toh(x) (((uint64_t)(x) << 56) | \\\n                      (((uint64_t)(x) << 40) & 0xff000000000000ULL) | \\\n                      (((uint64_t)(x) << 24) & 0xff0000000000ULL) | \\\n                      (((uint64_t)(x) << 8)  & 0xff00000000ULL) | \\\n                      (((uint64_t)(x) >> 8)  & 0xff000000ULL) | \\\n                      (((uint64_t)(x) >> 24) & 0xff0000ULL) | \\\n                      (((uint64_t)(x) >> 40) & 0xff00ULL) | \\\n                      ((uint64_t)(x)  >> 56))\n#endif\n#ifdef _MSC_VER\n#  define ROTATE(x, b)  _rotl64(x, b)\n#else\n#  define ROTATE(x, b) (uint64_t)( ((x) << (b)) | ( (x) >> (64 - (b))) )\n#endif\n#define HALF_ROUND(a,b,c,d,s,t)         \\\n    a += b; c += d;             \\\n    b = ROTATE(b, s) ^ a;           \\\n    d = ROTATE(d, t) ^ c;           \\\n    a = ROTATE(a, 32);\n#define DOUBLE_ROUND(v0,v1,v2,v3)       \\\n    HALF_ROUND(v0,v1,v2,v3,13,16);      \\\n    HALF_ROUND(v2,v1,v0,v3,17,21);      \\\n    HALF_ROUND(v0,v1,v2,v3,13,16);      \\\n    HALF_ROUND(v2,v1,v0,v3,17,21);\nstatic uint64_t\nsiphash24(uint64_t k0, uint64_t k1, const void *src, Py_ssize_t src_sz) {\n    uint64_t b = (uint64_t)src_sz << 56;\n    const uint64_t *in = (uint64_t*)src;\n    uint64_t v0 = k0 ^ 0x736f6d6570736575ULL;\n    uint64_t v1 = k1 ^ 0x646f72616e646f6dULL;\n    uint64_t v2 = k0 ^ 0x6c7967656e657261ULL;\n    uint64_t v3 = k1 ^ 0x7465646279746573ULL;\n    uint64_t t;\n    uint8_t *pt;\n    uint8_t *m;\n    while (src_sz >= 8) {\n        uint64_t mi = _le64toh(*in);\n        in += 1;\n        src_sz -= 8;\n        v3 ^= mi;\n        DOUBLE_ROUND(v0,v1,v2,v3);\n        v0 ^= mi;\n    }\n    t = 0;\n    pt = (uint8_t *)&t;\n    m = (uint8_t *)in;\n    switch (src_sz) {\n        case 7: pt[6] = m[6]; /* fall through */\n        case 6: pt[5] = m[5]; /* fall through */\n        case 5: pt[4] = m[4]; /* fall through */\n        case 4: memcpy(pt, m, sizeof(uint32_t)); break;\n        case 3: pt[2] = m[2]; /* fall through */\n        case 2: pt[1] = m[1]; /* fall through */\n        case 1: pt[0] = m[0]; /* fall through */\n    }\n    b |= _le64toh(t);\n    v3 ^= b;\n    DOUBLE_ROUND(v0,v1,v2,v3);\n    v0 ^= b;\n    v2 ^= 0xff;\n    DOUBLE_ROUND(v0,v1,v2,v3);\n    DOUBLE_ROUND(v0,v1,v2,v3);\n    /* modified */\n    t = (v0 ^ v1) ^ (v2 ^ v3);\n    return t;\n}\nstatic Py_hash_t\npysiphash(const void *src, Py_ssize_t src_sz) {\n    return (Py_hash_t)siphash24(\n        _le64toh(_Py_HashSecret.siphash.k0), _le64toh(_Py_HashSecret.siphash.k1),\n        src, src_sz);\n}\nuint64_t\n_Py_KeyedHash(uint64_t key, const void *src, Py_ssize_t src_sz)\n{\n    return siphash24(key, 0, src, src_sz);\n}\n#if Py_HASH_ALGORITHM == Py_HASH_SIPHASH24\nstatic PyHash_FuncDef PyHash_Func = {pysiphash, \"siphash24\", 64, 128};\n#endif"
  },
  {
    "url": "https://stackoverflow.com/questions/2070276/where-can-i-find-source-or-algorithm-of-pythons-hash-function",
    "body": "static Py_hash_t\ntuplehash(PyTupleObject *v)\n{\n    Py_uhash_t x;  /* Unsigned for defined overflow behavior. */\n    Py_hash_t y;\n    Py_ssize_t len = Py_SIZE(v);\n    PyObject **p;\n    Py_uhash_t mult = _PyHASH_MULTIPLIER;\n    x = 0x345678UL;\n    p = v->ob_item;\n    while (--len >= 0) {\n        y = PyObject_Hash(*p++);\n        if (y == -1)\n            return -1;\n        x = (x ^ y) * mult;\n        /* the cast might truncate len; that doesn't change hash stability */\n        mult += (Py_hash_t)(82520UL + len + len);\n    }\n    x += 97531UL;\n    if (x == (Py_uhash_t)-1)\n        x = -2;\n    return x;\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/69786885/after-conda-update-python-kernel-crashes-when-matplotlib-is-used",
    "body": "  2021-10-31 10:47:22  (rev 3)\n     bokeh  {2.3.3 (defaults/win-64) -> 2.4.1 (defaults/win-64)}\n     click  {8.0.1 (defaults/noarch) -> 8.0.3 (defaults/noarch)}\n     filelock  {3.0.12 (defaults/noarch) -> 3.3.1 (defaults/noarch)}\n     freetype  {2.10.4 (defaults/win-64) -> 2.11.0 (defaults/win-64)}\n     imagecodecs  {2021.6.8 (defaults/win-64) -> 2021.8.26 (defaults/win-64)}\n     joblib  {1.0.1 (defaults/noarch) -> 1.1.0 (defaults/noarch)}\n     lerc  {2.2.1 (defaults/win-64) -> 3.0 (defaults/win-64)}\n     more-itertools  {8.8.0 (defaults/noarch) -> 8.10.0 (defaults/noarch)}\n     pyopenssl  {20.0.1 (defaults/noarch) -> 21.0.0 (defaults/noarch)}\n     scikit-learn  {0.24.2 (defaults/win-64) -> 1.0.1 (defaults/win-64)}\n     statsmodels  {0.12.2 (defaults/win-64) -> 0.13.0 (defaults/win-64)}\n     sympy  {1.8 (defaults/win-64) -> 1.9 (defaults/win-64)}\n     tqdm  {4.62.2 (defaults/noarch) -> 4.62.3 (defaults/noarch)}\n     xlwings  {0.24.7 (defaults/win-64) -> 0.24.9 (defaults/win-64)}"
  },
  {
    "url": "https://stackoverflow.com/questions/22029562/python-how-to-make-simple-animated-loading-while-process-is-running",
    "body": "from itertools import cycle\nfrom shutil import get_terminal_size\nfrom threading import Thread\nfrom time import sleep\nclass Loader:\n    def __init__(self, desc=\"Loading...\", end=\"Done!\", timeout=0.1):\n        \"\"\"\n        A loader-like context manager\n        Args:\n            desc (str, optional): The loader's description. Defaults to \"Loading...\".\n            end (str, optional): Final print. Defaults to \"Done!\".\n            timeout (float, optional): Sleep time between prints. Defaults to 0.1.\n        \"\"\"\n        self.desc = desc\n        self.end = end\n        self.timeout = timeout\n        self._thread = Thread(target=self._animate, daemon=True)\n        self.steps = [\"⢿\", \"⣻\", \"⣽\", \"⣾\", \"⣷\", \"⣯\", \"⣟\", \"⡿\"]\n        self.done = False\n    def start(self):\n        self._thread.start()\n        return self\n    def _animate(self):\n        for c in cycle(self.steps):\n            if self.done:\n                break\n            print(f\"\\r{self.desc} {c}\", flush=True, end=\"\")\n            sleep(self.timeout)\n    def __enter__(self):\n        self.start()\n    def stop(self):\n        self.done = True\n        cols = get_terminal_size((80, 20)).columns\n        print(\"\\r\" + \" \" * cols, end=\"\", flush=True)\n        print(f\"\\r{self.end}\", flush=True)\n    def __exit__(self, exc_type, exc_value, tb):\n        # handle exceptions with those variables ^\n        self.stop()\nif __name__ == \"__main__\":\n    with Loader(\"Loading with context manager...\"):\n        for i in range(10):\n            sleep(0.25)\n    loader = Loader(\"Loading with object...\", \"That was fast!\", 0.05).start()\n    for i in range(10):\n        sleep(0.25)\n    loader.stop()"
  },
  {
    "url": "https://stackoverflow.com/questions/17553543/pyserial-non-blocking-read-loop",
    "body": "import serial\nimport time # Optional (required if using time.sleep() below)\nser = serial.Serial(port='COM4', baudrate=9600)\nwhile (True):\n    # Check if incoming bytes are waiting to be read from the serial input\n    # buffer.\n    # NB: for PySerial v3.0 or later, use property `in_waiting` instead of\n    # function `inWaiting()` below!\n    if (ser.inWaiting() > 0):\n        # read the bytes and convert from binary array to ASCII\n        data_str = ser.read(ser.inWaiting()).decode('ascii')\n        # print the incoming string without putting a new-line\n        # ('\\n') automatically after every print()\n        print(data_str, end='')\n    # Put the rest of your code you want here\n\n    # Optional, but recommended: sleep 10 ms (0.01 sec) once per loop to let\n    # other threads on your PC run during this time.\n    time.sleep(0.01)"
  },
  {
    "url": "https://stackoverflow.com/questions/44780736/setting-up-s3-for-logs-in-airflow",
    "body": "# -*- coding: utf-8 -*-\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nfrom airflow.config_templates.airflow_local_settings import DEFAULT_LOGGING_CONFIG\nfrom airflow import configuration as conf\nfrom copy import deepcopy\nS3_LOG_FOLDER = 's3://your/s3/log/folder'\nLOG_LEVEL = conf.get('logging', 'LOGGING_LEVEL').upper()\nLOG_FORMAT = conf.get('logging', 'log_format')\nBASE_LOG_FOLDER = conf.get('logging', 'BASE_LOG_FOLDER')\nPROCESSOR_LOG_FOLDER = conf.get('scheduler', 'child_process_log_directory')\nFILENAME_TEMPLATE = '{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log'\nPROCESSOR_FILENAME_TEMPLATE = '{{ filename }}.log'\nLOGGING_CONFIG = deepcopy(DEFAULT_LOGGING_CONFIG)\n# Attach formatters to loggers (airflow.task, airflow.processor)\nLOGGING_CONFIG['formatters']['airflow.task'] = { 'format': LOG_FORMAT }\nLOGGING_CONFIG['formatters']['airflow.processor'] = { 'format': LOG_FORMAT }\n# Add an S3 task handler\nLOGGING_CONFIG['handlers']['s3.task'] = {\n    'class': 'airflow.providers.amazon.aws.log.s3_task_handler.S3TaskHandler',\n    'formatter': 'airflow.task',\n    'base_log_folder': os.path.expanduser(BASE_LOG_FOLDER),\n    's3_log_folder': S3_LOG_FOLDER,\n    'filename_template': FILENAME_TEMPLATE\n}\n# Specify handler for airflow.task\nLOGGING_CONFIG['loggers']['airflow.task']['handlers'] = ['task', 's3.task']"
  },
  {
    "url": "https://stackoverflow.com/questions/55590343/asyncio-run-or-run-until-complete",
    "body": "import asyncio, sys, types\ndef run(coro):\n    if sys.version_info >= (3, 7):\n        return asyncio.run(coro)\n    # Emulate asyncio.run() on older versions\n    # asyncio.run() requires a coroutine, so require it here as well\n    if not isinstance(coro, types.CoroutineType):\n        raise TypeError(\"run() requires a coroutine object\")\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        return loop.run_until_complete(coro)\n    finally:\n        loop.close()\n        asyncio.set_event_loop(None)"
  },
  {
    "url": "https://stackoverflow.com/questions/63392426/how-to-use-tailwindcss-with-django",
    "body": "#!/bin/sh\nset -e\nTAILWIND_ARCHITECTURE=arm64 # chose the right architecture for you\nTAILWIND_VERSION=v3.1.4 # chose the right version\nSOURCE_NAME=tailwindcss-linux-${TAILWIND_ARCHITECTURE}\nOUTPUT_NAME=tailwindcss\nDOWNLOAD_URL=https://github.com/tailwindlabs/tailwindcss/releases/download/${TAILWIND_VERSION}/${SOURCE_NAME}\ncurl -sLO ${DOWNLOAD_URL} && chmod +x ${SOURCE_NAME}\nmv ${SOURCE_NAME} ${OUTPUT_NAME} # rename it\nmv ${OUTPUT_NAME} /usr/bin # move it to be used globally in a folder already in the PATH var"
  },
  {
    "url": "https://stackoverflow.com/questions/32054066/how-to-run-multiple-coroutines-concurrently-using-asyncio",
    "body": "import asyncio\nasync def factorial(name, number):\n    f = 1\n    for i in range(2, number + 1):\n        print(f\"Task {name}: Compute factorial({i})...\")\n        await asyncio.sleep(1)\n        f *= i\n    print(f\"Task {name}: factorial({number}) = {f}\")\nasync def main():\n    # Schedule three calls *concurrently*:\n    await asyncio.gather(\n        factorial(\"A\", 2),\n        factorial(\"B\", 3),\n        factorial(\"C\", 4),\n    )\nasyncio.run(main())\n# Expected output:\n#\n#     Task A: Compute factorial(2)...\n#     Task B: Compute factorial(2)...\n#     Task C: Compute factorial(2)...\n#     Task A: factorial(2) = 2\n#     Task B: Compute factorial(3)...\n#     Task C: Compute factorial(3)...\n#     Task B: factorial(3) = 6\n#     Task C: Compute factorial(4)...\n#     Task C: factorial(4) = 24"
  },
  {
    "url": "https://stackoverflow.com/questions/54023782/pydantic-make-field-none-in-validator-based-on-other-fields-value",
    "body": ">>> from datetime import date\n>>> from typing import List, Optional\n>>> from pydantic import BaseModel, validator\n>>> class Model(BaseModel):\n        some_list: List[date]\n        some_date: Optional[date]\n\n        @validator(\"some_date\", always=True)\n        def validate_date(cls, value, values):\n            if len(values[\"some_list\"]) < 2:\n                return None\n            return values[\"some_list\"][0]\n>>> Model(some_list=['2019-01-03', '2020-01-03', '2021-01-03'])\nModel(some_list=[datetime.date(2019, 1, 3), datetime.date(2020, 1, 3), datetime.date(2021, 1, 3)],\n      some_date=datetime.date(2019, 1, 3))"
  },
  {
    "url": "https://stackoverflow.com/questions/21213417/select-checkbox-using-selenium-with-python",
    "body": "import time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.common.exceptions import NoSuchElementException\nbrowser = webdriver.Firefox()\nurl = 'http://reverb.echo.nasa.gov/reverb/'\nbrowser.get(url)\nfor i in range(10):\n    try:\n        browser.find_element(By.XPATH,\n            \".//*[contains(text(), '15 Minute Stream Flow Data: USGS (FIFE)')]\"\n        ).click()\n        break\n    except NoSuchElementException as e:\n        print('Retry in 1 second')\n        time.sleep(1)\nelse:\n    raise e"
  },
  {
    "url": "https://stackoverflow.com/questions/58293187/opencv-real-time-streaming-video-capture-is-slow-how-to-drop-frames-or-get-sync",
    "body": "from threading import Thread\nimport cv2, time\nclass ThreadedCamera(object):\n    def __init__(self, src=0):\n        self.capture = cv2.VideoCapture(src)\n        self.capture.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n\n        # FPS = 1/X\n        # X = desired FPS\n        self.FPS = 1/30\n        self.FPS_MS = int(self.FPS * 1000)\n\n        # Start frame retrieval thread\n        self.thread = Thread(target=self.update, args=())\n        self.thread.daemon = True\n        self.thread.start()\n\n    def update(self):\n        while True:\n            if self.capture.isOpened():\n                (self.status, self.frame) = self.capture.read()\n            time.sleep(self.FPS)\n\n    def show_frame(self):\n        cv2.imshow('frame', self.frame)\n        cv2.waitKey(self.FPS_MS)\nif __name__ == '__main__':\n    src = 'https://videos3.earthcam.com/fecnetwork/9974.flv/chunklist_w1421640637.m3u8'\n    threaded_camera = ThreadedCamera(src)\n    while True:\n        try:\n            threaded_camera.show_frame()\n        except AttributeError:\n            pass"
  },
  {
    "url": "https://stackoverflow.com/questions/16650945/merge-on-single-level-of-multiindex",
    "body": "index_left = pd.MultiIndex.from_tuples([('K0', 'X0'), ('K0', 'X1'),\n                                        ('K1', 'X2')],\n                                        names=['key', 'X'])\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                     'B': ['B0', 'B1', 'B2']}, index=index_left)\nindex_right = pd.MultiIndex.from_tuples([('K0', 'Y0'), ('K1', 'Y1'),\n                                        ('K2', 'Y2'), ('K2', 'Y3')],\n                                        names=['key', 'Y'])\nright = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']}, index=index_right)\nleft.join(right)"
  },
  {
    "url": "https://stackoverflow.com/questions/9522877/pythonic-way-to-have-a-choice-of-2-3-options-as-an-argument-to-a-function",
    "body": "from typing import Literal, get_args, get_origin\nfrom sys import _getframe\ndef enforce_literals(function):\n    kwargs = _getframe(1).f_locals\n    for name, type_ in function.__annotations__.items():\n        value = kwargs.get(name)\n        options = get_args(type_)\n        if get_origin(type_) is Literal and name in kwargs and value not in options:\n            raise AssertionError(f\"'{value}' is not in {options} for '{name}'\")\n_TYPES = Literal[\"solar\", \"view\", \"both\"]\n_NUMS = Literal[1, 2, 3, 4, 5]\ndef func(a, b, c, type_: _TYPES = \"solar\", num: _NUMS = 1):\n    enforce_literals(func)\nfunc(1, 2, 3, \"solar\", 6)"
  },
  {
    "url": "https://stackoverflow.com/questions/61693014/how-to-hide-plotly-yaxis-title-in-python",
    "body": "import plotly.graph_objects as go\nfig = go.Figure(\n    data=[go.Bar(y=[2, 1, 3])],\n    layout_title_text=\"A Figure with no axis-title and modified margins\",\n    layout = {\n        'xaxis': {'title': 'x-label',\n                'visible': False,\n                'showticklabels': True},\n        'yaxis': {'title': 'y-label',\n                'visible': False,\n                'showticklabels': False},\n        # specify margins in px\n        'margin': dict(\n            l = 10,        # left\n            r = 10,        # right\n            t = 50,        # top\n            b = 10,        # bottom\n        ),\n    },\n)\nfig"
  },
  {
    "url": "https://stackoverflow.com/questions/61693014/how-to-hide-plotly-yaxis-title-in-python",
    "body": "## ALL THREE METHODS BELOW ARE EQUIVALENT\n# No dict-flattening\n# layout = dict with yaxis as key\nlayout = {'yaxis': {'title': 'y-label',\n                    'visible': False,\n                    'showticklabels': False}\n}\n# Partial dict-flattening\n# layout_yaxis = dict with key-names\n#     title, visible, showticklabels\nlayout_yaxis = {'title': 'y-label',\n                'visible': False,\n                'showticklabels': False}\n# Complete dict-flattening\n# layout_yaxis_key-name for each of the key-names\nlayout_yaxis_title = 'y-label'\nlayout_yaxis_visible = False\nlayout_yaxis_showticklabels = False"
  },
  {
    "url": "https://stackoverflow.com/questions/61693014/how-to-hide-plotly-yaxis-title-in-python",
    "body": "import plotly.graph_objects as go\n# Method-1: Shortest (less detailed)\nfig = go.Figure(\n    data=[go.Bar(y=[2, 1, 3])],\n    layout_title_text=\"A Figure Displaying Itself\",\n    layout_yaxis_visible = False,\n    layout_xaxis_title = 'x-label'\n)\nfig.show()\n# Method-2: A hibrid of dicts and underscore-separated-syntax\nfig = go.Figure(\n    data=[go.Bar(y=[2, 1, 3])],\n    layout_title_text=\"A Figure Displaying Itself\",\n    layout_xaxis_title = 'x-label',\n    layout_yaxis = {'title': 'y-label',\n                        'visible': False,\n                        'showticklabels': False}\n)\nfig.show()\n# Method-3: A complete dict syntax\nfig = go.Figure(\n    data=[go.Bar(y=[2, 1, 3])],\n    layout_title_text=\"A Figure Displaying Itself\",\n    layout = {'xaxis': {'title': 'x-label',\n                        'visible': True,\n                        'showticklabels': True},\n              'yaxis': {'title': 'y-label',\n                        'visible': False,\n                        'showticklabels': False}\n              }\n)\nfig.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/61392633/how-to-validate-more-than-one-field-of-a-pydantic-model",
    "body": "import pydantic\nclass Parent(pydantic.BaseModel):\n    name: str\n    comments: str\nclass Customer(Parent):\n    address: str\n    phone: str\n    # If you want to apply the Validator to the fields \"name\", \"comments\", \"address\", \"phone\"\n    @pydantic.validator(\"name\", \"comments\", \"address\", \"phone\")\n    @classmethod\n    def validate_all_fields_one_by_one(cls, field_value):\n        # Do the validation instead of printing\n        print(f\"{cls}: Field value {field_value}\")\n        return field_value  # this is the value written to the class field\n    # if you want to validate to content of \"phone\" using the other fields of the Parent and Child class\n    @pydantic.validator(\"phone\")\n    @classmethod\n    def validate_one_field_using_the_others(cls, field_value, values, field, config):\n        parent_class_name = values[\"name\"]\n        parent_class_address = values[\"address\"] # works because \"address\" is already validated once we validate \"phone\"\n        # Do the validation instead of printing\n        print(f\"{field_value} is the {field.name} of {parent_class_name}\")\n        return field_value\nCustomer(name=\"Peter\", comments=\"Pydantic User\", address=\"Home\", phone=\"117\")"
  },
  {
    "url": "https://stackoverflow.com/questions/72425408/interrupt-not-prevent-from-starting-screensaver",
    "body": "import win32con, win32api, win32service\nimport random\n# Get a handle to the current active Desktop\nhdesk = win32service.OpenInputDesktop(0, False, win32con.MAXIMUM_ALLOWED);\n# Get a handle to the Desktop this process is associated with\nhdeskOld = win32service.GetThreadDesktop(win32api.GetCurrentThreadId())\n# Set this process to handle messages and input on the active Desktop\nhdesk.SetThreadDesktop()\n# Move the mouse some random amount, most Screen Savers will react to this,\n# close the window, which in turn causes Windows to destroy this Desktop\n# Also, move the mouse a few times to avoid the edge case of moving\n# it randomly to the location it was already at.\nfor _ in range(4):\n    win32api.SetCursorPos((random.randint(0, 100), random.randint(0, 100)))\n# Revert back to the old desktop association so the rest of this script works\nhdeskOld.SetThreadDesktop()"
  },
  {
    "url": "https://stackoverflow.com/questions/72425408/interrupt-not-prevent-from-starting-screensaver",
    "body": "from datetime import datetime, timedelta\nimport ctypes\nimport tkinter as tk\n# Constants for calling SetThreadExecutionState\nES_CONTINUOUS = 0x80000000\nES_SYSTEM_REQUIRED = 0x00000001\nES_DISPLAY_REQUIRED= 0x00000002\n# Example work, show nothing, but when the timer hits, \"alert\" the user\nALERT_AT = datetime.utcnow() + timedelta(minutes=2)\ndef timer(root):\n    # Called every second until we alert the user\n    # TODO: This is just alerting the user after a set time goes by,\n    #       you could perform a custom check here, to see if the user\n    #       should be alerted based off other conditions.\n    if datetime.utcnow() >= ALERT_AT:\n        # Just alert the user\n        root.configure(bg='red')\n    else:\n        # Nothing to do, check again in a bit\n        root.after(1000, timer, root)\n# Create a full screen window\nroot = tk.Tk()\n# Simple way to dismiss the window\nroot.bind(\"<Escape>\", lambda e: e.widget.destroy())\nroot.wm_attributes(\"-fullscreen\", 1)\nroot.wm_attributes(\"-topmost\", 1)\nroot.configure(bg='black')\nroot.config(cursor=\"none\")\nroot.after(1000, timer, root)\n# Disable the screen saver while the main window is shown\nctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS | ES_DISPLAY_REQUIRED)\nroot.mainloop()\n# All done, let the screen saver run again\nctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS)"
  },
  {
    "url": "https://stackoverflow.com/questions/57893902/how-can-i-set-an-attribute-in-a-frozen-dataclass-custom-init-method",
    "body": "def dataclass_with_default_init(_cls=None, *args, **kwargs):\n    def wrap(cls):\n        # Save the current __init__ and remove it so dataclass will\n        # create the default __init__.\n        user_init = getattr(cls, \"__init__\")\n        delattr(cls, \"__init__\")\n        # let dataclass process our class.\n        result = dataclass(cls, *args, **kwargs)\n        # Restore the user's __init__ save the default init to __default_init__.\n        setattr(result, \"__default_init__\", result.__init__)\n        setattr(result, \"__init__\", user_init)\n        # Just in case that dataclass will return a new instance,\n        # (currently, does not happen), restore cls's __init__.\n        if result is not cls:\n            setattr(cls, \"__init__\", user_init)\n        return result\n    # Support both dataclass_with_default_init() and dataclass_with_default_init\n    if _cls is None:\n        return wrap\n    else:\n        return wrap(_cls)"
  },
  {
    "url": "https://stackoverflow.com/questions/1877999/delete-final-line-in-file-with-python",
    "body": "with open(sys.argv[1], \"r+\", encoding = \"utf-8\") as file:\n\n    # Move the pointer (similar to a cursor in a text editor) to the end of the file\n    file.seek(0, os.SEEK_END)\n\n    # This code means the following code skips the very last character in the file -\n    # i.e. in the case the last line is null we delete the last line\n    # and the penultimate one\n    pos = file.tell() - 1\n\n    # Read each character in the file one at a time from the penultimate\n    # character going backwards, searching for a newline character\n    # If we find a new line, exit the search\n    while pos > 0 and file.read(1) != \"\\n\":\n        pos -= 1\n        file.seek(pos, os.SEEK_SET)\n\n    # So long as we're not at the start of the file, delete all the characters ahead\n    # of this position\n    if pos > 0:\n        file.seek(pos, os.SEEK_SET)\n        file.truncate()"
  },
  {
    "url": "https://stackoverflow.com/questions/67639234/count-how-many-arguments-passed-as-positional",
    "body": "from functools import wraps\nfrom inspect import signature\ndef checkargs(func):\n    @wraps(func)\n    def inner(*args, **kwargs):\n        for param in signature(func).parameters:\n            if param in kwargs:\n                print(param, 'passed with its keyword!')\n            else:\n                print(param, 'passed positionally.')\n        result = func(*args, **kwargs)\n        return result\n    return inner\n>>>  @checkargs\n...: def foo(x, y, z) -> int:\n...:     return x + y\n>>> foo(2, 3, z=4)\nx passed positionally.\ny passed positionally.\nz passed with its keyword!\n9\n>>> inspect.getfullargspec(foo)\nFullArgSpec(args=[], varargs='args', varkw='kwargs', defaults=None,\nkwonlyargs=[], kwonlydefaults=None, annotations={'return': <class 'int'>})\n                                             _____________HERE____________"
  },
  {
    "url": "https://stackoverflow.com/questions/67639234/count-how-many-arguments-passed-as-positional",
    "body": "from functools import wraps\nfrom inspect import signature\nfrom typing import Callable, ParamSpec, TypeVar, TYPE_CHECKING\nT = TypeVar(\"T\")\nP = ParamSpec(\"P\")\ndef check_args(func: Callable[P, T]) -> Callable[P, T]:\n    \"\"\"\n    Decorator to monitor whether an argument is passed\n    positionally or with its keyword, during function call.\n    \"\"\"\n    @wraps(func)\n    def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n        for param in signature(func).parameters:\n            if param in kwargs:\n                print(param, 'passed with its keyword!')\n            else:\n                print(param, 'passed positionally.')\n        return func(*args, **kwargs)\n    return inner"
  },
  {
    "url": "https://stackoverflow.com/questions/11927278/how-to-configure-logging-in-python",
    "body": "# app.py (runs when application starts)\nimport logging\nimport os.path\ndef main():\n    logging_config = {\n        'version': 1,\n        'disable_existing_loggers': False,\n        'formatters': {\n            'standard': {\n                'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n            },\n        },\n        'handlers': {\n            'default_handler': {\n                'class': 'logging.FileHandler',\n                'level': 'DEBUG',\n                'formatter': 'standard',\n                'filename': os.path.join('logs', 'application.log'),\n                'encoding': 'utf8'\n            },\n        },\n        'loggers': {\n            '': {\n                'handlers': ['default_handler'],\n                'level': 'DEBUG',\n                'propagate': False\n            }\n        }\n    }\n    logging.config.dictConfig(logging_config)\n    # start application ...\nif __name__ == '__main__':\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/47776486/python-struct-error-i-format-requires-2147483648-number-2147483647",
    "body": "import functools\nimport logging\nimport struct\nimport sys\nlogger = logging.getLogger()\ndef patch_mp_connection_bpo_17560():\n    \"\"\"Apply PR-10305 / bpo-17560 connection send/receive max size update\n    See the original issue at https://bugs.python.org/issue17560 and\n    https://github.com/python/cpython/pull/10305 for the pull request.\n    This only supports Python versions 3.3 - 3.7, this function\n    does nothing for Python versions outside of that range.\n    \"\"\"\n    patchname = \"Multiprocessing connection patch for bpo-17560\"\n    if not (3, 3) < sys.version_info < (3, 8):\n        logger.info(\n            patchname + \" not applied, not an applicable Python version: %s\",\n            sys.version\n        )\n        return\n    from multiprocessing.connection import Connection\n    orig_send_bytes = Connection._send_bytes\n    orig_recv_bytes = Connection._recv_bytes\n    if (\n        orig_send_bytes.__code__.co_filename == __file__\n        and orig_recv_bytes.__code__.co_filename == __file__\n    ):\n        logger.info(patchname + \" already applied, skipping\")\n        return\n    @functools.wraps(orig_send_bytes)\n    def send_bytes(self, buf):\n        n = len(buf)\n        if n > 0x7fffffff:\n            pre_header = struct.pack(\"!i\", -1)\n            header = struct.pack(\"!Q\", n)\n            self._send(pre_header)\n            self._send(header)\n            self._send(buf)\n        else:\n            orig_send_bytes(self, buf)\n    @functools.wraps(orig_recv_bytes)\n    def recv_bytes(self, maxsize=None):\n        buf = self._recv(4)\n        size, = struct.unpack(\"!i\", buf.getvalue())\n        if size == -1:\n            buf = self._recv(8)\n            size, = struct.unpack(\"!Q\", buf.getvalue())\n        if maxsize is not None and size > maxsize:\n            return None\n        return self._recv(size)\n    Connection._send_bytes = send_bytes\n    Connection._recv_bytes = recv_bytes\n    logger.info(patchname + \" applied\")"
  },
  {
    "url": "https://stackoverflow.com/questions/58280484/ssl-module-in-python-is-not-available-on-osx",
    "body": "Mac-Admin:~ admin$ python3\nPython 3.7.4 (default, Sep  7 2019, 18:27:02)\n[Clang 10.0.1 (clang-1001.0.46.4)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import ssl\n>>> ssl\n<module 'ssl' from '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py'>\n>>> import _ssl\n>>> _ssl\n<module '_ssl' from '/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload/_ssl.cpython-37m-darwin.so'>"
  },
  {
    "url": "https://stackoverflow.com/questions/11665628/read-data-from-csv-file-and-transform-from-string-to-correct-data-type-includin",
    "body": "#!/usr/bin/env python3.6\nimport ast\nimport csv\nfrom typing import NamedTuple\nclass Record(NamedTuple):\n    \"\"\" Define the fields and their types in a record. \"\"\"\n    IsActive: bool\n    Type: str\n    Price: float\n    States: ast.literal_eval  # Handles string represenation of literals.\n    @classmethod\n    def _transform(cls: 'Record', dict_: dict) -> dict:\n        \"\"\" Convert string values in given dictionary to corresponding Record\n            field type.\n        \"\"\"\n        return {name: cls.__annotations__[name](value)\n                    for name, value in dict_.items()}\nfilename = 'test_transform.csv'\nwith open(filename, newline='') as file:\n    for i, row in enumerate(csv.DictReader(file)):\n        row = Record._transform(row)\n        print(f'row {i}: {row}')"
  },
  {
    "url": "https://stackoverflow.com/questions/11665628/read-data-from-csv-file-and-transform-from-string-to-correct-data-type-includin",
    "body": "#!/usr/bin/env python3.7\nimport ast\nimport csv\nfrom dataclasses import dataclass, fields\nfrom typing import Type, TypeVar\nT = TypeVar('T', bound='GenericRecord')\nclass GenericRecord:\n    \"\"\" Generic base class for transforming dataclasses. \"\"\"\n    @classmethod\n    def _transform(cls: Type[T], dict_: dict) -> dict:\n        \"\"\" Convert string values in given dictionary to corresponding type. \"\"\"\n        return {field.name: field.type(dict_[field.name])\n                    for field in fields(cls)}\n@dataclass\nclass CSV_Record(GenericRecord):\n    \"\"\" Define the fields and their types in a record.\n        Field names must match column names in CSV file header.\n    \"\"\"\n    IsActive: bool\n    Type: str\n    Price: float\n    States: ast.literal_eval  # Handles string represenation of literals.\nfilename = 'test_transform.csv'\nwith open(filename, newline='') as file:\n    for i, row in enumerate(csv.DictReader(file)):\n        row = CSV_Record._transform(row)\n        print(f'row {i}: {row}')"
  },
  {
    "url": "https://stackoverflow.com/questions/11665628/read-data-from-csv-file-and-transform-from-string-to-correct-data-type-includin",
    "body": "#!/usr/bin/env python3.8\nimport ast\nimport csv\nfrom dataclasses import dataclass, fields\nfrom typing import TypedDict\ndef transform(dict_, typed_dict) -> dict:\n    \"\"\" Convert values in given dictionary to corresponding types in TypedDict . \"\"\"\n    fields = typed_dict.__annotations__\n    return {name: fields[name](value) for name, value in dict_.items()}\nclass CSV_Record_Types(TypedDict):\n    \"\"\" Define the fields and their types in a record.\n        Field names must match column names in CSV file header.\n    \"\"\"\n    IsActive: bool\n    Type: str\n    Price: float\n    States: ast.literal_eval\nfilename = 'test_transform.csv'\nwith open(filename, newline='') as file:\n    for i, row in enumerate(csv.DictReader(file), 1):\n        row = transform(row, CSV_Record_Types)\n        print(f'row {i}: {row}')"
  },
  {
    "url": "https://stackoverflow.com/questions/75740652/fastapi-streamingresponse-not-streaming-with-generator-function",
    "body": "from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport asyncio\napp = FastAPI()\nasync def fake_data_streamer():\n    for i in range(10):\n        yield b'some fake data\\n\\n'\n        await asyncio.sleep(0.5)\n# If your generator contains blocking operations such as time.sleep(), then define the\n# generator function with normal `def`. Alternatively, use `async def` and run any\n# blocking operations in an external ThreadPool/ProcessPool. (see 2nd paragraph of this answer)\n'''\nimport time\ndef fake_data_streamer():\n    for i in range(10):\n        yield b'some fake data\\n\\n'\n        time.sleep(0.5)\n'''\n\n@app.get('/')\nasync def main():\n    return StreamingResponse(fake_data_streamer(), media_type='text/event-stream')\n    # or, use:\n    '''\n    headers = {'X-Content-Type-Options': 'nosniff'}\n    return StreamingResponse(fake_data_streamer(), headers=headers, media_type='text/plain')\n    '''"
  },
  {
    "url": "https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%c3%a9rs-coefficient-matrix",
    "body": "import scipy.stats as ss\ndef cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher,\n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))"
  },
  {
    "url": "https://stackoverflow.com/questions/63682956/fastapi-retrieve-url-from-view-name-route-name",
    "body": "from fastapi import FastAPI, Request\napp = FastAPI()\n@app.get('/hello/')\ndef hello_world():\n    return {\"msg\": \"Hello World\"}\n@app.get('/hello/{number}/')\ndef hello_world_number(number: int):\n    return {\"msg\": \"Hello World Number\", \"number\": number}\n@app.get('/')\ndef named_url_reveres(request: Request):\n    return {\n        \"URL for 'hello_world'\": request.url_for(\"hello_world\"),\n        \"URL for 'hello_world_number' with number '1'\": request.url_for(\"hello_world_number\", number=1),\n        \"URL for 'hello_world_number' with number '2''\": request.url_for(\"hello_world_number\", number=2})\n    }\n# Result Response\n{\n    \"URL for 'hello_world'\": \"http://0.0.0.0:6022/hello/\",\n    \"URL for 'hello_world_number' with number '1'\": \"http://0.0.0.0:6022/hello/1/\",\n    \"URL for 'hello_world_number' with number '2''\": \"http://0.0.0.0:6022/hello/2/\"\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/5824382/enabling-comparison-for-classes",
    "body": "import functools\n@functools.total_ordering\nclass Student:\n    def _is_valid_operand(self, other):\n        return (hasattr(other, \"lastname\") and\n                hasattr(other, \"firstname\"))\n    def __eq__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return ((self.lastname.lower(), self.firstname.lower()) ==\n                (other.lastname.lower(), other.firstname.lower()))\n    def __lt__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return ((self.lastname.lower(), self.firstname.lower()) <\n                (other.lastname.lower(), other.firstname.lower()))"
  },
  {
    "url": "https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch",
    "body": "import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _WeightedLoss\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1, weight = None):\n        \"\"\"if smoothing == 0, it's one-hot method\n           if 0 < smoothing < 1, it's smooth method\n        \"\"\"\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.weight = weight\n        self.cls = classes\n        self.dim = dim\n    def forward(self, pred, target):\n        assert 0 <= self.smoothing < 1\n        pred = pred.log_softmax(dim=self.dim)\n        if self.weight is not None:\n            pred = pred * self.weight.unsqueeze(0)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
  },
  {
    "url": "https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch",
    "body": "class SmoothCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n    def k_one_hot(self, targets:torch.Tensor, n_classes:int, smoothing=0.0):\n        with torch.no_grad():\n            targets = torch.empty(size=(targets.size(0), n_classes),\n                                  device=targets.device) \\\n                                  .fill_(smoothing /(n_classes-1)) \\\n                                  .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n        return targets\n    def reduce_loss(self, loss):\n        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n        if self.reduction == 'sum' else loss\n    def forward(self, inputs, targets):\n        assert 0 <= self.smoothing < 1\n        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n        log_preds = F.log_softmax(inputs, -1)\n        if self.weight is not None:\n            log_preds = log_preds * self.weight.unsqueeze(0)\n        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))"
  },
  {
    "url": "https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch",
    "body": "import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _WeightedLoss\nif __name__==\"__main__\":\n    # 1. Devin Yang\n    crit = LabelSmoothingLoss(classes=5, smoothing=0.5)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1],\n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n    # 2. Shital Shah\n    crit = SmoothCrossEntropyLoss(smoothing=0.5)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1],\n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\ntensor(1.4178)\ntensor(1.4178)"
  },
  {
    "url": "https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch",
    "body": "class LabelSmoothingLoss(torch.nn.Module):\n    def __init__(self, smoothing: float = 0.1,\n                 reduction=\"mean\", weight=None):\n        super(LabelSmoothingLoss, self).__init__()\n        self.smoothing   = smoothing\n        self.reduction = reduction\n        self.weight    = weight\n    def reduce_loss(self, loss):\n        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n         if self.reduction == 'sum' else loss\n    def linear_combination(self, x, y):\n        return self.smoothing * x + (1 - self.smoothing) * y\n    def forward(self, preds, target):\n        assert 0 <= self.smoothing < 1\n        if self.weight is not None:\n            self.weight = self.weight.to(preds.device)\n        n = preds.size(-1)\n        log_preds = F.log_softmax(preds, dim=-1)\n        loss = self.reduce_loss(-log_preds.sum(dim=-1))\n        nll = F.nll_loss(\n            log_preds, target, reduction=self.reduction, weight=self.weight\n        )\n        return self.linear_combination(loss / n, nll)"
  },
  {
    "url": "https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch",
    "body": "class LabelSmoothing(nn.Module):\n    \"\"\"NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        \"\"\"Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n    def forward(self, x, target):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()"
  },
  {
    "url": "https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch",
    "body": "if __name__==\"__main__\":\n    # Wangleiofficial\n    crit = LabelSmoothingLoss(smoothing=0.3, reduction=\"mean\")\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1],\n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n    # NVIDIA\n    crit = LabelSmoothing(smoothing=0.3)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1],\n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\ntensor(1.3883)\ntensor(1.3883)"
  },
  {
    "url": "https://stackoverflow.com/questions/59499061/how-to-run-custom-shell-script-file-before-pre-commit-hook",
    "body": "- repo: local\n  hooks:\n    - id: simple-pylint\n      name: simple-pylint\n      entry: pylint\n      args: [\"api/\"]\n      language: system\n      types: [python]\n      pass_filenames: false\n    - id: inline-pylint-with-bash\n      name: inline-pylint-with-bash\n      entry: bash -c 'lines=$(pylint api/ | wc -l) && (( lines > 10)) && exit 1'\n      language: system\n      types: [python]\n      pass_filenames: false\n\n    - id: custom-script-file\n      name: custom-script-file\n      entry: relative/path/to/repo/root/check_pylint.sh\n      language: script\n      types: [python]\n      pass_filenames: false"
  },
  {
    "url": "https://stackoverflow.com/questions/21808657/what-is-a-unicode-string",
    "body": ">>> my_str = 'A unicode \\u018e string \\xf1' # no need for \"u\" prefix\n# the escape sequence \"\\u\" denotes a Unicode code point (in hex)\n>>> my_str\n'A unicode Ǝ string ñ'\n# the Unicode code points U+018E and U+00F1 were displayed\n# as their corresponding glyphs\n>>> my_bytes = my_str.encode('utf-8') # convert to a bytes object\n>>> my_bytes\nb'A unicode \\xc6\\x8e string \\xc3\\xb1'\n# the \"b\" prefix means a bytes literal\n# the escape sequence \"\\x\" denotes a byte using its hex value\n# the code points U+018E and U+00F1 were encoded as 2-byte sequences\n>>> my_str2 = my_bytes.decode('utf-8') # convert back to str\n>>> my_str2 == my_str\nTrue"
  },
  {
    "url": "https://stackoverflow.com/questions/52413246/how-to-provide-a-reproducible-copy-of-your-dataframe-with-to-clipboard",
    "body": "# if you have a datetime column, convert it to a str\ndf['date'] = df['date'].astype('str')\n# if you have a datetime index, convert it to a str\ndf.index = df.index.astype('str')\n# output to a dict\ndf.head(10).to_dict(orient='index')\n# which will look like\n{'2020-07-30': {'a': 2, 'b': 4},\n '2020-07-31': {'a': 1, 'b': 5},\n '2020-08-01': {'a': 2, 'b': 2},\n '2020-08-02': {'a': 9, 'b': 8},\n '2020-08-03': {'a': 4, 'b': 0},\n '2020-08-04': {'a': 3, 'b': 3},\n '2020-08-05': {'a': 7, 'b': 7},\n '2020-08-06': {'a': 7, 'b': 0},\n '2020-08-07': {'a': 8, 'b': 4},\n '2020-08-08': {'a': 3, 'b': 2}}\n# copy the previous dict and paste into a code block on SO\n# the dict can be converted to a dataframe with\n# df = pd.DataFrame.from_dict(d, orient='index')  # d is the name of the dict\n# convert datatime column or index back to datetime"
  },
  {
    "url": "https://stackoverflow.com/questions/39614027/list-available-font-families-in-tkinter",
    "body": "System\nTerminal\nFixedsys\nModern\nRoman\nScript\nCourier\nMS Serif\nMS Sans Serif\nSmall Fonts\nBell Gothic Std Black\nBell Gothic Std Light\nEccentric Std\nStencil Std\nTekton Pro\nTekton Pro Cond\nTekton Pro Ext\nTrajan Pro\nRosewood Std Regular\nPrestige Elite Std\nPoplar Std\nOrator Std\nOCR A Std\nNueva Std Cond\nMinion Pro SmBd\nMinion Pro Med\nMinion Pro Cond\nMesquite Std\nLithos Pro Regular\nKozuka Mincho Pro R\n@Kozuka Mincho Pro R\nKozuka Mincho Pro M\n@Kozuka Mincho Pro M\nKozuka Mincho Pro L\n@Kozuka Mincho Pro L\nKozuka Mincho Pro H\n@Kozuka Mincho Pro H\nKozuka Mincho Pro EL\n@Kozuka Mincho Pro EL\nKozuka Mincho Pro B\n@Kozuka Mincho Pro B\nKozuka Gothic Pro R\n@Kozuka Gothic Pro R\nKozuka Gothic Pro M\n@Kozuka Gothic Pro M\nKozuka Gothic Pro L\n@Kozuka Gothic Pro L\nKozuka Gothic Pro H\n@Kozuka Gothic Pro H\nKozuka Gothic Pro EL\n@Kozuka Gothic Pro EL\nKozuka Gothic Pro B\n@Kozuka Gothic Pro B\nHobo Std\nGiddyup Std\nCooper Std Black\nCharlemagne Std\nChaparral Pro\nBrush Script Std\nBlackoak Std\nBirch Std\nAdobe Garamond Pro\nAdobe Garamond Pro Bold\nAdobe Kaiti Std R\n@Adobe Kaiti Std R\nAdobe Heiti Std R\n@Adobe Heiti Std R\nAdobe Fangsong Std R\n@Adobe Fangsong Std R\nAdobe Caslon Pro\nAdobe Caslon Pro Bold\nAdobe Arabic\nAdobe Devanagari\nAdobe Hebrew\nAdobe Ming Std L\n@Adobe Ming Std L\nAdobe Myungjo Std M\n@Adobe Myungjo Std M\nAdobe Song Std L\n@Adobe Song Std L\nKozuka Gothic Pr6N B\n@Kozuka Gothic Pr6N B\nKozuka Gothic Pr6N EL\n@Kozuka Gothic Pr6N EL\nKozuka Gothic Pr6N H\n@Kozuka Gothic Pr6N H\nKozuka Gothic Pr6N L\n@Kozuka Gothic Pr6N L\nKozuka Gothic Pr6N M\n@Kozuka Gothic Pr6N M\nKozuka Gothic Pr6N R\n@Kozuka Gothic Pr6N R\nKozuka Mincho Pr6N B\n@Kozuka Mincho Pr6N B\nKozuka Mincho Pr6N EL\n@Kozuka Mincho Pr6N EL\nKozuka Mincho Pr6N H\n@Kozuka Mincho Pr6N H\nKozuka Mincho Pr6N L\n@Kozuka Mincho Pr6N L\nKozuka Mincho Pr6N M\n@Kozuka Mincho Pr6N M\nKozuka Mincho Pr6N R\n@Kozuka Mincho Pr6N R\nLetter Gothic Std\nMinion Pro\nMyriad Hebrew\nMyriad Pro\nMyriad Pro Cond\nMyriad Pro Light\nRosewood Std Fill\nMarlett\nArial\nArabic Transparent\nArial Baltic\nArial CE\nArial CYR\nArial Greek\nArial TUR\nBatang\n@Batang\nBatangChe\n@BatangChe\nGungsuh\n@Gungsuh\nGungsuhChe\n@GungsuhChe\nCourier New\nCourier New Baltic\nCourier New CE\nCourier New CYR\nCourier New Greek\nCourier New TUR\nDaunPenh\nDokChampa\nEstrangelo Edessa\nEuphemia\nGautami\nVani\nGulim\n@Gulim\nGulimChe\n@GulimChe\nDotum\n@Dotum\nDotumChe\n@DotumChe\nImpact\nIskoola Pota\nKalinga\nKartika\nKhmer UI\nLao UI\nLatha\nLucida Console\nMalgun Gothic\n@Malgun Gothic\nMangal\nMeiryo\n@Meiryo\nMeiryo UI\n@Meiryo UI\nMicrosoft Himalaya\nMicrosoft JhengHei\n@Microsoft JhengHei\nMicrosoft YaHei\n@Microsoft YaHei\nMingLiU\n@MingLiU\nPMingLiU\n@PMingLiU\nMingLiU_HKSCS\n@MingLiU_HKSCS\nMingLiU-ExtB\n@MingLiU-ExtB\nPMingLiU-ExtB\n@PMingLiU-ExtB\nMingLiU_HKSCS-ExtB\n@MingLiU_HKSCS-ExtB\nMongolian Baiti\nMS Gothic\n@MS Gothic\nMS PGothic\n@MS PGothic\nMS UI Gothic\n@MS UI Gothic\nMS Mincho\n@MS Mincho\nMS PMincho\n@MS PMincho\nMV Boli\nMicrosoft New Tai Lue\nNyala\nMicrosoft PhagsPa\nPlantagenet Cherokee\nRaavi\nSegoe Script\nSegoe UI\nSegoe UI Semibold\nSegoe UI Light\nSegoe UI Symbol\nShruti\nSimSun\n@SimSun\nNSimSun\n@NSimSun\nSimSun-ExtB\n@SimSun-ExtB\nSylfaen\nMicrosoft Tai Le\nTimes New Roman\nTimes New Roman Baltic\nTimes New Roman CE\nTimes New Roman CYR\nTimes New Roman Greek\nTimes New Roman TUR\nTunga\nVrinda\nShonar Bangla\nMicrosoft Yi Baiti\nTahoma\nMicrosoft Sans Serif\nAngsana New\nAparajita\nCordia New\nEbrima\nGisha\nKokila\nLeelawadee\nMicrosoft Uighur\nMoolBoran\nSymbol\nUtsaah\nVijaya\nWingdings\nAndalus\nArabic Typesetting\nSimplified Arabic\nSimplified Arabic Fixed\nSakkal Majalla\nTraditional Arabic\nAharoni\nDavid\nFrankRuehl\nLevenim MT\nMiriam\nMiriam Fixed\nNarkisim\nRod\nFangSong\n@FangSong\nSimHei\n@SimHei\nKaiTi\n@KaiTi\nAngsanaUPC\nBrowallia New\nBrowalliaUPC\nCordiaUPC\nDilleniaUPC\nEucrosiaUPC\nFreesiaUPC\nIrisUPC\nJasmineUPC\nKodchiangUPC\nLilyUPC\nDFKai-SB\n@DFKai-SB\nLucida Sans Unicode\nArial Black\nCalibri\nCambria\nCambria Math\nCandara\nComic Sans MS\nConsolas\nConstantia\nCorbel\nFranklin Gothic Medium\nGabriola\nGeorgia\nPalatino Linotype\nSegoe Print\nTrebuchet MS\nVerdana\nWebdings\nHaettenschweiler\nMS Outlook\nBook Antiqua\nCentury Gothic\nBookshelf Symbol 7\nMS Reference Sans Serif\nMS Reference Specialty\nBradley Hand ITC\nFreestyle Script\nFrench Script MT\nJuice ITC\nKristen ITC\nLucida Handwriting\nMistral\nPapyrus\nPristina\nTempus Sans ITC\nGaramond\nMonotype Corsiva\nAgency FB\nArial Rounded MT Bold\nBlackadder ITC\nBodoni MT\nBodoni MT Black\nBodoni MT Condensed\nBookman Old Style\nCalisto MT\nCastellar\nCentury Schoolbook\nCopperplate Gothic Bold\nCopperplate Gothic Light\nCurlz MT\nEdwardian Script ITC\nElephant\nEngravers MT\nEras Bold ITC\nEras Demi ITC\nEras Light ITC\nEras Medium ITC\nFelix Titling\nForte\nFranklin Gothic Book\nFranklin Gothic Demi\nFranklin Gothic Demi Cond\nFranklin Gothic Heavy\nFranklin Gothic Medium Cond\nGigi\nGill Sans MT\nGill Sans MT Condensed\nGill Sans Ultra Bold\nGill Sans Ultra Bold Condensed\nGill Sans MT Ext Condensed Bold\nGloucester MT Extra Condensed\nGoudy Old Style\nGoudy Stout\nImprint MT Shadow\nLucida Sans\nLucida Sans Typewriter\nMaiandra GD\nOCR A Extended\nPalace Script MT\nPerpetua\nPerpetua Titling MT\nRage Italic\nRockwell\nRockwell Condensed\nRockwell Extra Bold\nScript MT Bold\nTw Cen MT\nTw Cen MT Condensed\nTw Cen MT Condensed Extra Bold\nAlgerian\nBaskerville Old Face\nBauhaus 93\nBell MT\nBerlin Sans FB\nBerlin Sans FB Demi\nBernard MT Condensed\nBodoni MT Poster Compressed\nBritannic Bold\nBroadway\nBrush Script MT\nCalifornian FB\nCentaur\nChiller\nColonna MT\nCooper Black\nFootlight MT Light\nHarlow Solid Italic\nHarrington\nHigh Tower Text\nJokerman\nKunstler Script\nLucida Bright\nLucida Calligraphy\nLucida Fax\nMagneto\nMatura MT Script Capitals\nModern No. 20\nNiagara Engraved\nNiagara Solid\nOld English Text MT\nOnyx\nParchment\nPlaybill\nPoor Richard\nRavie\nInformal Roman\nShowcard Gothic\nSnap ITC\nStencil\nViner Hand ITC\nVivaldi\nVladimir Script\nWide Latin\nCentury\nWingdings 2\nWingdings 3\nArial Unicode MS\n@Arial Unicode MS\nArial Narrow\nRupee Foradian\nRupee\nDevLys 010\nCalibri Light\nMonoton\nUbuntu Medium\nUbuntu\nUbuntu Light\nYatra One\nHelvLight\nLato\nGreat Vibes"
  },
  {
    "url": "https://stackoverflow.com/questions/8763451/how-to-handle-urllibs-timeout-in-python-3",
    "body": "from socket import timeout\nfrom urllib.error import HTTPError, URLError\ntry:\n    response = urllib.request.urlopen(url, timeout=10).read().decode('utf-8')\nexcept HTTPError as error:\n    logging.error('HTTP Error: Data of %s not retrieved because %s\\nURL: %s', name, error, url)\nexcept URLError as error:\n    if isinstance(error.reason, timeout):\n        logging.error('Timeout Error: Data of %s not retrieved because %s\\nURL: %s', name, error, url)\n    else:\n        logging.error('URL Error: Data of %s not retrieved because %s\\nURL: %s', name, error, url)\nelse:\n    logging.info('Access successful.')"
  },
  {
    "url": "https://stackoverflow.com/questions/65316863/is-asyncio-to-thread-method-different-to-threadpoolexecutor",
    "body": "async def to_thread(func, /, *args, **kwargs):\n    \"\"\"Asynchronously run function *func* in a separate thread.\n    Any *args and **kwargs supplied for this function are directly passed\n    to *func*. Also, the current :class:`contextvars.Context` is propogated,\n    allowing context variables from the main thread to be accessed in the\n    separate thread.\n    Return a coroutine that can be awaited to get the eventual result of *func*.\n    \"\"\"\n    loop = events.get_running_loop()\n    ctx = contextvars.copy_context()\n    func_call = functools.partial(ctx.run, func, *args, **kwargs)\n    return await loop.run_in_executor(None, func_call)"
  },
  {
    "url": "https://stackoverflow.com/questions/25692440/mocking-a-subprocess-call-in-python",
    "body": "from unittest import mock\nimport subprocess\ndef run_script(file_path):\n    process = subprocess.Popen([\"myscript\", -M, file_path], stdout=subprocess.PIPE)\n    output, err = process.communicate()\n    return process.returncode\n@mock.patch(\"subprocess.Popen\")\ndef test_run_script(self, mock_subproc_popen):\n    process_mock = mock.Mock()\n    attrs = {\"communicate.return_value\": (\"output\", \"error\")}\n    process_mock.configure_mock(**attrs)\n    mock_subproc_popen.return_value = process_mock\n    am.account_manager(\"path\")  # this calls run_script somewhere, is that right?\n    self.assertTrue(mock_subproc_popen.called)"
  },
  {
    "url": "https://stackoverflow.com/questions/74289077/attributeerror-multiprocessingdataloaderiter-object-has-no-attribute-next",
    "body": "class WineDataset(Dataset):\n    def __init__(self):\n        # Initialize data, download, etc.\n        # read with numpy or pandas\n        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n        self.n_samples = xy.shape[0]\n        # here the first column is the class label, the rest are the features\n        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n    # support indexing such that dataset[i] can be used to get i-th sample\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n    # we can call len(dataset) to return the size\n    def __len__(self):\n        return self.n_samples\ndataset = WineDataset()\ndataloader = DataLoader(dataset=dataset,\n                              batch_size=4,\n                              shuffle=True,\n                              num_workers=2)\ndataiter = iter(dataloader)\ndata = next(dataiter)"
  },
  {
    "url": "https://stackoverflow.com/questions/55271912/flask-cli-throws-oserror-errno-8-exec-format-error-when-run-through-docker",
    "body": ">    [cfati@cfati-5510-0:/cygdrive/e/Work/Dev/StackOverflow/q055271912]> ~/sopr.sh\n>    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n>\n>    [064bit prompt]> ls\n>    code00.py  code01.py\n>    [064bit prompt]>\n>    [064bit prompt]> cat code00.py\n>    print(\"This is:\", __file__)\n>\n>    [064bit prompt]> python3 -c \"import os, subprocess;subprocess.Popen(os.path.join(os.getcwd(), \\\"code00.py\\\")).communicate()\"\n>    Traceback (most recent call last):\n>      File \"<string>\", line 1, in <module>\n>      File \"/usr/lib/python3.6/subprocess.py\", line 709, in __init__\n>        restore_signals, start_new_session)\n>      File \"/usr/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n>        raise child_exception_type(errno_num, err_msg, err_filename)\n>    OSError: [Errno 8] Exec format error: '/cygdrive/e/Work/Dev/StackOverflow/q055271912/code00.py'\n>    [064bit prompt]>\n>    [064bit prompt]> cat code01.py\n>    #!/usr/bin/env python3\n>\n>    print(\"This is:\", __file__)\n>\n>    [064bit prompt]> python3 -c \"import os, subprocess;subprocess.Popen(os.path.join(os.getcwd(), \\\"code01.py\\\")).communicate()\"\n>    This is: /cygdrive/e/Work/Dev/StackOverflow/q055271912/code01.py\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/25513043/python-argparse-fails-to-parse-hex-formatting-to-int-type",
    "body": "def _get_value(self, action, arg_string):\n        type_func = self._registry_get('type', action.type, action.type)\n        if not _callable(type_func):\n            msg = _('%r is not callable')\n            raise ArgumentError(action, msg % type_func)\n\n        # convert the value to the appropriate type\n        try:\n            result = type_func(arg_string)\n\n        # ArgumentTypeErrors indicate errors\n        except ArgumentTypeError:\n            name = getattr(action.type, '__name__', repr(action.type))\n            msg = str(_sys.exc_info()[1])\n            raise ArgumentError(action, msg)\n\n        # TypeErrors or ValueErrors also indicate errors\n        except (TypeError, ValueError):\n            name = getattr(action.type, '__name__', repr(action.type))\n            msg = _('invalid %s value: %r')\n            raise ArgumentError(action, msg % (name, arg_string))\n\n        # return the converted value\n        return result"
  },
  {
    "url": "https://stackoverflow.com/questions/55169645/square-detection-in-image",
    "body": "import cv2\nimport numpy as np\n# Load image, grayscale, median blur, sharpen image\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nblur = cv2.medianBlur(gray, 5)\nsharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\nsharpen = cv2.filter2D(blur, -1, sharpen_kernel)\n# Threshold and morph close\nthresh = cv2.threshold(sharpen, 160, 255, cv2.THRESH_BINARY_INV)[1]\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\nclose = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)\n# Find contours and filter using threshold area\ncnts = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nmin_area = 100\nmax_area = 1500\nimage_number = 0\nfor c in cnts:\n    area = cv2.contourArea(c)\n    if area > min_area and area < max_area:\n        x,y,w,h = cv2.boundingRect(c)\n        ROI = image[y:y+h, x:x+w]\n        cv2.imwrite('ROI_{}.png'.format(image_number), ROI)\n        cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n        image_number += 1\ncv2.imshow('sharpen', sharpen)\ncv2.imshow('close', close)\ncv2.imshow('thresh', thresh)\ncv2.imshow('image', image)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/10120295/valid-characters-in-a-python-class-name",
    "body": "> identifier   ::=  xid_start xid_continue*\n> id_start     ::=  <all characters in general categories Lu, Ll, Lt, Lm, Lo, Nl, the underscore, and characters with the Other_ID_Start property>\n> id_continue  ::=  <all characters in id_start, plus characters in the categories Mn, Mc, Nd, Pc and others with the Other_ID_Continue property>\n> xid_start    ::=  <all characters in id_start whose NFKC normalization is in \"id_start xid_continue*\">\n> xid_continue ::=  <all characters in id_continue whose NFKC normalization is in \"id_continue*\">\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/9586630/python-paths-and-import-order",
    "body": "myproject/ # <-- This is not a package (no __init__.py file).\n  modules/ # <-- This is a package (has an __init__.py file).\n    __init__.py\n    foo.py\n  run.py\n  second.py\nexecuted with: python /path/to/the/myproject/run.py\nwill cause sys.path[0] to be \"/path/to/the/myproject/\"\nrun.py contents:\nimport modules.foo as foo # will import \"/path/to/the/myproject/\" + \"modules/foo.py\"\nimport second # will import \"/path/to/the/myproject/\" + \"second.py\"\nsecond.py contents:\nimport modules.foo as foo # will import \"/path/to/the/myproject/\" + \"modules/foo.py\""
  },
  {
    "url": "https://stackoverflow.com/questions/55876683/hook-into-the-builtin-python-f-string-format-machinery-to-override-extend-built",
    "body": "# line 2      # Put the value of function parameter x on the stack\n  2           0 LOAD_FAST                0 (x)\n              # Put the format spec on the stack as a string\n              2 LOAD_CONST               1 ('foo')\n              # Pop both values from the stack and perform the actual formatting\n              # This puts the formatted string on the stack\n              4 FORMAT_VALUE             4 (with format)\n              # pop the result from the stack and return it\n              6 RETURN_VALUE"
  },
  {
    "url": "https://stackoverflow.com/questions/55876683/hook-into-the-builtin-python-f-string-format-machinery-to-override-extend-built",
    "body": "from bytecode import Bytecode\ndef formathack_rewrite_bytecode__(code):\n    \"\"\"\n    Modifies a code object to override the behavior of the FORMAT_VALUE\n    instructions used by f-strings.\n    \"\"\"\n    decompiled = Bytecode.from_code(code)\n    modified_instructions = []\n    for instruction in decompiled:\n        name = getattr(instruction, 'name', None)\n        if name == 'FORMAT_VALUE':\n            # 0x04 means that a format spec is present\n            if instruction.arg & 0x04 == 0x04:\n                callback_arg_count = 2\n            else:\n                callback_arg_count = 1\n            modified_instructions.extend([\n                # Load in the callback\n                Instr(\"LOAD_GLOBAL\", \"formathack_hook__\"),\n                # Shuffle around the top of the stack to put the arguments on top\n                # of the function global\n                Instr(\"ROT_THREE\" if callback_arg_count == 2 else \"ROT_TWO\"),\n                # Call the callback function instead of executing FORMAT_VALUE\n                Instr(\"CALL_FUNCTION\", callback_arg_count)\n            ])\n        # Kind of nasty: we want to recursively alter the code of functions.\n        elif name == 'LOAD_CONST' and isinstance(instruction.arg, types.CodeType):\n            modified_instructions.extend([\n                Instr(\"LOAD_CONST\", formathack_rewrite_bytecode__(instruction.arg), lineno=instruction.lineno)\n            ])\n        else:\n            modified_instructions.append(instruction)\n    modified_bytecode = Bytecode(modified_instructions)\n    # For functions, copy over argument definitions\n    modified_bytecode.argnames = decompiled.argnames\n    modified_bytecode.argcount = decompiled.argcount\n    modified_bytecode.name = decompiled.name\n    return modified_bytecode.to_code()"
  },
  {
    "url": "https://stackoverflow.com/questions/55876683/hook-into-the-builtin-python-f-string-format-machinery-to-override-extend-built",
    "body": "class _FormatHackLoader(importlib.machinery.SourceFileLoader):\n    \"\"\"\n    A module loader that modifies the code of the modules it imports to override\n    the behavior of f-strings. Nasty stuff.\n    \"\"\"\n    @classmethod\n    def find_spec(cls, name, path, target=None):\n        # Start out with a spec from a default finder\n        spec = importlib.machinery.PathFinder.find_spec(\n            fullname=name,\n             # Only apply to modules and packages in the current directory\n             # This prevents standard library modules or site-packages\n             # from being patched.\n            path=[\"\"],\n            target=target\n        )\n        if spec is None:\n            return None\n\n        # Modify the loader in the spec to this loader\n        spec.loader = cls(name, spec.origin)\n        return spec\n    def get_code(self, fullname):\n        # This is called by exec_module to get the code of the module\n        # to execute it.\n        code = super().get_code(fullname)\n        # Rewrite the code to modify the f-string formatting opcodes\n        rewritten_code = formathack_rewrite_bytecode__(code)\n        return rewritten_code\n    def exec_module(self, module):\n        # We introduce the callback that hooks into the f-string formatting\n        # process in every imported module\n        module.__dict__[\"formathack_hook__\"] = formathack_hook__\n        return super().exec_module(module)"
  },
  {
    "url": "https://stackoverflow.com/questions/55876683/hook-into-the-builtin-python-f-string-format-machinery-to-override-extend-built",
    "body": "def install():\n    # If the _FormatHackLoader is not registered as a finder,\n    # do it now!\n    if sys.meta_path[0] is not _FormatHackLoader:\n        sys.meta_path.insert(0, _FormatHackLoader)\n        # Tricky part: we want to be able to use our custom f-string behavior\n        # in the main module where install was called. That module was loaded\n        # with a standard loader though, so that's impossible without additional\n        # dirty hacks.\n        # Here, we execute the module _again_, this time with _FormatHackLoader\n        module_globals = inspect.currentframe().f_back.f_globals\n        module_name = module_globals[\"__name__\"]\n        module_file = module_globals[\"__file__\"]\n        loader = _FormatHackLoader(module_name, module_file)\n        loader.load_module(module_name)\n        # This is actually pretty important. If we don't exit here, the main module\n        # will continue from the formathack.install method, causing it to run twice!\n        sys.exit(0)"
  },
  {
    "url": "https://stackoverflow.com/questions/52285104/3d-scatterplots-with-hue-colormap-and-legend",
    "body": "import re, seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\n# generate data\nn = 200\nx = np.random.uniform(1, 20, size=n)\ny = np.random.uniform(1, 100, size=n)\nz = np.random.uniform(1, 100, size=n)\n# axes instance\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig, auto_add_to_figure=False)\nfig.add_axes(ax)\n# get colormap from seaborn\ncmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n# plot\nsc = ax.scatter(x, y, z, s=40, c=x, marker='o', cmap=cmap, alpha=1)\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n# legend\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n# save\nplt.savefig(\"scatter_hue\", bbox_inches='tight')"
  },
  {
    "url": "https://stackoverflow.com/questions/78997019/in-python-3-12-why-does-%c3%96l-take-less-memory-than-%c3%96",
    "body": "import ctypes\nimport sys\nclass PyUnicodeObject(ctypes.Structure):\n    _fields_ = [\n        (\"ob_refcnt\", ctypes.c_ssize_t),\n        (\"ob_type\", ctypes.c_void_p),\n        (\"length\", ctypes.c_ssize_t),\n        (\"hash\", ctypes.c_ssize_t),\n        (\"state\", ctypes.c_uint64),\n    ]\nclass StateBitField(ctypes.LittleEndianStructure):\n    _fields_ = [\n        (\"interned\", ctypes.c_uint, 2),\n        (\"kind\", ctypes.c_uint, 3),\n        (\"compact\", ctypes.c_uint, 1),\n        (\"ascii\", ctypes.c_uint, 1),\n        (\"statically_allocated\", ctypes.c_uint, 1),\n        (\"_padding\", ctypes.c_uint, 24),\n    ]\n    def __repr__(self):\n        return \", \".join(f\"{k}: {getattr(self, k)}\" for k, *_ in self._fields_ if not k.startswith(\"_\"))\ndef dump_s(s: str):\n    o = PyUnicodeObject.from_address(id(s))\n    state_int = o.state\n    state = StateBitField.from_buffer(ctypes.c_uint64(state_int))\n    print(f\"{s!r}\".ljust(8), f\"{o.length=}, {sys.getsizeof(s)=}, {state}\")\ndump_s('5')\ndump_s('a')\ndump_s('ä')\ndump_s('vvv')\ndump_s('ÖÖÖ')\ndump_s(str(chr(214)))  # avoid the string having been interned into module source\ndump_s(str(chr(214) + chr(108)))  # avoid the string having been interned into module source"
  },
  {
    "url": "https://stackoverflow.com/questions/78997019/in-python-3-12-why-does-%c3%96l-take-less-memory-than-%c3%96",
    "body": "'5'      o.length=1, sys.getsizeof(s)=42, interned: 3, kind: 1, compact: 1, ascii: 1, statically_allocated: 1\n'a'      o.length=1, sys.getsizeof(s)=42, interned: 3, kind: 1, compact: 1, ascii: 1, statically_allocated: 1\n'ä'      o.length=1, sys.getsizeof(s)=61, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 1\n'vvv'    o.length=3, sys.getsizeof(s)=44, interned: 2, kind: 1, compact: 1, ascii: 1, statically_allocated: 0\n'ÖÖÖ'    o.length=3, sys.getsizeof(s)=60, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 0\n'Ö'      o.length=1, sys.getsizeof(s)=61, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 1\n'Öl'     o.length=2, sys.getsizeof(s)=59, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 0\n'Ö'      o.length=1, sys.getsizeof(s)=61, interned: 0, kind: 1, compact: 1, ascii: 0, statically_allocated: 1"
  },
  {
    "url": "https://stackoverflow.com/questions/39767297/how-to-use-sha256-hmac-in-python-code",
    "body": "`\nimport hashlib\nimport hmac\n# Define my and key as per question\nmy = \"/api/embedded_dashboard?data=%7B%22dashboard%22%3A7863%2C%22embed%22%3A%22v2%22%2C%22filters%22%3A%5B%7B%22name%22%3A%22Filter1%22%2C%22value%22%3A%22value1%22%7D%2C%7B%22name%22%3A%22Filter2%22%2C%22value%22%3A%221234%22%7D%5D%7D\"\nkey = \"e179017a-62b0-4996-8a38-e91aa9f1\"\n# Encode as per other answers\nbyte_key = key.encode(\"UTF-8\")\nmessage = my.encode()\n# Now use the hmac.new function and the hexdigest method\nh = hmac.new(byte_key, message, hashlib.sha256).hexdigest()\n# Print the output\nprint(h)"
  },
  {
    "url": "https://stackoverflow.com/questions/66597894/why-cannot-add-ppa-deadsnakes",
    "body": "$ sudo add-apt-repository -y 'ppa:deadsnakes/ppa'\nCannot add PPA: 'ppa:~deadsnakes/ubuntu/ppa'.\nERROR: '~deadsnakes' user or team does not exist.\n$ sudo -E add-apt-repository -y 'ppa:deadsnakes/ppa'\n This PPA contains more recent Python versions packaged for Ubuntu.\nDisclaimer: there's no guarantee of timely updates in case of security problems or other issues. If you want to use them in a security-or-otherwise-critical environment (say, on a production server), you do so at your own risk.\nUpdate Note\n===========\n..."
  },
  {
    "url": "https://stackoverflow.com/questions/70916649/how-to-change-the-x-axis-and-y-axis-labels-in-plotly",
    "body": "import pandas as pd\nimport io, requests\ndf = pd.read_csv(\n    io.StringIO(\n        requests.get(\n            \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv\"\n        ).text\n    )\n)\ndf[\"Date\"] = pd.to_datetime(df[\"date\"])\ndf[\"Country\"] = df[\"location\"]\ndf[\"7day_rolling_avg\"] = df[\"daily_people_vaccinated_per_hundred\"]\nDate = df[df.Country == \"India\"].Date\nNew_cases = df[df.Country == \"India\"][\"7day_rolling_avg\"]\npx.line(df, x=Date, y=New_cases, title=\"India Daily New Covid Cases\").update_layout(\n    xaxis_title=\"Date\", yaxis_title=\"7 day avg\"\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/60703127/how-to-catch-botocore-errorfactory-usernotfoundexception",
    "body": "import boto3\neks = boto3.client('eks')\nprint(dir(eks.exceptions))\n# ['BadRequestException',\n# 'ClientError',\n# 'ClientException',\n# 'InvalidParameterException',\n# 'InvalidRequestException',\n# 'NotFoundException',\n# 'ResourceInUseException',\n# 'ResourceLimitExceededException',\n# 'ResourceNotFoundException',\n# 'ServerException',\n# 'ServiceUnavailableException',\n# 'UnsupportedAvailabilityZoneException', ...]\ntry:\n    response = eks.list_nodegroups(clusterName='my-cluster')\nexcept eks.exceptions.ResourceNotFoundException as e:\n    # do something with e\n    print(\"handled: \" + str(e))\ncognito_idp = boto3.client('cognito-idp')\nprint(dir(cognito_idp.exceptions))\n# [ 'ClientError',\n# 'ConcurrentModificationException',\n# 'DeveloperUserAlreadyRegisteredException',\n# 'ExternalServiceException',\n# 'InternalErrorException',\n# 'InvalidIdentityPoolConfigurationException',\n# 'InvalidParameterException',\n# 'LimitExceededException',\n# 'NotAuthorizedException',\n# 'ResourceConflictException',\n# 'ResourceNotFoundException',\n# 'TooManyRequestsException', ... ]\ntry:\n    response = cognito_idp.admin_get_user(\n        UserPoolId='pool_id',\n        Username='username'\n    )\nexcept cognito_idp.exceptions.UserNotFoundException as e:\n    # do something with e\n    print(\"handled: \" + str(e))"
  },
  {
    "url": "https://stackoverflow.com/questions/27396339/attributeerror-cant-set-attribute",
    "body": "class MAMLMetaLearner(nn.Module):\n    def __init__(\n            self,\n            args,\n            base_model,\n            inner_debug=False,\n            target_type='classification'\n    ):\n        super().__init__()\n        self.args = args  # args for experiment\n        self.base_model = base_model\n        assert base_model is args.model\n        self.inner_debug = inner_debug\n        self.target_type = target_type\n    @property\n    def lr_inner(self) -> float:\n        return self.args.inner_lr\n    @lr_inner.setter\n    def lr_inner(self, new_val: float):\n        self.args.inner_lr = new_val"
  },
  {
    "url": "https://stackoverflow.com/questions/50533812/what-is-the-best-way-to-define-constant-variables-python-3",
    "body": "from typing import Final\n# Annotate module variables\n# (with or without an explicit type, using the syntax Final[<type>])\n# (type is auto-determined in absence of an explicit type)\nPI: Final[float] = 3.141592654\nANSWER_TO_EVERYTHING: Final = 42\n# Annotate instance variables in class bodies\n# (explicit type is needed if no value is assigned)\nclass Point:\n    x: Final[int]\n    y: Final = 0\n    def __init__(self, x: int):\n        self.x = x\n# Annotate instance variables directly\n# (only allowed in __init__ methods)\nclass Person:\n    def __init__(self, birth_year: int):\n        self.birth_year: Final = birth_year"
  },
  {
    "url": "https://stackoverflow.com/questions/71520075/zip-longest-for-the-left-list-always",
    "body": "10 iterables of 10,000 to 90,000 elements, first has 50,000:\n────────────────────────────────────────────────────────────\n 2.2 ms   2.2 ms   2.3 ms  limit_cheat\n 2.6 ms   2.6 ms   2.6 ms  Kelly_Bundy_chain\n 3.3 ms   3.3 ms   3.3 ms  Kelly_Bundy_compress\n50.2 ms  50.6 ms  50.7 ms  CrazyChucky\n54.7 ms  55.0 ms  55.0 ms  Sven_Marnach\n74.8 ms  74.9 ms  75.0 ms  Mad_Physicist\n 5.4 ms   5.4 ms   5.4 ms  Kelly_Bundy_3\n 5.9 ms   6.0 ms   6.0 ms  Kelly_Bundy_4\n 4.6 ms   4.7 ms   4.7 ms  Kelly_Bundy_5\n10,000 iterables of 0 to 100 elements, first has 50:\n────────────────────────────────────────────────────\n 4.6 ms   4.7 ms   4.8 ms  limit_cheat\n 4.8 ms   4.8 ms   4.8 ms  Kelly_Bundy_compress\n 8.4 ms   8.4 ms   8.4 ms  Kelly_Bundy_chain\n27.1 ms  27.3 ms  27.5 ms  CrazyChucky\n38.3 ms  38.5 ms  38.7 ms  Sven_Marnach\n73.0 ms  73.0 ms  73.1 ms  Mad_Physicist\n 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_3\n 4.9 ms   4.9 ms   5.0 ms  Kelly_Bundy_4\n 5.0 ms   5.0 ms   5.0 ms  Kelly_Bundy_5"
  },
  {
    "url": "https://stackoverflow.com/questions/71520075/zip-longest-for-the-left-list-always",
    "body": "def limit_cheat(*iterables, fillvalue=None):\n    return islice(zip_longest(*iterables, fillvalue=fillvalue), cheat_length)\ndef Kelly_Bundy_chain(first, *rest, fillvalue=None):\n    return zip(first, *map(chain, rest, repeat(repeat(fillvalue))))\ndef Kelly_Bundy_compress(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return compress(zip_longest(b, *rest, fillvalue=fillvalue), zip(a))\ndef CrazyChucky(*iterables, fillvalue=None):\n    SENTINEL = object()\n\n    for first, *others in zip_longest(*iterables, fillvalue=SENTINEL):\n        if first is SENTINEL:\n            return\n        others = [i if i is not SENTINEL else fillvalue for i in others]\n        yield (first, *others)\ndef Sven_Marnach(first, *rest, fillvalue=None):\n    rest = [iter(r) for r in rest]\n    for x in first:\n        yield x, *(next(r, fillvalue) for r in rest)\ndef Mad_Physicist(*args, fillvalue=None):\n    # zip_by_first('ABCD', 'xy', fillvalue='-') --> Ax By C- D-\n    # zip_by_first('ABC', 'xyzw', fillvalue='-') --> Ax By Cz\n    if not args:\n        return\n    iterators = [iter(it) for it in args]\n    while True:\n        values = []\n        for i, it in enumerate(iterators):\n            try:\n                value = next(it)\n            except StopIteration:\n                if i == 0:\n                    return\n                iterators[i] = repeat(fillvalue)\n                value = fillvalue\n            values.append(value)\n        yield tuple(values)\ndef Kelly_Bundy_3(first, *rest, fillvalue=None):\n    a, b = tee(first)\n    return map(itemgetter(1), zip(a, zip_longest(b, *rest, fillvalue=fillvalue)))\ndef Kelly_Bundy_4(first, *rest, fillvalue=None):\n    sentinel = object()\n    for z in zip_longest(chain(first, [sentinel]), *rest, fillvalue=fillvalue):\n        if z[0] is sentinel:\n            break\n        yield z\ndef Kelly_Bundy_5(first, *rest, fillvalue=None):\n    stopped = False\n    def stop():\n        nonlocal stopped\n        stopped = True\n        return\n        yield\n    for z in zip_longest(chain(first, stop()), *rest, fillvalue=fillvalue):\n        if stopped:\n            break\n        yield z\nimport timeit\nfrom itertools import chain, repeat, zip_longest, islice, tee, compress\nfrom operator import itemgetter\nfrom collections import deque\nfuncs = [\n    limit_cheat,\n    Kelly_Bundy_chain,\n    Kelly_Bundy_compress,\n    CrazyChucky,\n    Sven_Marnach,\n    Mad_Physicist,\n    Kelly_Bundy_3,\n    Kelly_Bundy_4,\n    Kelly_Bundy_5,\n]\ndef test(args_creator):\n    # Correctness\n    expect = list(funcs[0](*args_creator()))\n    for func in funcs:\n        result = list(func(*args_creator()))\n        print(result == expect, func.__name__)\n\n    # Speed\n    tss = [[] for _ in funcs]\n    for _ in range(5):\n        print()\n        print(args_creator.__name__)\n        for func, ts in zip(funcs, tss):\n            t = min(timeit.repeat(lambda: deque(func(*args_creator()), 0), number=1))\n            ts.append(t)\n            print(*('%4.1f ms ' % (t * 1e3) for t in sorted(ts)[:3]), func.__name__)\ndef args_few_but_long_iterables():\n    global cheat_length\n    cheat_length = 50_000\n    first = repeat(0, 50_000)\n    rest = [repeat(i, 10_000 * i) for i in range(1, 10)]\n    return first, *rest\ndef args_many_but_short_iterables():\n    global cheat_length\n    cheat_length = 50\n    first = repeat(0, 50)\n    rest = [repeat(i, i % 101) for i in range(1, 10_000)]\n    return first, *rest\ntest(args_few_but_long_iterables)\nfuncs[1:3] = funcs[1:3][::-1]\ntest(args_many_but_short_iterables)"
  },
  {
    "url": "https://stackoverflow.com/questions/28453545/connecting-to-an-oracle-database-using-sqlalchemy",
    "body": "from sqlalchemy.engine import create_engine\nDIALECT = 'oracle'\nSQL_DRIVER = 'cx_oracle'\nUSERNAME = 'your_username' #enter your username\nPASSWORD = 'your_password' #enter your password\nHOST = 'subdomain.domain.tld' #enter the oracle db host url\nPORT = 1521 # enter the oracle port number\nSERVICE = 'your_oracle_service_name' # enter the oracle db service name\nENGINE_PATH_WIN_AUTH = DIALECT + '+' + SQL_DRIVER + '://' + USERNAME + ':' + PASSWORD +'@' + HOST + ':' + str(PORT) + '/?service_name=' + SERVICE\nengine = create_engine(ENGINE_PATH_WIN_AUTH)\n#test query\nimport pandas as pd\ntest_df = pd.read_sql_query('SELECT * FROM global_name', engine)"
  },
  {
    "url": "https://stackoverflow.com/questions/19130986/python-equivalent-of-golangs-select-on-channels",
    "body": "import threading\nimport Queue\ndef main():\n    c1 = Queue.Queue(maxsize=0)\n    c2 = Queue.Queue(maxsize=0)\n    quit = Queue.Queue(maxsize=0)\n    def func1():\n        for i in range(10):\n            c1.put(i)\n        quit.put(0)\n    threading.Thread(target=func1).start()\n    def func2():\n        for i in range(2):\n            c2.put(i)\n    threading.Thread(target=func2).start()\n    combined = Queue.Queue(maxsize=0)\n    def listen_and_forward(queue):\n        while True:\n            combined.put((queue, queue.get()))\n    t = threading.Thread(target=listen_and_forward, args=(c1,))\n    t.daemon = True\n    t.start()\n    t = threading.Thread(target=listen_and_forward, args=(c2,))\n    t.daemon = True\n    t.start()\n    t = threading.Thread(target=listen_and_forward, args=(quit,))\n    t.daemon = True\n    t.start()\n    while True:\n        which, message = combined.get()\n        if which is c1:\n            print 'Received value from c1'\n        elif which is c2:\n            print 'Received value from c2'\n        elif which is quit:\n            print 'Received value from quit'\n            return\nmain()"
  },
  {
    "url": "https://stackoverflow.com/questions/19130986/python-equivalent-of-golangs-select-on-channels",
    "body": "import threading\nimport Queue\ndef select(*queues):\n    combined = Queue.Queue(maxsize=0)\n    def listen_and_forward(queue):\n        while True:\n            combined.put((queue, queue.get()))\n    for queue in queues:\n        t = threading.Thread(target=listen_and_forward, args=(queue,))\n        t.daemon = True\n        t.start()\n    while True:\n        yield combined.get()\ndef main():\n    c1 = Queue.Queue(maxsize=0)\n    c2 = Queue.Queue(maxsize=0)\n    quit = Queue.Queue(maxsize=0)\n    def func1():\n        for i in range(10):\n            c1.put(i)\n        quit.put(0)\n    threading.Thread(target=func1).start()\n    def func2():\n        for i in range(2):\n            c2.put(i)\n    threading.Thread(target=func2).start()\n    for which, msg in select(c1, c2, quit):\n        if which is c1:\n            print 'Received value from c1'\n        elif which is c2:\n            print 'Received value from c2'\n        elif which is quit:\n            print 'Received value from quit'\n            return\nmain()"
  },
  {
    "url": "https://stackoverflow.com/questions/48170682/can-structured-logging-be-done-with-pythons-standard-library",
    "body": "from datetime import datetime\nimport json\nimport logging\nimport sys\nimport traceback\nAPP_NAME = 'hello world json logging'\nAPP_VERSION = 'git rev-parse HEAD'\nLOG_LEVEL = logging._nameToLevel['INFO']\nclass JsonEncoderStrFallback(json.JSONEncoder):\n  def default(self, obj):\n    try:\n      return super().default(obj)\n    except TypeError as exc:\n      if 'not JSON serializable' in str(exc):\n        return str(obj)\n      raise\nclass JsonEncoderDatetime(JsonEncoderStrFallback):\n  def default(self, obj):\n    if isinstance(obj, datetime):\n      return obj.strftime('%Y-%m-%dT%H:%M:%S%z')\n    else:\n      return super().default(obj)\nlogging.basicConfig(\n  format='%(json_formatted)s',\n  level=LOG_LEVEL,\n  handlers=[\n    # if you wish to also log to a file:\n    # logging.FileHandler(log_file_path, 'a'),\n    logging.StreamHandler(sys.stdout),\n  ],\n)\n_record_factory_bak = logging.getLogRecordFactory()\ndef record_factory(*args, **kwargs) -> logging.LogRecord:\n  record = _record_factory_bak(*args, **kwargs)\n\n  record.json_formatted = json.dumps(\n    {\n      'level': record.levelname,\n      'unixtime': record.created,\n      'thread': record.thread,\n      'location': '{}:{}:{}'.format(\n        record.pathname or record.filename,\n        record.funcName,\n        record.lineno,\n      ),\n      'exception': record.exc_info,\n      'traceback': (\n        traceback.format_exception(*record.exc_info)\n        if record.exc_info\n        else None\n      ),\n      'app': {\n        'name': APP_NAME,\n        'releaseId': APP_VERSION,\n        'message': record.getMessage(),\n      },\n    },\n    cls=JsonEncoderDatetime,\n  )\n  # clear exc data since it is included in the json format\n  # without clearing this, logging.exception will print the\n  # traceback across multiple lines, which is not json formatted\n  record.exc_info = None\n  record.exc_text = None\n  return record\nlogging.setLogRecordFactory(record_factory)"
  },
  {
    "url": "https://stackoverflow.com/questions/59180574/string-concatenation-with-vs-f-string",
    "body": "from timeit import timeit\nimport matplotlib.pyplot as plt\nn = 1000000000\nsetup = \"\"\"\\\na = 'a'*{str_len}\nb = 'b'*{str_len}\n\"\"\"\nfstr_stmt = \"\"\"\\\nf'{a}{b}'\n\"\"\"\nconcat_stmt = \"\"\"\\\na+b\n\"\"\"\nstr_lens = [10, 100, 1000, 10000, 100000, 1000000]\nfstr_t = []\nconcat_t = []\nfor str_len in str_lens:\n    n_iters = n//str_len\n    fstr_t.append(timeit(setup=setup.format(str_len=str_len), stmt=fstr_stmt, number=n_iters)/n_iters)\n    concat_t.append(timeit(setup=setup.format(str_len=str_len), stmt=concat_stmt, number=n_iters)/n_iters)\n    ratio = fstr_t[-1]/concat_t[-1]\n    print(f\"For two strings of length {str_len:7d}, concatenation is {ratio:.5f} times faster than f-strings\")\nplt.plot(str_lens, fstr_t, \"r*-\")\nplt.plot(str_lens, concat_t, \"c*-\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"String length (log scale)\")\nplt.ylabel(\"Seconds per iteration (log scale)\")\nplt.grid()\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/59180574/string-concatenation-with-vs-f-string",
    "body": "For two strings of length      10, concatenation is 1.06938 times faster than f-strings\nFor two strings of length     100, concatenation is 1.14887 times faster than f-strings\nFor two strings of length    1000, concatenation is 1.13994 times faster than f-strings\nFor two strings of length   10000, concatenation is 1.26934 times faster than f-strings\nFor two strings of length  100000, concatenation is 1.21585 times faster than f-strings\nFor two strings of length 1000000, concatenation is 1.01816 times faster than f-strings"
  },
  {
    "url": "https://stackoverflow.com/questions/59180574/string-concatenation-with-vs-f-string",
    "body": "For three strings of length      10, concatenation is 0.77931 times faster than f-strings\nFor three strings of length     100, concatenation is 0.67699 times faster than f-strings\nFor three strings of length    1000, concatenation is 0.60220 times faster than f-strings\nFor three strings of length   10000, concatenation is 1.27484 times faster than f-strings\nFor three strings of length  100000, concatenation is 0.98911 times faster than f-strings\nFor three strings of length 1000000, concatenation is 0.60201 times faster than f-strings"
  },
  {
    "url": "https://stackoverflow.com/questions/22381497/python-scikit-learn-linear-model-parameter-standard-error",
    "body": "...\n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.2867      0.247     -1.161      0.290      -0.891       0.317\nx1             0.1750      0.297      0.590      0.577      -0.551       0.901\nx2            -0.6929      0.352     -1.969      0.096      -1.554       0.168\nx3             0.2234      0.325      0.687      0.518      -0.572       1.019\n=============================================================================="
  },
  {
    "url": "https://stackoverflow.com/questions/64124931/how-to-fix-versionconflict-locking-failure-in-pipenv",
    "body": "Traceback (most recent call last):\n  File \"/usr/local/bin/pipenv\", line 8, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/decorators.py\", line 73, in new_func\n    return ctx.invoke(f, obj, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/decorators.py\", line 21, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/cli/command.py\", line 252, in install\n    site_packages=state.site_packages\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 1928, in do_install\n    site_packages=site_packages,\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 580, in ensure_project\n    pypi_mirror=pypi_mirror,\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 512, in ensure_virtualenv\n    python=python, site_packages=site_packages, pypi_mirror=pypi_mirror\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 999, in do_create_virtualenv\n    project._environment.add_dist(\"pipenv\")\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 135, in add_dist\n    self.extend_dists(dist)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 127, in extend_dists\n    extras = self.resolve_dist(dist, self.base_working_set)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 122, in resolve_dist\n    deps |= cls.resolve_dist(dist, working_set)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 121, in resolve_dist\n    dist = working_set.find(req)\n  File \"/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 642, in find\n    raise VersionConflict(dist, req)\npkg_resources.VersionConflict: (importlib-metadata 2.0.0 (/usr/local/lib/python3.6/site-packages), Requirement.parse('importlib-metadata<2,>=0.12; python_version < \"3.8\"'))"
  },
  {
    "url": "https://stackoverflow.com/questions/28237955/same-name-for-classmethod-and-instancemethod",
    "body": "from typing import Generic, Callable, TypeVar, overload\nfrom typing_extensions import Concatenate, ParamSpec, Self\n_T = TypeVar(\"_T\")\n_R_co = TypeVar(\"_R_co\", covariant=True)\n_R1_co = TypeVar(\"_R1_co\", covariant=True)\n_R2_co = TypeVar(\"_R2_co\", covariant=True)\n_P = ParamSpec(\"_P\")\nclass class_or_instancemethod(classmethod[_T, _P, _R_co]):\n    def __get__(\n        self, instance: _T, type_: type[_T] | None = None\n    ) -> Callable[_P, _R_co]:\n        descr_get = super().__get__ if instance is None else self.__func__.__get__\n        return descr_get(instance, type_)\nclass hybridmethod(Generic[_T, _P, _R1_co, _R2_co]):\n    fclass: Callable[Concatenate[type[_T], _P], _R1_co]\n    finstance: Callable[Concatenate[_T, _P], _R2_co] | None\n    __doc__: str | None\n    __isabstractmethod__: bool\n    def __init__(\n        self,\n        fclass: Callable[Concatenate[type[_T], _P], _R1_co],\n        finstance: Callable[Concatenate[_T, _P], _R2_co] | None = None,\n        doc: str | None = None,\n    ):\n        self.fclass = fclass\n        self.finstance = finstance\n        self.__doc__ = doc or fclass.__doc__\n        # support use on abstract base classes\n        self.__isabstractmethod__ = bool(getattr(fclass, \"__isabstractmethod__\", False))\n    def classmethod(self, fclass: Callable[Concatenate[type[_T], _P], _R1_co]) -> Self:\n        return type(self)(fclass, self.finstance, None)\n    def instancemethod(self, finstance: Callable[Concatenate[_T, _P], _R2_co]) -> Self:\n        return type(self)(self.fclass, finstance, self.__doc__)\n    @overload\n    def __get__(self, instance: None, cls: type[_T]) -> Callable[_P, _R1_co]: ...\n    @overload\n    def __get__(self, instance: _T, cls: type[_T] | None = ...) -> Callable[_P, _R1_co] | Callable[_P, _R2_co]: ...\n    def __get__(self, instance: _T, cls: type[_T] | None = None) -> Callable[_P, _R1_co] | Callable[_P, _R2_co]:\n        if instance is None or self.finstance is None:\n            # either bound to the class, or no instance method available\n            return self.fclass.__get__(cls, None)\n        return self.finstance.__get__(instance, cls)"
  },
  {
    "url": "https://stackoverflow.com/questions/41296313/stacked-bar-chart-with-centered-labels",
    "body": "# plot\ng = sns.displot(data=df, x='cat', hue='variable', weights='value', discrete=True, multiple='stack')\n# iterate through each axes\nfor ax in g.axes.flat:\n    # iterate through each container\n    for c in ax.containers:\n        # Optional: if the segment is small or 0, customize the labels\n        labels = [v.get_height() if v.get_height() > 0 else '' for v in c]\n        # remove the labels parameter if it's not needed for customized labels\n        ax.bar_label(c, labels=labels, label_type='center')"
  },
  {
    "url": "https://stackoverflow.com/questions/41296313/stacked-bar-chart-with-centered-labels",
    "body": "plt.style.use('ggplot')\nax = df.plot(stacked=True, kind='bar', figsize=(12, 8), rot='horizontal')\n# .patches is everything inside of the chart\nfor rect in ax.patches:\n    # Find where everything is located\n    height = rect.get_height()\n    width = rect.get_width()\n    x = rect.get_x()\n    y = rect.get_y()\n\n    # The height of the bar is the data value and can be used as the label\n    label_text = f'{height}'  # f'{height:.2f}' to format decimal values\n\n    # ax.text(x, y, text)\n    label_x = x + width / 2\n    label_y = y + height / 2\n    # plot only when height is greater than specified value\n    if height > 0:\n        ax.text(label_x, label_y, label_text, ha='center', va='center', fontsize=8)\n\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nax.set_ylabel(\"Count\", fontsize=18)\nax.set_xlabel(\"Class\", fontsize=18)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/14355499/how-to-model-a-unique-constraint-in-sqlalchemy",
    "body": "from sqlalchemy.orm import declarative_base\nfrom sqlalchemy import Column, Integer, UniqueConstraint\nBase = declarative_base()\nclass Model(Base):\n    __tablename__ = \"model\"\n    model_id = Column(Integer, primary_key=True)\n    # will create \"model_num_key\" UNIQUE CONSTRAINT, btree (num)\n    num = Column(Integer, unique=True)\n    # same with UniqueConstraint:\n    num2 = Column(Integer)\n    __table_args__ = (UniqueConstraint(\"num2\", name=\"model_num2_key\"),)\n    # for multiple columns:\n    # __table_args__ = (UniqueConstraint(\"num\", \"num2\", name=\"two_columns\"),)"
  },
  {
    "url": "https://stackoverflow.com/questions/65883869/can-i-override-fields-from-a-pydantic-parent-model-to-make-them-optional",
    "body": "class ParentBase(BaseModel):\n    \"\"\"Shared properties.\"\"\"\n    name: str\n    email: str\nclass ParentCreate(ParentBase):\n    \"\"\"Properties to receive on item creation.\"\"\"\n    # dont need id here if your db autocreates it\n    pass\nclass ParentUpdate(ParentBase):\n    \"\"\"Properties to receive on item update.\"\"\"\n    # dont need id as you are likely PUTing to /parents/{id}\n    # other fields should not be optional in a PUT\n    # maybe what you are wanting is a PATCH schema?\n    pass\nclass ParentInDBBase(ParentBase):\n    \"\"\"Properties shared by models stored in DB - !exposed in create/update.\"\"\"\n    # primary key exists in db, but not in base/create/update\n    id: int\nclass Parent(ParentInDBBase):\n    \"\"\"Properties to return to client.\"\"\"\n    # optionally include things like relationships returned to consumer\n    # related_things: List[Thing]\n    pass\nclass ParentInDB(ParentInDBBase):\n    \"\"\"Additional properties stored in DB.\"\"\"\n    # could be secure things like passwords?\n    pass"
  },
  {
    "url": "https://stackoverflow.com/questions/73365780/why-is-not-recommended-to-install-poetry-with-homebrew",
    "body": "❯ pipx list\nvenvs are in /Users/redacted/.local/pipx/venvs\napps are exposed on your $PATH at /Users/redacted/Code/dotfiles/bin\n   package hatch 1.7.0, installed using Python 3.11.5\n    - hatch\n   package poetry 1.2.2, installed using Python 3.11.5\n    - poetry\n   package poetry 1.3.2 (poetry@1.3), installed using Python 3.11.5\n    - poetry@1.3\n   package poetry 1.6.1 (poetry@1.6), installed using Python 3.11.5\n    - poetry@1.6\n❯ poetry --version\nPoetry (version 1.2.2)\n❯ poetry@1.3 --version\nPoetry (version 1.3.2)"
  },
  {
    "url": "https://stackoverflow.com/questions/54491156/validate-json-data-using-python",
    "body": "import json\nfrom jsonschema import validate\n# Describe what kind of json you expect.\nschema = {\n    \"type\" : \"object\",\n    \"properties\" : {\n        \"description\" : {\"type\" : \"string\"},\n        \"status\" : {\"type\" : \"boolean\"},\n        \"value_a\" : {\"type\" : \"number\"},\n        \"value_b\" : {\"type\" : \"number\"},\n    },\n}\n# Convert json to python object.\nmy_json = json.loads('{\"description\": \"Hello world!\", \"status\": true, \"value_a\": 1, \"value_b\": 3.14}')\n# Validate will raise exception if given json is not\n# what is described in schema.\nvalidate(instance=my_json, schema=schema)\n# print for debug\nprint(my_json)"
  },
  {
    "url": "https://stackoverflow.com/questions/77001129/how-to-configure-fastapi-logging-so-that-it-works-both-with-uvicorn-locally-and",
    "body": "LOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': True,\n    'formatters': {\n        'standard': {\n            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n        },\n        'custom_formatter': {\n            'format': \"%(asctime)s [%(processName)s: %(process)d] [%(threadName)s: %(thread)d] [%(levelname)s] %(name)s: %(message)s\"\n\n        },\n    },\n    'handlers': {\n        'default': {\n            'formatter': 'standard',\n            'class': 'logging.StreamHandler',\n            'stream': 'ext://sys.stdout',  # Default is stderr\n        },\n        'stream_handler': {\n            'formatter': 'custom_formatter',\n            'class': 'logging.StreamHandler',\n            'stream': 'ext://sys.stdout',  # Default is stderr\n        },\n        'file_handler': {\n            'formatter': 'custom_formatter',\n            'class': 'logging.handlers.RotatingFileHandler',\n            'filename': 'app.log',\n            'maxBytes': 1024 * 1024 * 1, # = 1MB\n            'backupCount': 3,\n        },\n    },\n    'loggers': {\n        'uvicorn': {\n            'handlers': ['default', 'file_handler'],\n            'level': 'TRACE',\n            'propagate': False\n        },\n        'uvicorn.access': {\n            'handlers': ['stream_handler', 'file_handler'],\n            'level': 'TRACE',\n            'propagate': False\n        },\n        'uvicorn.error': {\n            'handlers': ['stream_handler', 'file_handler'],\n            'level': 'TRACE',\n            'propagate': False\n        },\n        'uvicorn.asgi': {\n            'handlers': ['stream_handler', 'file_handler'],\n            'level': 'TRACE',\n            'propagate': False\n        },\n    },\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/77001129/how-to-configure-fastapi-logging-so-that-it-works-both-with-uvicorn-locally-and",
    "body": "LOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': True,\n    'formatters': {\n        'standard': ...,  # same as above or customize that as well\n        'custom_formatter': {\n            'format': \"{'time':'%(asctime)s', 'process_name': '%(processName)s', 'process_id': '%(process)s', 'thread_name': '%(threadName)s', 'thread_id': '%(thread)s','level': '%(levelname)s', 'logger_name': '%(name)s', 'message': '%(message)s'}\"\n        },\n    },\n    ...  # the rest is the same as in the original settings.py above\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/77001129/how-to-configure-fastapi-logging-so-that-it-works-both-with-uvicorn-locally-and",
    "body": "import logging, json\nclass CustomJSONFormatter(logging.Formatter):\n    def __init__(self, fmt):\n        logging.Formatter.__init__(self, fmt)\n    def format(self, record):\n        logging.Formatter.format(self, record)\n        return json.dumps(get_log(record), indent=2)\ndef get_log(record):\n    d = {\n        \"time\": record.asctime,\n        \"process_name\": record.processName,\n        \"process_id\": record.process,\n        \"thread_name\": record.threadName,\n        \"thread_id\": record.thread,\n        \"level\": record.levelname,\n        \"logger_name\": record.name,\n        \"pathname\": record.pathname,\n        \"line\": record.lineno,\n        \"message\": record.message,\n    }\n    if hasattr(record, \"extra_info\"):\n        d[\"req\"] = record.extra_info[\"req\"]\n        d[\"res\"] = record.extra_info[\"res\"]\n    return d\nLOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': True,\n    'formatters': {\n        'standard': ...,  # same as above or customize that as well\n        'custom_formatter': {\n            '()':  lambda: CustomJSONFormatter(fmt='%(asctime)s')\n        },\n    },\n    ...  # the rest is the same as in the original settings.py above\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/77001129/how-to-configure-fastapi-logging-so-that-it-works-both-with-uvicorn-locally-and",
    "body": "{\n  \"time\": \"2024-10-27 11:05:00,300\",\n  \"process_name\": \"MainProcess\",\n  \"process_id\": 4102,\n  \"thread_name\": \"AnyIO worker thread\",\n  \"thread_id\": 1147,\n  \"level\": \"INFO\",\n  \"logger_name\": \"uvicorn.error\",\n  \"pathname\": \"C:\\\\...\",\n  \"line\": 33,\n  \"message\": \"GET /\",\n  \"req\": {\n    \"url\": \"/\",\n    \"headers\": {\n      \"host\": \"localhost:8000\",\n      \"user-agent\": \"Mozilla...\",\n      \"accept\": \"text/html,application/xhtml+xml,...\"\n    },\n    \"method\": \"GET\",\n    \"http_version\": \"1.1\",\n    \"original_url\": \"/\",\n    \"query\": {}\n  },\n  \"res\": {\n    \"status_code\": 200,\n    \"status\": \"OK\"\n  }\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/77001129/how-to-configure-fastapi-logging-so-that-it-works-both-with-uvicorn-locally-and",
    "body": "from fastapi import FastAPI\nimport logging\nimport uvicorn\nimport sys\napp = FastAPI()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\"%(asctime)s [%(processName)s: %(process)d] [%(threadName)s: %(thread)d] [%(levelname)s] %(name)s: %(message)s\")\nstream_handler = logging.StreamHandler(sys.stdout)\nstream_handler.setFormatter(formatter)\nfile_handler = logging.FileHandler(\"info.log\")\nfile_handler.setFormatter(formatter)\nlogger.addHandler(stream_handler)\nlogger.addHandler(file_handler)\nlogger.info('API is starting up')\n@app.get('/')\nasync def main():\n    logger.info('GET /')\n    return 'ok'\nif __name__ == '__main__':\n    uvicorn.run(app, log_level=\"trace\")  # or `log_config=settings.LOGGING_CONFIG`"
  },
  {
    "url": "https://stackoverflow.com/questions/4596962/display-graph-without-saving-using-pydot",
    "body": "import io\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport networkx as nx\n# create a `networkx` graph\ng = nx.MultiDiGraph()\ng.add_nodes_from([1,2])\ng.add_edge(1, 2)\n# convert from `networkx` to a `pydot` graph\npydot_graph = nx.drawing.nx_pydot.to_pydot(g)\n# render the `pydot` by calling `dot`, no file saved to disk\npng_str = pydot_graph.create_png(prog='dot')\n# treat the DOT output as an image file\nsio = io.BytesIO()\nsio.write(png_str)\nsio.seek(0)\nimg = mpimg.imread(sio)\n# plot the image\nimgplot = plt.imshow(img, aspect='equal')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/62853539/how-to-plot-on-secondary-y-axis-with-plotly-express",
    "body": "# import some stuff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\n# create some data\ndf = pd.DataFrame()\nn = 50\ndf[\"Time\"] = np.arange(n)\ndf[\"Linear-\"] = np.arange(n)+np.random.rand(n)\ndf[\"Linear+\"] = np.arange(n)+np.random.rand(n)\ndf[\"Log-\"] = np.arange(n)+np.random.rand(n)\ndf[\"Log+\"] = np.arange(n)+np.random.rand(n)\ndf.set_index(\"Time\", inplace=True)\nsubfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n# create two independent figures with px.line each containing data from multiple columns\nfig = px.line(df, y=df.filter(regex=\"Linear\").columns, render_mode=\"webgl\",)\nfig2 = px.line(df, y=df.filter(regex=\"Log\").columns, render_mode=\"webgl\",)\nfig2.update_traces(yaxis=\"y2\")\nsubfig.add_traces(fig.data + fig2.data)\nsubfig.layout.xaxis.title=\"Time\"\nsubfig.layout.yaxis.title=\"Linear Y\"\nsubfig.layout.yaxis2.type=\"log\"\nsubfig.layout.yaxis2.title=\"Log Y\"\n# recoloring is necessary otherwise lines from fig und fig2 would share each color\n# e.g. Linear-, Log- = blue; Linear+, Log+ = red... we don't want this\nsubfig.for_each_trace(lambda t: t.update(line=dict(color=t.marker.color)))\nsubfig.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/62067400/understanding-accumulated-gradients-in-pytorch",
    "body": "def calculate_loss(x: torch.Tensor) -> torch.Tensor:\n    y = 2 * x\n    y_hat = model(x)\n    loss = (y - y_hat) ** 2\n    return loss.mean()\n# With mulitple batches of size 1\nbatches = [torch.tensor([4.0]), torch.tensor([2.0])]\noptimizer.zero_grad()\nfor i, batch in enumerate(batches):\n    # The loss needs to be scaled, because the mean should be taken across the whole\n    # dataset, which requires the loss to be divided by the number of batches.\n    loss = calculate_loss(batch) / len(batches)\n    loss.backward()\n    print(f\"Batch size 1 (batch {i}) - grad: {model.weight.grad}\")\n    print(f\"Batch size 1 (batch {i}) - weight: {model.weight}\")\n# Updating the model only after all batches\noptimizer.step()\nprint(f\"Batch size 1 (final) - grad: {model.weight.grad}\")\nprint(f\"Batch size 1 (final) - weight: {model.weight}\")"
  },
  {
    "url": "https://stackoverflow.com/questions/69670125/how-to-log-raw-http-request-response-in-fastapi",
    "body": "from fastapi import FastAPI, APIRouter, Response, Request\nfrom starlette.background import BackgroundTask\nfrom fastapi.routing import APIRoute\nfrom starlette.types import Message\nfrom typing import Dict, Any\nimport logging\napp = FastAPI()\nlogging.basicConfig(filename='info.log', level=logging.DEBUG)\ndef log_info(req_body, res_body):\n    logging.info(req_body)\n    logging.info(res_body)\n# not needed when using FastAPI>=0.108.0.\n'''\nasync def set_body(request: Request, body: bytes):\n    async def receive() -> Message:\n        return {'type': 'http.request', 'body': body}\n    request._receive = receive\n'''\n@app.middleware('http')\nasync def some_middleware(request: Request, call_next):\n    req_body = await request.body()\n    #await set_body(request, req_body)  # not needed when using FastAPI>=0.108.0.\n    response = await call_next(request)\n\n    chunks = []\n    async for chunk in response.body_iterator:\n        chunks.append(chunk)\n    res_body = b''.join(chunks)\n\n    task = BackgroundTask(log_info, req_body, res_body)\n    return Response(content=res_body, status_code=response.status_code,\n        headers=dict(response.headers), media_type=response.media_type, background=task)\n@app.post('/')\ndef main(payload: Dict[Any, Any]):\n    return payload"
  },
  {
    "url": "https://stackoverflow.com/questions/69670125/how-to-log-raw-http-request-response-in-fastapi",
    "body": "from fastapi import FastAPI, APIRouter, Response, Request\nfrom starlette.background import BackgroundTask\nfrom starlette.responses import StreamingResponse\nfrom fastapi.routing import APIRoute\nfrom starlette.types import Message\nfrom typing import Callable, Dict, Any\nimport logging\nimport httpx\ndef log_info(req_body, res_body):\n    logging.info(req_body)\n    logging.info(res_body)\n\nclass LoggingRoute(APIRoute):\n    def get_route_handler(self) -> Callable:\n        original_route_handler = super().get_route_handler()\n        async def custom_route_handler(request: Request) -> Response:\n            req_body = await request.body()\n            response = await original_route_handler(request)\n            tasks = response.background\n\n            if isinstance(response, StreamingResponse):\n                chunks = []\n                async for chunk in response.body_iterator:\n                    chunks.append(chunk)\n                res_body = b''.join(chunks)\n\n                task = BackgroundTask(log_info, req_body, res_body)\n                response = Response(content=res_body, status_code=response.status_code,\n                        headers=dict(response.headers), media_type=response.media_type)\n            else:\n                task = BackgroundTask(log_info, req_body, response.body)\n\n            # check if the original response had background tasks already attached to it\n            if tasks:\n                tasks.add_task(task)  # add the new task to the tasks list\n                response.background = tasks\n            else:\n                response.background = task\n\n            return response\n\n        return custom_route_handler\napp = FastAPI()\nrouter = APIRouter(route_class=LoggingRoute)\nlogging.basicConfig(filename='info.log', level=logging.DEBUG)\n@router.post('/')\ndef main(payload: Dict[Any, Any]):\n    return payload\n@router.get('/video')\ndef get_video():\n    url = 'https://storage.googleapis.com/gtv-videos-bucket/sample/ForBiggerBlazes.mp4'\n\n    def gen():\n        with httpx.stream('GET', url) as r:\n            for chunk in r.iter_raw():\n                yield chunk\n    return StreamingResponse(gen(), media_type='video/mp4')\napp.include_router(router)"
  },
  {
    "url": "https://stackoverflow.com/questions/37669222/how-can-i-hint-that-a-type-is-comparable-with-typing",
    "body": "from __future__ import annotations\nfrom abc import abstractmethod\nfrom typing import MutableSequence, Protocol, TypeVar\nclass Comparable(Protocol):\n    \"\"\"Protocol for annotating comparable types.\"\"\"\n    @abstractmethod\n    def __lt__(self: CT, other: CT) -> bool:\n        pass\nCT = TypeVar(\"CT\", bound=Comparable)\ndef comparison_sort(s: MutableSequence[CT]) -> None:\n    pass\ncomparison_sort([1, 2, 3])  # OK\ncomparison_sort([1.0, 2.0, 3.0])  # OK\ncomparison_sort([\"42\", \"420\", \"2137\"])  # OK\ncomparison_sort([1, 2, \"3\"])  # mypy error"
  },
  {
    "url": "https://stackoverflow.com/questions/56808693/customizing-the-order-of-legends-in-plotly",
    "body": "import plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ntrace1 = go.Bar(x=['A', 'B', 'C'],\n                y=[20, 14, 23],\n                name='first')\ntrace2 = go.Bar(x=['A', 'B', 'C'],\n                y=[12, 18, 29],\n                name='second')\ndata = [trace1, trace2]\nlayout = go.Layout(barmode='stack',\n                   legend={'traceorder':'normal'})\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='stacked-bar')"
  },
  {
    "url": "https://stackoverflow.com/questions/24917700/adding-a-row-to-a-multiindex-dataframe-series",
    "body": "# say you have dataframe x\nx\nOut[78]:\n              a    b       time\nindA indB\na    i      0.0  NaN 2018-09-12\nb    j      1.0  2.0 2018-10-12\nc    k      2.0  3.0 2018-11-12\n     f      NaN  NaN        NaT\nd    i      5.0  NaN        NaT\nx.loc[('a','k'),:] = (3.5,6,pd.NaT)\nx\nOut[80]:\n              a    b       time\nindA indB\na    i      0.0  NaN 2018-09-12\nb    j      1.0  2.0 2018-10-12\nc    k      2.0  3.0 2018-11-12\n     f      NaN  NaN        NaT\nd    i      5.0  NaN        NaT\na    k      3.5  6.0        NaT"
  },
  {
    "url": "https://stackoverflow.com/questions/45093811/installing-pygraphviz-on-windows-10-64-bit-python-3-6",
    "body": ">    [cfati@cfati-5510-0:/cygdrive/e/Work/Dev/StackOverflow/q045093811/src/graphviz]> ~/sopr.sh\n>    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n>\n>    [064bit prompt]> git clone https://gitlab.com/graphviz/graphviz.git .\n>    Cloning into '.'...\n>    remote: Enumerating objects: 71728, done.\n>    remote: Counting objects: 100% (71728/71728), done.\n>    remote: Compressing objects: 100% (19331/19331), done.\n>    remote: Total 71728 (delta 52200), reused 71681 (delta 52157)\n>    Receiving objects: 100% (71728/71728), 163.79 MiB | 480.00 KiB/s, done.\n>    Resolving deltas: 100% (52200/52200), done.\n>    Checking out files: 100% (3870/3870), done.\n>    [064bit prompt]>\n>    [064bit prompt]> git submodule update --init\n>    Submodule 'dependencies/criterion' (https://github.com/Snaipe/Criterion.git) registered for path 'dependencies/criterion'\n>    Submodule 'windows/dependencies/graphviz-build-utilities' (https://github.com/ErwinJanssen/graphviz-build-utilities.git) registered for path 'windows/dependencies/graphviz-build-utilities'\n>    Submodule 'windows/dependencies/libraries' (https://github.com/ErwinJanssen/graphviz-windows-dependencies.git) registered for path 'windows/dependencies/libraries'\n>    Cloning into '/cygdrive/e/Work/Dev/StackOverflow/q045093811/src/graphviz/dependencies/criterion'...\n>    Cloning into '/cygdrive/e/Work/Dev/StackOverflow/q045093811/src/graphviz/windows/dependencies/graphviz-build-utilities'...\n>    Cloning into '/cygdrive/e/Work/Dev/StackOverflow/q045093811/src/graphviz/windows/dependencies/libraries'...\n>    Submodule path 'dependencies/criterion': checked out '301d143ea42c024f22b673b69c72a4cb3c8d151f'\n>    Submodule path 'windows/dependencies/graphviz-build-utilities': checked out '050fff84ce195e0740878748760fd801eeb07b23'\n>    Submodule path 'windows/dependencies/libraries': checked out '141d3a21be904fa8dc2ae3ed01d36684db07a35d'\n>    [064bit prompt]>\n>    [064bit prompt]> git show head\n>    commit 89292b5945933b1501293c04894ed9cf886241be (HEAD -> master, origin/master, origin/HEAD)\n>    Merge: 429d43615 97811bd35\n>    Author: Stephen C North <scnorth@gmail.com>\n>    Date:   Mon Feb 4 08:09:40 2019 -0500\n>\n>        Merge branch 'wasbridge/graphviz-master' into HEAD\n>\n>    [064bit prompt]> git status\n>    On branch master\n>    Your branch is up to date with 'origin/master'.\n>\n>    nothing to commit, working tree clean\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/45093811/installing-pygraphviz-on-windows-10-64-bit-python-3-6",
    "body": ">    [064bit prompt]> git status\n>    On branch master\n>    Your branch is up to date with 'origin/master'.\n>\n>    Changes not staged for commit:\n>      (use \"git add <file>...\" to update what will be committed)\n>      (use \"git checkout -- <file>...\" to discard changes in working directory)\n>      (commit or discard the untracked or modified content in submodules)\n>\n>            modified:   lib/cdt/cdt.vcxproj\n>            modified:   lib/cgraph/cgraph.vcxproj\n>            modified:   windows/dependencies/graphviz-build-utilities (modified content)\n>\n>    no changes added to commit (use \"git add\" and/or \"git commit -a\")\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/45093811/installing-pygraphviz-on-windows-10-64-bit-python-3-6",
    "body": ">    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811]> sopr.bat\n>    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n>\n>    [prompt]> \"c:\\Install\\x86\\Microsoft\\Visual Studio Community\\2015\\vc\\vcvarsall.bat\" x64\n>\n>    [prompt]> set PATH=%PATH%;%CD%\\src\\graphviz\\windows\\dependencies\\graphviz-build-utilities\n>\n>    [prompt]> msbuild src\\graphviz\\lib\\cdt\\cdt.vcxproj /t:Rebuild /p:Platform=x64;Configuration=Release;SolutionDir=%CD%\\src\\graphviz\\;OutDir=%CD%\\bin\\Win\\dynamic\\064\\UCRTv140\\md\\Release\\graphviz\\ >build_cdt_064.txt 2>&1\n>\n>    [prompt]> echo %errorlevel%\n>    0\n>\n>    [prompt]> dir /b\n>    bin\n>    build_cdt.txt\n>    other\n>    src\n>\n>    [prompt]> msbuild src\\graphviz\\lib\\cgraph\\cgraph.vcxproj /t:Rebuild /p:Platform=x64;Configuration=Release;SolutionDir=%CD%\\src\\graphviz\\;OutDir=%CD%\\bin\\Win\\dynamic\\064\\UCRTv140\\md\\Release\\graphviz\\ >build_cgraph_064.txt 2>&1\n>\n>    [prompt]> echo %errorlevel%\n>    0\n>\n>    [prompt]> dir /b \"bin\\Win\\dynamic\\064\\UCRTv140\\md\\Release\\graphviz\"\n>    cdt.dll\n>    cdt.dll.lastcodeanalysissucceeded\n>    cdt.exp\n>    cdt.lib\n>    cgraph.dll\n>    cgraph.dll.lastcodeanalysissucceeded\n>    cgraph.exp\n>    cgraph.lib\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/45093811/installing-pygraphviz-on-windows-10-64-bit-python-3-6",
    "body": "--- pygraphviz/graphviz_wrap.c.orig\t2018-09-10 16:07:12.000000000 +0300\n+++ pygraphviz/graphviz_wrap.c\t2019-02-26 18:05:20.281741400 +0200\n@@ -2988,7 +2988,18 @@\n\n\n #if PY_VERSION_HEX >= 0x03000000\n-extern PyTypeObject PyIOBase_Type;\n+static PyObject *PyIOBase_TypeObj;\n+\n+static int init_file_emulator(void)\n+{\n+  PyObject *io = PyImport_ImportModule(\"_io\");\n+  if (io == NULL)\n+    return -1;\n+  PyIOBase_TypeObj = PyObject_GetAttrString(io, \"_IOBase\");\n+  if (PyIOBase_TypeObj == NULL)\n+    return -1;\n+  return 0;\n+}\n #endif\n\n\n@@ -3449,7 +3460,7 @@\n   {\n #if PY_VERSION_HEX >= 0x03000000 || defined(PYPY_VERSION)\n #if !defined(PYPY_VERSION)\n-    if (!PyObject_IsInstance(obj0, (PyObject *)&PyIOBase_Type)) {\n+    if (!PyObject_IsInstance(obj0, PyIOBase_TypeObj)) {\n       PyErr_SetString(PyExc_TypeError, \"not a file handle\");\n       return NULL;\n     }\n@@ -3523,7 +3534,7 @@\n   {\n #if PY_VERSION_HEX >= 0x03000000 || defined(PYPY_VERSION)\n #if !defined(PYPY_VERSION)\n-    if (!PyObject_IsInstance(obj1, (PyObject *)&PyIOBase_Type)) {\n+    if (!PyObject_IsInstance(obj1, PyIOBase_TypeObj)) {\n       PyErr_SetString(PyExc_TypeError, \"not a file handle\");\n       return NULL;\n     }\n@@ -6051,6 +6062,12 @@\n\n   SWIG_InstallConstants(d,swig_const_table);\n\n+#if PY_VERSION_HEX >= 0x03000000\n+  if (init_file_emulator() < 0) {\n+    return NULL;\n+  }\n+#endif\n+\n   PyDict_SetItemString(md,(char*)\"cvar\", SWIG_globals());\n   SWIG_addvarlink(SWIG_globals(),(char*)\"Agdirected\",Swig_var_Agdirected_get, Swig_var_Agdirected_set);\n   SWIG_addvarlink(SWIG_globals(),(char*)\"Agstrictdirected\",Swig_var_Agstrictdirected_get, Swig_var_Agstrictdirected_set);"
  },
  {
    "url": "https://stackoverflow.com/questions/45093811/installing-pygraphviz-on-windows-10-64-bit-python-3-6",
    "body": ">    [prompt]> :: Restore the original prompt as cwd is important\n>    [prompt]> exit\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811]> set _TOP_DIR=%CD%\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811]> pushd src\\pygraphviz\\pygraphviz-pygraphviz-1.5\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811\\src\\pygraphviz\\pygraphviz-pygraphviz-1.5]> pushd pygraphviz && \"c:\\Install\\x64\\Cygwin\\Cygwin\\AllVers\\bin\\patch.exe\" -p 1 -buNi ..\\pygraphviz-1.5-all-pyiobase_b85d12ac22d39063f7dbcc396e825c563431e352.patch && popd\n>    patching file graphviz_wrap.c\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811\\src\\pygraphviz\\pygraphviz-pygraphviz-1.5]> echo %errorlevel%\n>    0\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811\\src\\pygraphviz\\pygraphviz-pygraphviz-1.5]> \"e:\\Work\\Dev\\VEnvs\\py_064_03.06.08_test0\\Scripts\\python.exe\" setup.py install --include-path=%_TOP_DIR%\\include --library-path=%_TOP_DIR%\\bin\\Win\\dynamic\\064\\UCRTv140\\md\\Release\\graphviz >%_TOP_DIR%\\install_pygraphviz_064.txt 2>&1\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811\\src\\pygraphviz\\pygraphviz-pygraphviz-1.5]> echo %errorlevel%\n>    0\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811\\src\\pygraphviz\\pygraphviz-pygraphviz-1.5]> popd\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811]> set PATH=%PATH%;%CD%\\bin\\Win\\dynamic\\064\\UCRTv140\\md\\Release\\graphviz\n>\n>    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q045093811]> \"e:\\Work\\Dev\\VEnvs\\py_064_03.06.08_test0\\Scripts\\python.exe\" -c \"import pygraphviz;print(dir(pygraphviz), \\\"\\n\\\", pygraphviz.graphviz._graphviz)\"\n>    ['AGraph', 'Attribute', 'DotError', 'Edge', 'ItemAttribute', 'Node', '__all__', '__author__', '__builtins__', '__cached__', '__date__', '__doc__', '__file__', '__license__', '__loader__', '__name__', '__package__', '__path__', '__revision__', '__spec__', '__version__', 'absolute_import', 'agraph', 'division', 'graphviz', 'print_function', 'release', 'test', 'tests', 'version']\n>     <module '_graphviz' (e:\\Work\\Dev\\VEnvs\\py_064_03.06.08_test0\\lib\\site-packages\\pygraphviz\\_graphviz.cp36-win_amd64.pyd)>\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/61234609/how-to-import-python-package-from-another-directory",
    "body": "#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport io\nimport re\nfrom glob import glob\nfrom os.path import basename\nfrom os.path import dirname\nfrom os.path import join\nfrom os.path import splitext\nfrom setuptools import find_packages\nfrom setuptools import setup\ndef read(*names, **kwargs):\n    with io.open(\n        join(dirname(__file__), *names),\n        encoding=kwargs.get('encoding', 'utf8')\n    ) as fh:\n        return fh.read()\nsetup(\n    name='nameless',\n    version='1.644.11',\n    license='BSD-2-Clause',\n    description='An example package. Generated with cookiecutter-pylibrary.',\n    author='mpr',\n    author_email='contact@ionelmc.ro',\n    packages=find_packages('src'),\n    package_dir={'': 'src'},\n    include_package_data=True,\n    zip_safe=False,\n    classifiers=[\n        # complete classifier list: http://pypi.python.org/pypi?%3Aaction=list_classifiers\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: Unix',\n        'Operating System :: POSIX',\n        'Operating System :: Microsoft :: Windows',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n        # uncomment if you test on these interpreters:\n        # 'Programming Language :: Python :: Implementation :: IronPython',\n        # 'Programming Language :: Python :: Implementation :: Jython',\n        # 'Programming Language :: Python :: Implementation :: Stackless',\n        'Topic :: Utilities',\n    ],\n    keywords=[\n        # eg: 'keyword1', 'keyword2', 'keyword3',\n    ],\n    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*',\n    install_requires=[\n        # eg: 'aspectlib==1.1.1', 'six>=1.7',\n    ],\n    extras_require={\n        # eg:\n        #   'rst': ['docutils>=0.11'],\n        #   ':python_version==\"2.6\"': ['argparse'],\n    },\n    setup_requires=[\n        # 'pytest-runner',\n    ],\n    entry_points={\n        'console_scripts': [\n            'api = api.api:main',\n        ]\n    },\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/69864793/efficient-summation-in-python",
    "body": "from fractions import Fraction\nimport math\nfrom functools import reduce\ndef naive(n):\n    return sum(x**2 * sum(range(x+1)) for x in range(n+1))\ndef lcm(ints):\n    return reduce(lambda r, i: r * i // math.gcd(r, i), ints)\ndef polynomial(xys):\n    xs, ys = zip(*xys)\n    n = len(xs)\n    A = [[Fraction(x**i) for i in range(n)] for x in xs]\n    b = list(ys)\n    for _ in range(2):\n        for i0 in range(n):\n            for i in range(i0 + 1, n):\n                f = A[i][i0] / A[i0][i0]\n                for j in range(i0, n):\n                    A[i][j] -= f * A[i0][j]\n                b[i] -= f * b[i0]\n        A = [row[::-1] for row in A[::-1]]\n        b.reverse()\n    coeffs = [b[i] / A[i][i] for i in range(n)]\n    denominator = lcm(c.denominator for c in coeffs)\n    coeffs = [int(c * denominator) for c in coeffs]\n    horner = str(coeffs[-1])\n    for c in coeffs[-2::-1]:\n        horner += ' * n'\n        if c:\n            horner = f\"({horner} {'+' if c > 0 else '-'} {abs(c)})\"\n    return f'{horner} // {denominator}'\nprint(polynomial((x, naive(x)) for x in range(6)))"
  },
  {
    "url": "https://stackoverflow.com/questions/66209119/automation-google-login-with-python-and-selenium-shows-this-browser-or-app-may",
    "body": "from selenium import webdriver\nimport geckodriver_autoinstaller\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\ngeckodriver_autoinstaller.install()\nprofile = webdriver.FirefoxProfile(\n    '/Users/<user name>/Library/Application Support/Firefox/Profiles/xxxxx.default-release')\nprofile.set_preference(\"dom.webdriver.enabled\", False)\nprofile.set_preference('useAutomationExtension', False)\nprofile.update_preferences()\ndesired = DesiredCapabilities.FIREFOX\ndriver = webdriver.Firefox(firefox_profile=profile,\n                           desired_capabilities=desired)"
  },
  {
    "url": "https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile",
    "body": "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, Conv1D\nfrom tensorflow.keras.layers import Flatten, Dropout\nfrom tensorflow.keras.models import Model\nimport numpy as np\nfrom time import time\ndef timeit(func, arg, iterations):\n    t0 = time()\n    for _ in range(iterations):\n        func(arg)\n    print(\"%.4f sec\" % (time() - t0))\nbatch_size = 32\nbatch_shape = (batch_size, 400, 16)\nipt   = Input(batch_shape=batch_shape)\nx     = Bidirectional(LSTM(512, activation='relu', return_sequences=True))(ipt)\nx     = LSTM(512, activation='relu', return_sequences=True)(ipt)\nx     = Conv1D(128, 400, 1, padding='same')(x)\nx     = Flatten()(x)\nx     = Dense(256, activation='relu')(x)\nx     = Dropout(0.5)(x)\nx     = Dense(128, activation='relu')(x)\nx     = Dense(64,  activation='relu')(x)\nout   = Dense(1,  activation='sigmoid')(x)\nmodel = Model(ipt, out)\nX = np.random.randn(*batch_shape)\ntimeit(model.predict, X, 10)\nmodel.compile('adam', loss='binary_crossentropy')\ntimeit(model.predict, X, 10)"
  },
  {
    "url": "https://stackoverflow.com/questions/51125356/proper-way-to-build-menus-with-python-telegram-bot",
    "body": "#!/usr/bin/env python3.8\nfrom telegram.ext import Updater\nfrom telegram.ext import CommandHandler, CallbackQueryHandler\nfrom telegram import InlineKeyboardButton, InlineKeyboardMarkup\n############################### Bot ############################################\ndef start(bot, update):\n  bot.message.reply_text(main_menu_message(),\n                         reply_markup=main_menu_keyboard())\ndef main_menu(bot, update):\n  bot.callback_query.message.edit_text(main_menu_message(),\n                          reply_markup=main_menu_keyboard())\ndef first_menu(bot, update):\n  bot.callback_query.message.edit_text(first_menu_message(),\n                          reply_markup=first_menu_keyboard())\ndef second_menu(bot, update):\n  bot.callback_query.message.edit_text(second_menu_message(),\n                          reply_markup=second_menu_keyboard())\ndef first_submenu(bot, update):\n  pass\ndef second_submenu(bot, update):\n  pass\ndef error(update, context):\n    print(f'Update {update} caused error {context.error}')\n############################ Keyboards #########################################\ndef main_menu_keyboard():\n  keyboard = [[InlineKeyboardButton('Menu 1', callback_data='m1')],\n              [InlineKeyboardButton('Menu 2', callback_data='m2')],\n              [InlineKeyboardButton('Menu 3', callback_data='m3')]]\n  return InlineKeyboardMarkup(keyboard)\ndef first_menu_keyboard():\n  keyboard = [[InlineKeyboardButton('Submenu 1-1', callback_data='m1_1')],\n              [InlineKeyboardButton('Submenu 1-2', callback_data='m1_2')],\n              [InlineKeyboardButton('Main menu', callback_data='main')]]\n  return InlineKeyboardMarkup(keyboard)\ndef second_menu_keyboard():\n  keyboard = [[InlineKeyboardButton('Submenu 2-1', callback_data='m2_1')],\n              [InlineKeyboardButton('Submenu 2-2', callback_data='m2_2')],\n              [InlineKeyboardButton('Main menu', callback_data='main')]]\n  return InlineKeyboardMarkup(keyboard)\n############################# Messages #########################################\ndef main_menu_message():\n  return 'Choose the option in main menu:'\ndef first_menu_message():\n  return 'Choose the submenu in first menu:'\ndef second_menu_message():\n  return 'Choose the submenu in second menu:'\n############################# Handlers #########################################\nupdater = Updater('XXXXXXXXX:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', use_context=True)\nupdater.dispatcher.add_handler(CommandHandler('start', start))\nupdater.dispatcher.add_handler(CallbackQueryHandler(main_menu, pattern='main'))\nupdater.dispatcher.add_handler(CallbackQueryHandler(first_menu, pattern='m1'))\nupdater.dispatcher.add_handler(CallbackQueryHandler(second_menu, pattern='m2'))\nupdater.dispatcher.add_handler(CallbackQueryHandler(first_submenu, pattern='m1_1'))\nupdater.dispatcher.add_handler(CallbackQueryHandler(second_submenu, pattern='m2_1'))\nupdater.dispatcher.add_error_handler(error)\nupdater.start_polling()\n################################################################################"
  },
  {
    "url": "https://stackoverflow.com/questions/66807878/pretty-print-dataclasses-prettier-with-line-breaks-and-indentation",
    "body": "[ins] In [1]: from dataclasses import dataclass\n         ...:\n         ...: @dataclass\n         ...: class Point:\n         ...:     x: int\n         ...:     y: int\n         ...:\n         ...: @dataclass\n         ...: class Coords:\n         ...:     my_points: list\n         ...:     my_dict: dict\n         ...:\n         ...: coords = Coords([Point(1, 2), Point(3, 4)], {'a': (1, 2), (1, 2): 'a'})\n[ins] In [15]: pprint.pprint(coords, width=20)\nCoords(my_points=[Point(x=1,\n                        y=2),\n                  Point(x=3,\n                        y=4)],\n       my_dict={'a': (1,\n                      2),\n                (1, 2): 'a'})"
  },
  {
    "url": "https://stackoverflow.com/questions/66807878/pretty-print-dataclasses-prettier-with-line-breaks-and-indentation",
    "body": "[ins] In [1]: from dataclasses import dataclass\n         ...:\n         ...: @dataclass\n         ...: class Point:\n         ...:     x: int\n         ...:     y: int\n         ...:\n         ...: @dataclass\n         ...: class Coords:\n         ...:     my_points: list\n         ...:     my_dict: dict\n         ...:\n         ...: coords = Coords([Point(1, 2), Point(3, 4)], {'a': (1, 2), (1, 2): 'a'})\n[nav] In [2]: import prettyprinter as pp\n[ins] In [3]: pp.pprint(coords)\nCoords(my_points=[Point(x=1, y=2), Point(x=3, y=4)], my_dict={'a': (1, 2), (1, 2): 'a'})"
  },
  {
    "url": "https://stackoverflow.com/questions/23696705/how-to-upload-a-file-to-sharepoint-site-using-python-script",
    "body": "import os\nfrom office365.runtime.auth.authentication_context import AuthenticationContext\nfrom office365.sharepoint.client_context import ClientContext\nbaseurl = 'https://your_company.sharepoint.com'\nbasesite = '/path/to/site' # every share point has a home.\nsiteurl = baseurl + basesite\nlocalpath = './file.txt'\nremotepath = \"Shared Documents/file.txt\" # existing folder path under sharepoint site.\nctx_auth = AuthenticationContext(siteurl) # should also be the siteurl\nctx_auth.acquire_token_for_user(username, password)\nctx = ClientContext(siteurl, ctx_auth) # make sure you auth to the siteurl.\nwith open(localpath, 'rb') as content_file:\n    file_content = content_file.read()\ndir, name = os.path.split(remotepath)\nfile = ctx.web.get_folder_by_server_relative_url(dir).upload_file(name, file_content).execute_query()"
  },
  {
    "url": "https://stackoverflow.com/questions/74922314/yield-from-vs-yield-in-for-loop",
    "body": "    >>> class Class5(collections.abc.Generator):\n    ...     def __init__(self, gen):\n    ...         self.gen = gen\n    ...     def send(self, value):\n    ...         return next(self.gen)\n    ...     def throw(self, value):\n    ...         raise StopIteration\n    ...     def close(self):          # optional, but more complete\n    ...         self.gen.close()\n    ...\n    >>> e = Class5((i for i in range(10)))\n    >>> next(e)        # NOTE iter is not necessary!\n    0\n    >>> next(e)\n    1\n    >>> next(iter(e))  # but still works\n    2\n    >>> next(iter(e))  # doesn't close e?? (should it?)\n    3\n    >>> e.close()\n    >>> next(e)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/usr/lib/python3.9/_collections_abc.py\", line 330, in __next__\n        return self.send(None)\n      File \"<stdin>\", line 5, in send\n    StopIteration"
  },
  {
    "url": "https://stackoverflow.com/questions/74922314/yield-from-vs-yield-in-for-loop",
    "body": ">>> a = Class1((i for i in range(3)))\n>>> dis.dis(a.__iter__)\n  6           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                0 (gen)\n              4 GET_ITER\n        >>    6 FOR_ITER                10 (to 18)\n              8 STORE_FAST               1 (el)\n  7          10 LOAD_FAST                1 (el)\n             12 YIELD_VALUE\n             14 POP_TOP\n             16 JUMP_ABSOLUTE            6\n        >>   18 LOAD_CONST               0 (None)\n             20 RETURN_VALUE\n>>> b = Class2((i for i in range(3)))\n>>> dis.dis(b.__iter__)\n  6           0 LOAD_FAST                0 (self)\n              2 LOAD_ATTR                0 (gen)\n              4 GET_YIELD_FROM_ITER\n              6 LOAD_CONST               0 (None)\n              8\n             10 POP_TOP\n             12 LOAD_CONST               0 (None)\n             14 RETURN_VALUE"
  },
  {
    "url": "https://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-openais-apis-gpts",
    "body": "from openai import OpenAI\nfrom openai.types.beta.threads.message_create_params import (\n    Attachment,\n    AttachmentToolFileSearch,\n)\nimport os\nfilename = \"foobar.pdf\"\nprompt = \"Extract the content from the file provided without altering it. Just output its exact content and nothing else.\"\nclient = OpenAI(api_key=os.environ.get(\"MY_OPENAI_KEY\"))\npdf_assistant = client.beta.assistants.create(\n    model=\"gpt-4o\",\n    description=\"An assistant to extract the contents of PDF files.\",\n    tools=[{\"type\": \"file_search\"}],\n    name=\"PDF assistant\",\n)\n# Create thread\nthread = client.beta.threads.create()\nfile = client.files.create(file=open(filename, \"rb\"), purpose=\"assistants\")\n# Create assistant\nclient.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    attachments=[\n        Attachment(\n            file_id=file.id, tools=[AttachmentToolFileSearch(type=\"file_search\")]\n        )\n    ],\n    content=prompt,\n)\n# Run thread\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id, assistant_id=pdf_assistant.id, timeout=1000\n)\nif run.status != \"completed\":\n    raise Exception(\"Run failed:\", run.status)\nmessages_cursor = client.beta.threads.messages.list(thread_id=thread.id)\nmessages = [message for message in messages_cursor]\n# Output text\nres_txt = messages[0].content[0].text.value\nprint(res_txt)"
  },
  {
    "url": "https://stackoverflow.com/questions/60785825/vscode-how-to-pass-pytest-command-line-arguments-running-in-debugger",
    "body": "import pytest\ndef pytest_addoption(parser):\n    parser.addoption('--username', action='store', help='Repository user')\n    parser.addoption('--password', action='store', help='Repository password')\ndef pytest_generate_tests(metafunc):\n    username = metafunc.config.option.username\n    if 'username' in metafunc.fixturenames and username is not None:\n        metafunc.parametrize('username', [username])\n    password = metafunc.config.option.password\n    if 'password' in metafunc.fixturenames and password is not None:\n        metafunc.parametrize('password', [password])"
  },
  {
    "url": "https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier",
    "body": "class XGBClassifier(XGBModel, XGBClassifierBase):\n    # pylint: disable=missing-docstring,invalid-name,too-many-instance-attributes\n    def __init__(self, objective=\"binary:logistic\", **kwargs):\n        super().__init__(objective=objective, **kwargs)\n    def fit(self, X, y, sample_weight=None, base_margin=None,\n            eval_set=None, eval_metric=None,\n            early_stopping_rounds=None, verbose=True, xgb_model=None,\n            sample_weight_eval_set=None, callbacks=None):\n        # pylint: disable = attribute-defined-outside-init,arguments-differ\n        evals_result = {}\n        self.classes_ = np.unique(y)\n        self.n_classes_ = len(self.classes_)\n        xgb_options = self.get_xgb_params()\n        if callable(self.objective):\n            obj = _objective_decorator(self.objective)\n            # Use default value. Is it really not used ?\n            xgb_options[\"objective\"] = \"binary:logistic\"\n        else:\n            obj = None\n        if self.n_classes_ > 2:\n            # Switch to using a multiclass objective in the underlying\n            # XGB instance\n            xgb_options['objective'] = 'multi:softprob'\n            xgb_options['num_class'] = self.n_classes_"
  },
  {
    "url": "https://stackoverflow.com/questions/72465421/how-to-use-poetry-with-docker",
    "body": "FROM python:3.10\n# Configure Poetry\nENV POETRY_VERSION=1.2.0\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VENV=/opt/poetry-venv\nENV POETRY_CACHE_DIR=/opt/.cache\n# Install poetry separated from system interpreter\nRUN python3 -m venv $POETRY_VENV \\\n\t&& $POETRY_VENV/bin/pip install -U pip setuptools \\\n\t&& $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n# Add `poetry` to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\nWORKDIR /app\n# Install dependencies\nCOPY poetry.lock pyproject.toml ./\nRUN poetry install\n# Run your app\nCOPY . /app\nCMD [ \"poetry\", \"run\", \"python\", \"-c\", \"print('Hello, World!')\" ]"
  },
  {
    "url": "https://stackoverflow.com/questions/72465421/how-to-use-poetry-with-docker",
    "body": "FROM python:3.10 as python-base\n# https://python-poetry.org/docs#ci-recommendations\nENV POETRY_VERSION=1.2.0\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VENV=/opt/poetry-venv\n# Tell Poetry where to place its cache and virtual environment\nENV POETRY_CACHE_DIR=/opt/.cache\n# Create stage for Poetry installation\nFROM python-base as poetry-base\n# Creating a virtual environment just for poetry and install it with pip\nRUN python3 -m venv $POETRY_VENV \\\n\t&& $POETRY_VENV/bin/pip install -U pip setuptools \\\n\t&& $POETRY_VENV/bin/pip install poetry==${POETRY_VERSION}\n# Create a new stage from the base python image\nFROM python-base as example-app\n# Copy Poetry to app image\nCOPY --from=poetry-base ${POETRY_VENV} ${POETRY_VENV}\n# Add Poetry to PATH\nENV PATH=\"${PATH}:${POETRY_VENV}/bin\"\nWORKDIR /app\n# Copy Dependencies\nCOPY poetry.lock pyproject.toml ./\n# [OPTIONAL] Validate the project is properly configured\nRUN poetry check\n# Install Dependencies\nRUN poetry install --no-interaction --no-cache --without dev\n# Copy Application\nCOPY . /app\n# Run Application\nEXPOSE 5000\nCMD [ \"poetry\", \"run\", \"python\", \"-m\", \"flask\", \"run\", \"--host=0.0.0.0\" ]"
  },
  {
    "url": "https://stackoverflow.com/questions/61140398/fastapi-return-a-file-response-with-the-output-of-a-sql-query",
    "body": "from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport io\nimport pandas as pd\napp = FastAPI()\n@app.get(\"/get_csv\")\nasync def get_csv():\n    df = pd.DataFrame(dict(col1 = 1, col2 = 2), index=[0])\n    stream = io.StringIO()\n    df.to_csv(stream, index = False)\n    response = StreamingResponse(iter([stream.getvalue()]),\n                                 media_type=\"text/csv\"\n                                )\n    response.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n    return response"
  },
  {
    "url": "https://stackoverflow.com/questions/17985216/simpler-way-to-draw-a-circle-with-tkinter",
    "body": "try:\n    import tkinter as tk\nexcept ImportError:\n    import Tkinter as tk  # Python 2\nroot = tk.Tk()\ncanvas = tk.Canvas(root, width=200, height=200, borderwidth=0, highlightthickness=0,\n                   bg=\"black\")\ncanvas.grid()\ndef _create_circle(self, x, y, r, **kwargs):\n    return self.create_oval(x-r, y-r, x+r, y+r, **kwargs)\ntk.Canvas.create_circle = _create_circle\ndef _create_circle_arc(self, x, y, r, **kwargs):\n    if \"start\" in kwargs and \"end\" in kwargs:\n        kwargs[\"extent\"] = kwargs.pop(\"end\") - kwargs[\"start\"]\n    return self.create_arc(x-r, y-r, x+r, y+r, **kwargs)\ntk.Canvas.create_circle_arc = _create_circle_arc\ncanvas.create_circle(100, 120, 50, fill=\"blue\", outline=\"#DDD\", width=4)\ncanvas.create_circle_arc(100, 120, 48, fill=\"green\", outline=\"\", start=45, end=140)\ncanvas.create_circle_arc(100, 120, 48, fill=\"green\", outline=\"\", start=275, end=305)\ncanvas.create_circle_arc(100, 120, 45, style=\"arc\", outline=\"white\", width=6,\n                         start=270-25, end=270+25)\ncanvas.create_circle(150, 40, 20, fill=\"#BBB\", outline=\"\")\nroot.title(\"Circles and Arcs\")\nroot.mainloop()"
  },
  {
    "url": "https://stackoverflow.com/questions/44193823/get-existing-table-using-sqlalchemy-metadata",
    "body": "from sqlalchemy import (MetaData, Table, Column, Integer, String, Sequence,\n                        create_engine)\nCONN = create_engine('sqlite:///db.sql')\nMETA_DATA = MetaData(bind=CONN, reflect=True)\nUSERS_TABLE = Table(\"users\", META_DATA,\n                    Column(\"id\", Integer, Sequence(\"user_id_seq\"),\n                           primary_key=True),\n                    Column(\"first_name\", String(255)),\n                    Column(\"last_name\", String(255)),\n                    keep_existing=True)\nMETA_DATA.create_all(CONN, checkfirst=True)"
  },
  {
    "url": "https://stackoverflow.com/questions/63895392/seaborn-is-not-plotting-within-defined-subplots",
    "body": "import seaborn as sns\nimport matplotlib.pyplot as plt\n# load data\npenguins = sns.load_dataset(\"penguins\", cache=False)\n# display(penguins.head())\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1           18.7              181.0       3750.0    MALE\n1  Adelie  Torgersen            39.5           17.4              186.0       3800.0  FEMALE\n2  Adelie  Torgersen            40.3           18.0              195.0       3250.0  FEMALE\n3  Adelie  Torgersen             NaN            NaN                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7           19.3              193.0       3450.0  FEMALE"
  },
  {
    "url": "https://stackoverflow.com/questions/63895392/seaborn-is-not-plotting-within-defined-subplots",
    "body": "# create a long dataframe\ndfl = penguins.melt(id_vars='species', value_vars=['bill_length_mm', 'bill_depth_mm'], var_name='bill_size', value_name='vals')\n# display(dfl.head())\n  species       bill_size  vals\n0  Adelie  bill_length_mm  39.1\n1  Adelie   bill_depth_mm  18.7\n2  Adelie  bill_length_mm  39.5\n3  Adelie   bill_depth_mm  17.4\n4  Adelie  bill_length_mm  40.3\n# plot\nsns.displot(data=dfl, x='vals', col='bill_size', kde=True, stat='density', common_bins=False, common_norm=False, height=4, facet_kws={'sharey': False, 'sharex': False})"
  },
  {
    "url": "https://stackoverflow.com/questions/57628064/automating-python-package-release-process",
    "body": "$ poetry update           # update dependencies, may be skipped\n$ poetry version patch    # bump version\nBumping version from 1.1.2 to 1.1.3\n# finalize git stuff, e.g. add -u, commit -m 'v1.1.3', tag v1.1.3, push\n$ poetry publish --build  # build and publish to PyPI\nBuilding my_django_lib (1.1.3)\n - Building sdist\n - Built my_django_lib-1.1.3.tar.gz\n - Building wheel\n - Built my_django_lib-1.1.3-py3-none-any.whl\nPublishing my_django_lib (1.1.3) to PyPI\n - Uploading my_django_lib-1.1.3-py3-none-any.whl 100%\n - Uploading my_django_lib-1.1.3.tar.gz 100%"
  },
  {
    "url": "https://stackoverflow.com/questions/43150687/colorbar-limits-are-not-respecting-set-vmin-vmax-in-plt-contourf-how-can-i-more",
    "body": "def myfunction():\n\n    def bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0, mux=0.0, muy=0.0, sigmaxy=0.0):\n        \"\"\"copied from here: https://github.com/matplotlib/matplotlib/blob/81e8154dbba54ac1607b21b22984cabf7a6598fa/lib/matplotlib/mlab.py#L1866\"\"\"\n        Xmu = X-mux\n        Ymu = Y-muy\n        rho = sigmaxy/(sigmax*sigmay)\n        z = Xmu**2/sigmax**2 + Ymu**2/sigmay**2 - 2*rho*Xmu*Ymu/(sigmax*sigmay)\n        denom = 2*np.pi*sigmax*sigmay*np.sqrt(1-rho**2)\n        return np.exp(-z/(2*(1-rho**2))) / denom\n\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)\n    Z2 = bivariate_normal(X, Y, 1.5, 0.5, 1, 1)\n    Z = 10.0 * (Z2 - Z1)\n    return X,Y,Z"
  },
  {
    "url": "https://stackoverflow.com/questions/43150687/colorbar-limits-are-not-respecting-set-vmin-vmax-in-plt-contourf-how-can-i-more",
    "body": "def clippedcolorbar(CS, **kwargs):\n    from matplotlib.cm import ScalarMappable\n    from numpy import arange, floor, ceil\n    fig = CS.ax.get_figure()\n    vmin = CS.get_clim()[0]\n    vmax = CS.get_clim()[1]\n    m = ScalarMappable(cmap=CS.get_cmap())\n    m.set_array(CS.get_array())\n    m.set_clim(CS.get_clim())\n    step = CS.levels[1] - CS.levels[0]\n    cliplower = CS.zmin<vmin\n    clipupper = CS.zmax>vmax\n    noextend = 'extend' in kwargs.keys() and kwargs['extend']=='neither'\n    # set the colorbar boundaries\n    boundaries = arange((floor(vmin/step)-1+1*(cliplower and noextend))*step, (ceil(vmax/step)+1-1*(clipupper and noextend))*step, step)\n    kwargs['boundaries'] = boundaries\n    # if the z-values are outside the colorbar range, add extend marker(s)\n    # This behavior can be disabled by providing extend='neither' to the function call\n    if not('extend' in kwargs.keys()) or kwargs['extend'] in ['min','max']:\n        extend_min = cliplower or ( 'extend' in kwargs.keys() and kwargs['extend']=='min' )\n        extend_max = clipupper or ( 'extend' in kwargs.keys() and kwargs['extend']=='max' )\n        if extend_min and extend_max:\n            kwargs['extend'] = 'both'\n        elif extend_min:\n            kwargs['extend'] = 'min'\n        elif extend_max:\n            kwargs['extend'] = 'max'\n    return fig.colorbar(m, **kwargs)"
  },
  {
    "url": "https://stackoverflow.com/questions/49695990/authenticate-from-linux-to-windows-sql-server-with-pyodbc",
    "body": "#!/usr/bin/env python\n# minimal example using Kerberos auth\nimport sys\nimport re\nimport pyodbc\ndriver='{ODBC Driver 17 for SQL Server}'\nserver = sys.argv[1]\ndatabase = sys.argv[2]\n# trusted_connection uses kerberos ticket and ignores UID and PASSWORD in connection string\n# https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/using-integrated-authentication?view=sql-server-ver15\ntry:\n    cnxn = pyodbc.connect(driver=driver, server=server, database=database, trusted_connection='yes')\n    cursor = cnxn.cursor()\nexcept pyodbc.Error as ex:\n    msg = ex.args[1]\n    if re.search('No Kerberos', msg):\n        print('You must login using kinit before using this script.')\n        exit(1)\n    else:\n        raise\n# Sample select query\ncursor.execute(\"SELECT @@version;\")\nrow = cursor.fetchone()\nwhile row:\n    print(row[0])\n    row = cursor.fetchone()\nprint('success')"
  },
  {
    "url": "https://stackoverflow.com/questions/49695990/authenticate-from-linux-to-windows-sql-server-with-pyodbc",
    "body": "user@localhost:~# kdestroy # make sure there are no active tickets\nkdestroy: No credentials cache found while destroying cache\nuser@localhost:~# python pyodbc_sql_server_test.py tcp:dbserver.example.com mydatabase\nYou must login using kinit before using this script.\nuser@localhost:~# kinit\nPassword for user@DOMAIN.LOCAL:\nuser@localhost:~# python pyodbc_sql_server_test.py tcp:dbserver.example.com mydatabase\nMicrosoft SQL Server 2016 (SP2-GDR) (KB4505220) - 13.0.5101.9 (X64)\n        Jun 15 2019 23:15:58\n        Copyright (c) Microsoft Corporation\n        Enterprise Edition (64-bit) on Windows Server 2016 Datacenter 10.0 <X64> (Build 14393: )\nsuccess\nuser@localhost:~#"
  },
  {
    "url": "https://stackoverflow.com/questions/60422693/weird-indexing-using-numpy",
    "body": "x = np.arange(120).reshape(2,3,4,5)\ny = np.array([0,2,4])\n# x looks like:\narray([[[[  0,   1,   2,   3,   4],    -+      =+\n         [  5,   6,   7,   8,   9],     Sheet1  |\n         [ 10,  11,  12,  13,  14],     |       |\n         [ 15,  16,  17,  18,  19]],   -+       |\n                                                Workbook1\n        [[ 20,  21,  22,  23,  24],    -+       |\n         [ 25,  26,  27,  28,  29],     Sheet2  |\n         [ 30,  31,  32,  33,  34],     |       |\n         [ 35,  36,  37,  38,  39]],   -+       |\n                                                |\n        [[ 40,  41,  42,  43,  44],    -+       |\n         [ 45,  46,  47,  48,  49],     Sheet3  |\n         [ 50,  51,  52,  53,  54],     |       |\n         [ 55,  56,  57,  58,  59]]],  -+      =+\n       [[[ 60,  61,  62,  63,  64],\n         [ 65,  66,  67,  68,  69],\n         [ 70,  71,  72,  73,  74],\n         [ 75,  76,  77,  78,  79]],\n        [[ 80,  81,  82,  83,  84],\n         [ 85,  86,  87,  88,  89],\n         [ 90,  91,  92,  93,  94],\n         [ 95,  96,  97,  98,  99]],\n        [[100, 101, 102, 103, 104],\n         [105, 106, 107, 108, 109],\n         [110, 111, 112, 113, 114],\n         [115, 116, 117, 118, 119]]]])"
  },
  {
    "url": "https://stackoverflow.com/questions/53498226/what-is-the-meaning-of-exclamation-and-question-marks-in-jupyter-notebook",
    "body": "Signature: df.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)\nDocstring:\nFill NA/NaN values using the specified method\nParameters\n----------\nvalue : scalar, dict, Series, or DataFrame\n    Value to use to fill holes (e.g. 0), alternately a\n    dict/Series/DataFrame of values specifying which value to use for\n    each index (for a Series) or column (for a DataFrame). (values not\n    in the dict/Series/DataFrame will not be filled). This value cannot\n    be a list.\nmethod : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n    Method to use for filling holes in reindexed Series\n    pad / ffill: propagate last valid observation forward to next valid\n    backfill / bfill: use NEXT valid observation to fill gap\naxis : {0, 1, 'index', 'columns'}\ninplace : boolean, default False\n    If True, fill in place. Note: this will modify any\n    other views on this object, (e.g. a no-copy slice for a column in a\n    DataFrame).\nlimit : int, default None\n    If method is specified, this is the maximum number of consecutive\n    NaN values to forward/backward fill. In other words, if there is\n    a gap with more than this number of consecutive NaNs, it will only\n    be partially filled. If method is not specified, this is the\n    maximum number of entries along the entire axis where NaNs will be\n    filled.\ndowncast : dict, default is None\n    a dict of item->dtype of what to downcast if possible,\n    or the string 'infer' which will try to downcast to an appropriate\n    equal type (e.g. float64 to int64 if possible)\nSee Also\n--------\nreindex, asfreq\nReturns\n-------\nfilled : DataFrame\nFile:      c:\\users\\root\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\nType:      method"
  },
  {
    "url": "https://stackoverflow.com/questions/53498226/what-is-the-meaning-of-exclamation-and-question-marks-in-jupyter-notebook",
    "body": "Signature: np.argmax(a, axis=None, out=None)\nDocstring:\nReturns the indices of the maximum values along an axis.\nParameters\n----------\na : array_like\n    Input array.\naxis : int, optional\n    By default, the index is into the flattened array, otherwise\n    along the specified axis.\nout : array, optional\n    If provided, the result will be inserted into this array. It should\n    be of the appropriate shape and dtype.\nReturns\n-------\nindex_array : ndarray of ints\n    Array of indices into the array. It has the same shape as `a.shape`\n    with the dimension along `axis` removed.\nSee Also\n--------\nndarray.argmax, argmin\namax : The maximum value along a given axis.\nunravel_index : Convert a flat index into an index tuple.\nNotes\n-----\nIn case of multiple occurrences of the maximum values, the indices\ncorresponding to the first occurrence are returned.\nExamples\n--------\n>>> a = np.arange(6).reshape(2,3)\n>>> a\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.argmax(a)\n5\n>>> np.argmax(a, axis=0)\narray([1, 1, 1])\n>>> np.argmax(a, axis=1)\narray([2, 2])\n>>> b = np.arange(6)\n>>> b[1] = 5\n>>> b\narray([0, 5, 2, 3, 4, 5])\n>>> np.argmax(b) # Only the first occurrence is returned.\n1\nFile:      c:\\users\\root\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\nType:      function"
  },
  {
    "url": "https://stackoverflow.com/questions/51720909/how-to-get-python-m-venv-to-directly-install-latest-pip-version",
    "body": "# in ~/.bashrc or wherever\nfunction ve() {\n    local py=\"python3\"\n    if [ ! -d ./.venv ]; then\n        echo \"creating venv...\"\n        if ! $py -m venv .venv --prompt=$(basename $PWD) --without-pip; then\n            echo \"ERROR: Problem creating venv\" >&2\n            return 1\n        else\n            local whl=$($py -c \"import pathlib, ensurepip; whl = list(pathlib.Path(ensurepip.__path__[0]).glob('_bundled/pip*.whl'))[0]; print(whl)\")\n            echo \"boostrapping pip using $whl\"\n            .venv/bin/python $whl/pip install --upgrade pip setuptools wheel\n            source .venv/bin/activate\n        fi\n    else\n        source .venv/bin/activate\n    fi\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/10988082/multivariate-polynomial-regression-with-numpy",
    "body": "import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n#X is the independent variable (bivariate in this case)\nX = np.array([[0.44, 0.68], [0.99, 0.23]])\n#vector is the dependent data\nvector = np.array([109.85, 155.72])\n#predict is an independent variable for which we'd like to predict the value\npredict= np.array([[0.49, 0.18]])\n#generate a model of polynomial features\npoly = PolynomialFeatures(degree=2)\n#transform the x data for proper fitting (for single variable type it returns,[1,x,x**2])\nX_ = poly.fit_transform(X)\n#transform the prediction to fit the model type\npredict_ = poly.fit_transform(predict)\n#here we can remove polynomial orders we don't want\n#for instance I'm removing the `x` component\nX_ = np.delete(X_,(1),axis=1)\npredict_ = np.delete(predict_,(1),axis=1)\n#generate the regression object\nclf = linear_model.LinearRegression()\n#preform the actual regression\nclf.fit(X_, vector)\nprint(\"X_ = \",X_)\nprint(\"predict_ = \",predict_)\nprint(\"Prediction = \",clf.predict(predict_))"
  },
  {
    "url": "https://stackoverflow.com/questions/30003068/how-to-get-a-list-of-all-indices-of-repeated-elements-in-a-numpy-array",
    "body": "import numpy as np\n# create a test array\nrecords_array = np.array([1, 2, 3, 1, 1, 3, 4, 3, 2])\n# creates an array of indices, sorted by unique element\nidx_sort = np.argsort(records_array)\n# sorts records array so all unique elements are together\nsorted_records_array = records_array[idx_sort]\n# returns the unique values, the index of the first occurrence of a value, and the count for each element\nvals, idx_start, count = np.unique(sorted_records_array, return_counts=True, return_index=True)\n# splits the indices into separate arrays\nres = np.split(idx_sort, idx_start[1:])\n#filter them with respect to their size, keeping only items occurring more than once\nvals = vals[count > 1]\nres = filter(lambda x: x.size > 1, res)"
  },
  {
    "url": "https://stackoverflow.com/questions/69334475/how-to-hint-at-number-types-i-e-subclasses-of-number-not-numbers-themselv",
    "body": ">>> # All classes have `object` in their mro\n>>> class Foo: pass\n>>> Foo.__mro__\n(<class '__main__.Foo'>, <class 'object'>)\n>>>\n>>> # Subclasses of a class have that class in their mro\n>>> class IntSubclass(int): pass\n>>> IntSubclass.__mro__\n(<class '__main__.IntSubclass'>, <class 'int'>, <class 'object'>)\n>>> issubclass(IntSubclass, int)\nTrue\n>>>\n>>> # But `Number` is not in the mro of `int`...\n>>> int.__mro__\n(<class 'int'>, <class 'object'>)\n>>> # ...Yet `int` still pretends to be a subclass of `Number`!\n>>> from numbers import Number\n>>> issubclass(int, Number)\nTrue\n>>> #?!?!!??"
  },
  {
    "url": "https://stackoverflow.com/questions/59586879/does-await-in-python-yield-to-the-event-loop",
    "body": "@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\nasync def sleep(delay, result=None, *, loop=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n    if loop is None:\n        loop = events.get_running_loop()\n    else:\n        warnings.warn(\"The loop argument is deprecated since Python 3.8, \"\n                      \"and scheduled for removal in Python 3.10.\",\n                      DeprecationWarning, stacklevel=2)\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()"
  },
  {
    "url": "https://stackoverflow.com/questions/59586879/does-await-in-python-yield-to-the-event-loop",
    "body": "import asyncio\nimport types\ndef task_print(s):\n    print(f\"{asyncio.current_task().get_name()}: {s}\")\nasync def other_task(s):\n    task_print(s)\nclass AwaitableCls:\n    def __await__(self):\n        task_print(\"    'Jumped straight into' another `await`; the act of `await awaitable` *itself* doesn't 'pause' anything\")\n        yield\n        task_print(\"    We're back to our awaitable object because that other task completed\")\n        asyncio.create_task(other_task(\"The event loop gets control when `yield` points (from an iterable coroutine) propagate up to the `current_task` through a suitable chain of `await` or `yield from` statements\"))\nasync def coro():\n    task_print(\"  'Jumped straight into' coro; the `await` keyword itself does nothing to 'pause' the current_task\")\n    await AwaitableCls()\n    task_print(\"  'Jumped straight back into' coro; we have another pending task, but leaving an `__await__` doesn't 'pause' the task any more than entering the `__await__` does\")\n@types.coroutine\ndef iterable_coro(context):\n    task_print(f\"`{context} iterable_coro`: pre-yield\")\n    yield None # None or a Future object are the only legitimate yields to the task in asyncio\n    task_print(f\"`{context} iterable_coro`: post-yield\")\nasync def original_task():\n    asyncio.create_task(other_task(\"Aha, but a (suitably unconsumed) *`yield`* DOES 'pause' the current_task allowing the event scheduler to `_wakeup` another task\"))\n    task_print(\"Original task\")\n    await coro()\n    task_print(\"'Jumped straight out of' coro. Leaving a coro, as with leaving/entering any awaitable, doesn't give control to the event loop\")\n    res = await iterable_coro(\"await\")\n    assert res is None\n    asyncio.create_task(other_task(\"This doesn't run until the very end because the generated None following the creation of this task is consumed by the `for` loop\"))\n    for y in iterable_coro(\"for y in\"):\n        task_print(f\"But 'ordinary' `yield` points (those which are consumed by the `current_task` itself) behave as ordinary without relinquishing control at the async/task-level; `y={y}`\")\n    task_print(\"Done with original task\")\nasyncio.get_event_loop().run_until_complete(original_task())"
  },
  {
    "url": "https://stackoverflow.com/questions/59586879/does-await-in-python-yield-to-the-event-loop",
    "body": "import types # no asyncio, nor any other loop framework\nasync def f1():\n    print(1)\n    print(await f2(),'= await f2()')\n    return 8\n@types.coroutine\ndef f2():\n    print(2)\n    print((yield 3),'= yield 3')\n    return 7\nclass F3:\n   def __await__(self):\n        print(4)\n        print((yield 5),'= yield 5')\n        print(10)\n        return 11\ntask1 = f1()\ntask2 = F3().__await__()\n\"\"\" You could say calls to send() represent our\n   \"manual task management\" in this script.\n\"\"\"\nprint(task1.send(None), '= task1.send(None)')\nprint(task2.send(None), '= task2.send(None)')\ntry:\n    print(task1.send(6), 'try task1.send(6)')\nexcept StopIteration as e:\n    print(e.value, '= except task1.send(6)')\ntry:\n    print(task2.send(9), 'try task2.send(9)')\nexcept StopIteration as e:\n    print(e.value, '= except task2.send(9)')"
  },
  {
    "url": "https://stackoverflow.com/questions/58986126/replacing-placeholder-for-tensorflow-v2",
    "body": "import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n@tf.function\ndef construct_graph(graph_dict, inputs, outputs):\n    queue = inputs[:]\n    make_dict = {}\n    for key, val in graph_dict.items():\n        if key in inputs:\n            make_dict[key] = tf.placeholder(tf.float32, name=key)\n        else:\n            make_dict[key] = None\n    # Breadth-First search of graph starting from inputs\n    while len(queue) != 0:\n        cur = graph_dict[queue[0]]\n        for outg in cur[\"outgoing\"]:\n            if make_dict[outg[0]]: # If discovered node, do add/multiply operation\n                make_dict[outg[0]] = tf.add(make_dict[outg[0]], tf.multiply(outg[1], make_dict[queue[0]]))\n            else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue\n                make_dict[outg[0]] = tf.multiply(make_dict[queue[0]], outg[1])\n                for outgo in graph_dict[outg[0]][\"outgoing\"]:\n                    queue.append(outgo[0])\n        queue.pop(0)\n    # Returns one data graph for each output\n    return [make_dict[x] for x in outputs]\ndef main():\n    graph_def = {\n        \"B\": {\n            \"incoming\": [],\n            \"outgoing\": [(\"A\", 1.0)]\n        },\n        \"C\": {\n            \"incoming\": [],\n            \"outgoing\": [(\"A\", 1.0)]\n        },\n        \"A\": {\n            \"incoming\": [(\"B\", 2.0), (\"C\", -1.0)],\n            \"outgoing\": [(\"D\", 3.0)]\n        },\n        \"D\": {\n            \"incoming\": [(\"A\", 2.0)],\n            \"outgoing\": []\n        }\n    }\n    outputs = construct_graph(graph_def, [\"B\", \"C\"], [\"A\"])\n    print(outputs)\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/58986126/replacing-placeholder-for-tensorflow-v2",
    "body": "import tensorflow as tf\ndef construct_graph(graph_dict, inputs, outputs):\n    queue = inputs[:]\n    make_dict = {}\n    for key, val in graph_dict.items():\n        if key in inputs:\n            # Use keras.Input instead of placeholders\n            make_dict[key] = tf.keras.Input(name=key, shape=(), dtype=tf.dtypes.float32)\n        else:\n            make_dict[key] = None\n    # Breadth-First search of graph starting from inputs\n    while len(queue) != 0:\n        cur = graph_dict[queue[0]]\n        for outg in cur[\"outgoing\"]:\n            if make_dict[outg[0]] is not None: # If discovered node, do add/multiply operation\n                make_dict[outg[0]] = tf.keras.layers.add([\n                    make_dict[outg[0]],\n                    tf.keras.layers.multiply(\n                        [[outg[1]], make_dict[queue[0]]],\n                    )],\n                )\n            else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue\n                make_dict[outg[0]] = tf.keras.layers.multiply(\n                    [make_dict[queue[0]], [outg[1]]]\n                )\n                for outgo in graph_dict[outg[0]][\"outgoing\"]:\n                    queue.append(outgo[0])\n        queue.pop(0)\n    # Returns one data graph for each output\n    model_inputs = [make_dict[key] for key in inputs]\n    model_outputs = [make_dict[key] for key in outputs]\n    return [tf.keras.Model(inputs=model_inputs, outputs=o) for o in model_outputs]\ndef main():\n    graph_def = {\n        \"B\": {\n            \"incoming\": [],\n            \"outgoing\": [(\"A\", 1.0)]\n        },\n        \"C\": {\n            \"incoming\": [],\n            \"outgoing\": [(\"A\", 1.0)]\n        },\n        \"A\": {\n            \"incoming\": [(\"B\", 2.0), (\"C\", -1.0)],\n            \"outgoing\": [(\"D\", 3.0)]\n        },\n        \"D\": {\n            \"incoming\": [(\"A\", 2.0)],\n            \"outgoing\": []\n        }\n    }\n    outputs = construct_graph(graph_def, [\"B\", \"C\"], [\"A\"])\n    print(\"Builded models:\", outputs)\n    for o in outputs:\n        o.summary(120)\n        print(\"Output:\", o((1.0, 1.0)))\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/58986126/replacing-placeholder-for-tensorflow-v2",
    "body": "Builded models: [<tensorflow.python.keras.engine.training.Model object at 0x7fa0b49f0f50>]\nModel: \"model\"\n________________________________________________________________________________________________________________________\nLayer (type)                           Output Shape               Param #       Connected to\n========================================================================================================================\nB (InputLayer)                         [(None,)]                  0\n________________________________________________________________________________________________________________________\nC (InputLayer)                         [(None,)]                  0\n________________________________________________________________________________________________________________________\ntf_op_layer_mul (TensorFlowOpLayer)    [(None,)]                  0             B[0][0]\n________________________________________________________________________________________________________________________\ntf_op_layer_mul_1 (TensorFlowOpLayer)  [(None,)]                  0             C[0][0]\n________________________________________________________________________________________________________________________\nadd (Add)                              (None,)                    0             tf_op_layer_mul[0][0]\n                                                                                tf_op_layer_mul_1[0][0]\n========================================================================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n________________________________________________________________________________________________________________________\nOutput: tf.Tensor([2.], shape=(1,), dtype=float32)"
  },
  {
    "url": "https://stackoverflow.com/questions/29766827/make-axis-ticks-labels-bold-when-using-usetex-true",
    "body": "import matplotlib.pyplot as plt\nimport numpy as np\ntmpData = np.random.random( 100 )\n# activate latex text rendering\nplt.rc('text', usetex=True)\nplt.rc('axes', linewidth=2)\nplt.rc('font', weight='bold')\nplt.rcParams['text.latex.preamble'] = r'\\usepackage{sfmath} \\boldmath'\n#create figure\nf = plt.figure(figsize=(10,10))\nax = plt.gca()\nplt.plot(np.arange(100), tmpData, label=r'\\textbf{Line 1}', linewidth=2)\nplt.ylabel(r'\\textbf{Y-AXIS}', fontsize=20)\nplt.xlabel(r'\\textbf{X-AXIS}', fontsize=20)\nax.xaxis.set_tick_params(labelsize=20)\nax.yaxis.set_tick_params(labelsize=20)\nplt.legend()"
  },
  {
    "url": "https://stackoverflow.com/questions/29766827/make-axis-ticks-labels-bold-when-using-usetex-true",
    "body": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime, timedelta\ntmpData = np.random.random(100)\nbase = datetime(2000, 1, 1)\narr = np.array([base + timedelta(days=i) for i in range(100)])\n# activate latex text rendering\nplt.rc('text', usetex=True)\nplt.rc('axes', linewidth=2)\nplt.rc('font', weight='bold')\nplt.rcParams['text.latex.preamble'] = r'\\usepackage{sfmath} \\boldmath'\n# create figure\nf = plt.figure(figsize=(10, 10))\nax = plt.gca()\nplt.plot(arr, tmpData, label=r'\\textbf{Line 1}', linewidth=2)\nplt.ylabel(r'\\textbf{Y-AXIS}', fontsize=20)\nplt.xlabel(r'\\textbf{X-AXIS}', fontsize=20)\nax.xaxis.set_tick_params(labelsize=20)\nax.yaxis.set_tick_params(labelsize=20)\n# Set x-axis major formatter and locator\nax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y'))\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nplt.legend()"
  },
  {
    "url": "https://stackoverflow.com/questions/45829353/python-type-checking-in-vs-code",
    "body": "{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"stopOnEntry\": false,\n            \"pythonPath\": \"${config:python.pythonPath}\",\n            \"program\": \"${file}\",\n            \"cwd\": \"${workspaceRoot}\",\n            \"env\": {},\n            \"envFile\": \"${workspaceRoot}/.env\",\n            \"debugOptions\": [\n                \"WaitOnAbnormalExit\",\n                \"WaitOnNormalExit\",\n                \"RedirectOutput\"\n            ]\n        }\n    ]\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/45943160/can-memmap-pandas-series-what-about-a-dataframe",
    "body": "from pandas.core.internals import BlockManager\nclass BlockManagerUnconsolidated(BlockManager):\n    def __init__(self, *args, **kwargs):\n        BlockManager.__init__(self, *args, **kwargs)\n        self._is_consolidated = False\n        self._known_consolidated = False\n    def _consolidate_inplace(self): pass\n    def _consolidate(self): return self.blocks\ndef df_from_arrays(arrays, columns, index):\n    from pandas.core.internals import make_block\n    def gen():\n        _len = None\n        p = 0\n        for a in arrays:\n            if _len is None:\n                _len = len(a)\n                assert len(index) == _len\n            assert _len == len(a)\n            yield make_block(values=a.reshape((1,_len)), placement=(p,))\n            p += 1\n    blocks = tuple(gen())\n    mgr = BlockManagerUnconsolidated(blocks=blocks, axes=[columns, index])\n    return pd.DataFrame(mgr, copy=False)"
  },
  {
    "url": "https://stackoverflow.com/questions/45943160/can-memmap-pandas-series-what-about-a-dataframe",
    "body": "def assert_readonly(iloc):\n    try:\n        iloc[0] = 999 # Should be non-editable\n        raise Exception(\"MUST BE READ ONLY (1)\")\n    except ValueError as e:\n        assert \"read-only\" in e.message\n# Original ndarray\nn = 1000\n_arr = np.arange(0,1000, dtype=float)\n# Convert it to a memmap\nmm = np.memmap(filename, mode='w+', shape=_arr.shape, dtype=_arr.dtype)\nmm[:] = _arr[:]\ndel _arr\nmm.flush()\nmm.flags['WRITEABLE'] = False  # Make immutable!\ndf = df_from_arrays(\n    [mm, mm, mm],\n    columns=['a', 'b', 'c'],\n    index=range(len(mm)))\nassert_read_only(df[\"a\"].iloc)\nassert_read_only(df[\"b\"].iloc)\nassert_read_only(df[\"c\"].iloc)"
  },
  {
    "url": "https://stackoverflow.com/questions/63272437/how-can-i-send-a-message-to-someone-with-telegram-api-using-my-own-account",
    "body": "import logging\nimport argparse\nfrom utils import setup_logging\nfrom telegram.client import Telegram\n\"\"\"\nSends a message to a chat\nUsage:\n    python examples/send_message.py api_id api_hash phone chat_id text\n\"\"\"\nif __name__ == '__main__':\n    setup_logging(level=logging.INFO)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('api_id', help='API id')  # https://my.telegram.org/apps\n    parser.add_argument('api_hash', help='API hash')\n    parser.add_argument('phone', help='Phone')\n    parser.add_argument('chat_id', help='Chat id', type=int)\n    parser.add_argument('text', help='Message text')\n    args = parser.parse_args()\n    tg = Telegram(\n        api_id=args.api_id,\n        api_hash=args.api_hash,\n        phone=args.phone,\n        database_encryption_key='changeme1234',\n    )\n    # you must call login method before others\n    tg.login()\n    # if this is the first run, library needs to preload all chats\n    # otherwise the message will not be sent\n    result = tg.get_chats()\n    # `tdlib` is asynchronous, so `python-telegram` always returns you an `AsyncResult` object.\n    # You can wait for a result with the blocking `wait` method.\n    result.wait()\n    if result.error:\n        print(f'get chats error: {result.error_info}')\n    else:\n        print(f'chats: {result.update}')\n    result = tg.send_message(\n        chat_id=args.chat_id,\n        text=args.text,\n    )\n    result.wait()\n    if result.error:\n        print(f'send message error: {result.error_info}')\n    else:\n        print(f'message has been sent: {result.update}')"
  },
  {
    "url": "https://stackoverflow.com/questions/56567841/django-count-and-sum-annotations-interfere-with-each-other",
    "body": ">>> from sandbox.models import Player\n>>> from django.db.models import Count, Sum\n>>> Player.objects.annotate(weapon_count=Count('unit_set__weapon_set')).values()\n<QuerySet [{'id': 1, 'name': 'player_1', 'weapon_count': 2}]>\n>>> Player.objects.annotate(rarity_sum=Sum('unit_set__rarity')).values()\n<QuerySet [{'id': 1, 'name': 'player_1', 'rarity_sum': 10}]>\n>>> Player.objects.annotate(\n...     weapon_count=Count('unit_set__weapon_set', distinct=True),\n...     rarity_sum=Sum('unit_set__rarity')).values()\n<QuerySet [{'id': 1, 'name': 'player_1', 'weapon_count': 2, 'rarity_sum': 20}]>"
  },
  {
    "url": "https://stackoverflow.com/questions/56567841/django-count-and-sum-annotations-interfere-with-each-other",
    "body": "sqlite> SELECT \"sandbox_player\".\"id\",\n   ...>        \"sandbox_player\".\"name\",\n   ...>        \"sandbox_weapon\".\"id\",\n   ...>        \"sandbox_unit\".\"rarity\"\n   ...> FROM \"sandbox_player\"\n   ...>          LEFT OUTER JOIN \"sandbox_unit\" ON (\"sandbox_player\".\"id\" = \"sandbox_unit\".\"player_id\")\n   ...>          LEFT OUTER JOIN \"sandbox_weapon\" ON (\"sandbox_unit\".\"id\" = \"sandbox_weapon\".\"unit_id\");\nid          name        id          rarity\n----------  ----------  ----------  ----------\n1           player_1    1           10\n1           player_1    2           10"
  },
  {
    "url": "https://stackoverflow.com/questions/56567841/django-count-and-sum-annotations-interfere-with-each-other",
    "body": ">>> from django.db.models import Count, IntegerField, OuterRef, Subquery, Sum\n>>> weapon_count = Player.objects.annotate(weapon_count=Count('unit_set__weapon_set')).filter(pk=OuterRef('pk'))\n>>> rarity_sum = Player.objects.annotate(rarity_sum=Sum('unit_set__rarity')).filter(pk=OuterRef('pk'))\n>>> qs = Player.objects.annotate(\n...     weapon_count=Subquery(weapon_count.values('weapon_count'), output_field=IntegerField()),\n...     rarity_sum=Subquery(rarity_sum.values('rarity_sum'), output_field=IntegerField())\n... )\n>>> qs.values()\n<QuerySet [{'id': 1, 'name': 'player_1', 'weapon_count': 2, 'rarity_sum': 10}]>"
  },
  {
    "url": "https://stackoverflow.com/questions/56567841/django-count-and-sum-annotations-interfere-with-each-other",
    "body": "SELECT \"sandbox_player\".\"id\", \"sandbox_player\".\"name\",\n(\n    SELECT COUNT(U2.\"id\") AS \"weapon_count\"\n    FROM \"sandbox_player\" U0\n    LEFT OUTER JOIN \"sandbox_unit\" U1\n        ON (U0.\"id\" = U1.\"player_id\")\n    LEFT OUTER JOIN \"sandbox_weapon\" U2\n        ON (U1.\"id\" = U2.\"unit_id\")\n    WHERE U0.\"id\" = (\"sandbox_player\".\"id\")\n    GROUP BY U0.\"id\", U0.\"name\"\n) AS \"weapon_count\",\n(\n    SELECT SUM(U1.\"rarity\") AS \"rarity_sum\"\n    FROM \"sandbox_player\" U0\n    LEFT OUTER JOIN \"sandbox_unit\" U1\n        ON (U0.\"id\" = U1.\"player_id\")\n    WHERE U0.\"id\" = (\"sandbox_player\".\"id\")\nGROUP BY U0.\"id\", U0.\"name\") AS \"rarity_sum\"\nFROM \"sandbox_player\""
  },
  {
    "url": "https://stackoverflow.com/questions/66521958/how-to-access-environment-secrets-from-a-github-workflow",
    "body": "jobs:\n  publish:\n    environment: CI    # <--- /!\\ Here is the link to the environment\n    needs: build\n    runs-on: ubuntu-latest\n    if: startsWith(github.ref, 'refs/tags/v')\n    steps:\n    - uses: actions/checkout@v2\n    # Some more steps here ...\n    - name: Publish to Test PyPI\n      env:\n        TWINE_USERNAME: \"__token__\"\n        TWINE_PASSWORD: ${{ secrets.TEST_PYPI_API_TOKEN }}\n        TWINE_REPOSITORY_URL: \"https://test.pypi.org/legacy/\"\n      run: |\n        echo KEY: '${TWINE_PASSWORD}'\n        twine check dist/*\n        twine upload --verbose --skip-existing dist/*"
  },
  {
    "url": "https://stackoverflow.com/questions/43618910/pil-drawing-a-semi-transparent-square-overlay-on-image",
    "body": "from PIL import Image, ImageDraw\nfrom io import BytesIO\nfrom urllib.request import urlopen\nTINT_COLOR = (0, 0, 0)  # Black\nTRANSPARENCY = .25  # Degree of transparency, 0-100%\nOPACITY = int(255 * TRANSPARENCY)\nurl = \"https://i.ytimg.com/vi/W4qijIdAPZA/maxresdefault.jpg\"\nwith BytesIO(urlopen(url).read()) as file:\n    img = Image.open(file)\n    img = img.convert(\"RGBA\")\n# Determine extent of the largest possible square centered on the image.\n# and the image's shorter dimension.\nif img.size[0] > img.size[1]:\n    shorter = img.size[1]\n    llx, lly = (img.size[0]-img.size[1]) // 2 , 0\nelse:\n    shorter = img.size[0]\n    llx, lly = 0, (img.size[1]-img.size[0]) // 2\n# Calculate upper point + 1 because second point needs to be just outside the\n# drawn rectangle when drawing rectangles.\nurx, ury = llx+shorter+1, lly+shorter+1\n# Make a blank image the same size as the image for the rectangle, initialized\n# to a fully transparent (0% opaque) version of the tint color, then draw a\n# semi-transparent version of the square on it.\noverlay = Image.new('RGBA', img.size, TINT_COLOR+(0,))\ndraw = ImageDraw.Draw(overlay)  # Create a context for drawing things on it.\ndraw.rectangle(((llx, lly), (urx, ury)), fill=TINT_COLOR+(OPACITY,))\n# Alpha composite these two images together to obtain the desired result.\nimg = Image.alpha_composite(img, overlay)\nimg = img.convert(\"RGB\") # Remove alpha for saving in jpg format.\nimg.save('dark-cat.jpg')\nimg.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/31948285/display-data-streamed-from-a-flask-view-as-it-updates",
    "body": "<p>This is the latest output: <span id=\"latest\"></span></p>\n<p>This is all the output:</p>\n<ul id=\"output\"></ul>\n<script>\n    var latest = document.getElementById('latest');\n    var output = document.getElementById('output');\n    var xhr = new XMLHttpRequest();\n    xhr.open('GET', '{{ url_for('stream') }}');\n    xhr.send();\n    var position = 0;\n    function handleNewData() {\n        // the response text include the entire response so far\n        // split the messages, then take the messages that haven't been handled yet\n        // position tracks how many messages have been handled\n        // messages end with a newline, so split will always show one extra empty message at the end\n        var messages = xhr.responseText.split('\\n');\n        messages.slice(position, -1).forEach(function(value) {\n            latest.textContent = value;  // update the latest value in place\n            // build and append a new item to a list to log all output\n            var item = document.createElement('li');\n            item.textContent = value;\n            output.appendChild(item);\n        });\n        position = messages.length - 1;\n    }\n    var timer;\n    timer = setInterval(function() {\n        // check the response for new data\n        handleNewData();\n        // stop checking once the response has ended\n        if (xhr.readyState == XMLHttpRequest.DONE) {\n            clearInterval(timer);\n            latest.textContent = 'Done';\n        }\n    }, 1000);\n</script>"
  },
  {
    "url": "https://stackoverflow.com/questions/61577643/python-how-to-use-fastapi-and-uvicorn-run-without-blocking-the-thread",
    "body": "import contextlib\nimport time\nimport threading\nimport uvicorn\nclass Server(uvicorn.Server):\n    def install_signal_handlers(self):\n        pass\n    @contextlib.contextmanager\n    def run_in_thread(self):\n        thread = threading.Thread(target=self.run)\n        thread.start()\n        try:\n            while not self.started:\n                time.sleep(1e-3)\n            yield\n        finally:\n            self.should_exit = True\n            thread.join()\nconfig = uvicorn.Config(\"example:app\", host=\"127.0.0.1\", port=5000, log_level=\"info\")\nserver = Server(config=config)\nwith server.run_in_thread():\n    # Server is started.\n    ...\n    # Server will be stopped once code put here is completed\n    ...\n# Server stopped."
  },
  {
    "url": "https://stackoverflow.com/questions/46274961/removing-horizontal-lines-in-image-opencv-python-matplotlib",
    "body": "import cv2\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n# Remove horizontal\nhorizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25,1))\ndetected_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\ncnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    cv2.drawContours(image, [c], -1, (255,255,255), 2)\n# Repair image\nrepair_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,6))\nresult = 255 - cv2.morphologyEx(255 - image, cv2.MORPH_CLOSE, repair_kernel, iterations=1)\ncv2.imshow('thresh', thresh)\ncv2.imshow('detected_lines', detected_lines)\ncv2.imshow('image', image)\ncv2.imshow('result', result)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/58404225/vs-code-python-move-to-next-line-on-run-ctrl-enter",
    "body": "###############################\n# 1. Install extension \"macros\" in Visual Code\n#\n# Hit View on top menu\n# Search for extension named \"macros\" (by geddski)\n# Install \"macros\" extension\n#\n###############################\n###############################\n# 2. Add code below to keybindings.json\n#\n# Hit <Ctrl> + <Shift> + <P>\n# Enter in search bar: JSON\n# Select Open keyboard shortcuts\n#\n###############################\n{\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.pythonExecSelectionAndCursorDown\",\n        \"when\": \"editorTextFocus && editorLangId == 'python'\"\n    }\n###############################\n# 3. Add code below to settings.json\n#\n# Hit <Ctrl> + <Shift> + <P>\n# Enter in search bar: JSON\n# Select Open settings\n#\n###############################\n\"macros\": {  // Note: this requires macros extension by publisher:\"geddski\"\n        \"pythonExecSelectionAndCursorDown\": [\n            \"python.execSelectionInTerminal\",\n            \"cursorDown\"\n        ]\n    }"
  },
  {
    "url": "https://stackoverflow.com/questions/41064961/how-to-use-python-in-net-core-application",
    "body": "public class PythonScript\n{\n    private ScriptEngine _engine;\n    public PythonScript()\n    {\n        _engine = Python.CreateEngine();\n    }\n    public TResult RunFromString<TResult>(string code, string variableName)\n    {\n\t\t// for easier debugging write it out to a file and call: _engine.CreateScriptSourceFromFile(filePath);\n        ScriptSource source = _engine.CreateScriptSourceFromString(code, SourceCodeKind.Statements);\n        CompiledCode cc = source.Compile();\n        ScriptScope scope = _engine.CreateScope();\n        cc.Execute(scope);\n        return scope.GetVariable<TResult>(variableName);\n    }\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/58660378/how-use-pytest-to-unit-test-sqlalchemy-orm-classes",
    "body": "from sqlalchemy import create_engine\nfrom sqlalchemy.orm import scoped_session, sessionmaker\n@pytest.fixture(scope='session')\ndef db_engine(request):\n    \"\"\"yields a SQLAlchemy engine which is suppressed after the test session\"\"\"\n    db_url = request.config.getoption(\"--dburl\")\n    engine_ = create_engine(db_url, echo=True)\n    yield engine_\n    engine_.dispose()\n@pytest.fixture(scope='session')\ndef db_session_factory(db_engine):\n    \"\"\"returns a SQLAlchemy scoped session factory\"\"\"\n    return scoped_session(sessionmaker(bind=db_engine))\n@pytest.fixture(scope='function')\ndef db_session(db_session_factory):\n    \"\"\"yields a SQLAlchemy connection which is rollbacked after the test\"\"\"\n    session_ = db_session_factory()\n    yield session_\n    session_.rollback()\n    session_.close()"
  },
  {
    "url": "https://stackoverflow.com/questions/71287550/repeatedly-removing-the-maximum-average-subarray",
    "body": "# Lengths of the segments in the upper convex hull\n# of the cumulative sum graph\ndef upperSumHullLengths(arr):\n    if len(arr) < 2:\n        if len(arr) < 1:\n            return []\n        else:\n            return [1]\n\n    hull = [(0, 0),(1, arr[0])]\n    for x in range(2, len(arr)+1):\n        # this has x coordinate x-1\n        prevPoint = hull[len(hull) - 1]\n        # next point in cumulative sum\n        point = (x, prevPoint[1] + arr[x-1])\n        # remove points not on the convex hull\n        while len(hull) >= 2:\n            p0 = hull[len(hull)-2]\n            dx0 = prevPoint[0] - p0[0]\n            dy0 = prevPoint[1] - p0[1]\n            dx1 = x - prevPoint[0]\n            dy1 = point[1] - prevPoint[1]\n            if dy1*dx0 < dy0*dx1:\n                break\n            hull.pop()\n            prevPoint = p0\n        hull.append(point)\n\n    return [hull[i+1][0] - hull[i][0] for i in range(0, len(hull)-1)]\nprint(upperSumHullLengths([  1,   7,   8,   4,   2,   1,   4]))"
  },
  {
    "url": "https://stackoverflow.com/questions/11645285/why-cprofile-module-doesnt-work-with-unittest",
    "body": "from pstats import Stats\nimport unittest\nclass TestSplayTree(unittest.TestCase):\n    \"\"\"a simple test\"\"\"\n    def setUp(self):\n        \"\"\"init each test\"\"\"\n        self.testtree = SplayTree (1000000)\n        self.pr = cProfile.Profile()\n        self.pr.enable()\n        print \"\\n<<<---\"\n    def tearDown(self):\n        \"\"\"finish any test\"\"\"\n        p = Stats (self.pr)\n        p.strip_dirs()\n        p.sort_stats ('cumtime')\n        p.print_stats ()\n        print \"\\n--->>>\"\n\n    def xtest1 (self):\n        pass"
  },
  {
    "url": "https://stackoverflow.com/questions/13756356/different-databases-for-different-apps-in-django",
    "body": "# DB router for app1\nclass App1DBRouter(object):\n        \"\"\"\n    A router to control db operations\n    \"\"\"\n    route_app_labels = {'app1'}\n    db_name = 'db_app1'\n    def db_for_read(self, model, **hints):\n        \"\"\"\n        Attempts to read auth and contenttypes models go to self.db_name.\n        \"\"\"\n        if model._meta.app_label in self.route_app_labels:\n            return self.db_name\n        return None\n    def db_for_write(self, model, **hints):\n        \"\"\"\n        Attempts to write auth and contenttypes models go to self.db_name.\n        \"\"\"\n        if model._meta.app_label in self.route_app_labels:\n            return self.db_name\n        return None\n    def allow_relation(self, obj1, obj2, **hints):\n        \"\"\"\n        Allow relations if a model in the auth or contenttypes apps is\n        involved.\n        \"\"\"\n        if (\n            obj1._meta.app_label in self.route_app_labels or\n            obj2._meta.app_label in self.route_app_labels\n        ):\n           return True\n        return None\n    def allow_migrate(self, db, app_label, model_name=None, **hints):\n        \"\"\"\n        Make sure the auth and contenttypes apps only appear in the\n        self.db_name database.\n        \"\"\"\n        if app_label in self.route_app_labels:\n            return db == self.db_name\n        return None"
  },
  {
    "url": "https://stackoverflow.com/questions/60255595/if-i-cache-a-spark-dataframe-and-then-overwrite-the-reference-will-the-original",
    "body": ">>> def func():\n...     data = spark.createDataFrame([[1],[2],[3]]).toDF('col1')\n...     data.cache()\n...     data.count()\n...     return data\n...\n>>> sc._jsc.getPersistentRDDs()\n{}\n>>> df = func()\n>>> sc._jsc.getPersistentRDDs()\n{71: JavaObject id=o234}\n>>> df2 = df.filter('col1 != 2')\n>>> del df\n>>> import gc\n>>> gc.collect()\n93\n>>> sc._jvm.System.gc()\n>>> sc._jsc.getPersistentRDDs()\n{71: JavaObject id=o240}\n>>> df2.select('*').explain()\n== Physical Plan ==\n*(1) Filter (isnotnull(col1#174L) AND NOT (col1#174L = 2))\n+- *(1) ColumnarToRow\n   +- InMemoryTableScan [col1#174L], [isnotnull(col1#174L), NOT (col1#174L = 2)]\n         +- InMemoryRelation [col1#174L], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- *(1) Project [_1#172L AS col1#174L]\n                  +- *(1) Scan ExistingRDD[_1#172L]\n>>> del df2\n>>> gc.collect()\n85\n>>> sc._jvm.System.gc()\n>>> sc._jsc.getPersistentRDDs()\n{71: JavaObject id=o250}"
  },
  {
    "url": "https://stackoverflow.com/questions/60255595/if-i-cache-a-spark-dataframe-and-then-overwrite-the-reference-will-the-original",
    "body": ">>> def func():\n...     data = spark.createDataFrame([[1],[2],[3]]).toDF('col1')\n...     data.cache()\n...     data.count()\n...     return data\n...\n>>> sc._jsc.getPersistentRDDs()\n{}\n>>> df = func()\n>>> sc._jsc.getPersistentRDDs()\n{86: JavaObject id=o317}\n>>> df = df.filter('col1 != 2')\n>>> import gc\n>>> gc.collect()\n244\n>>> sc._jvm.System.gc()\n>>> sc._jsc.getPersistentRDDs()\n{86: JavaObject id=o323}\n>>> df.select('*').explain()\n== Physical Plan ==\n*(1) Filter (isnotnull(col1#220L) AND NOT (col1#220L = 2))\n+- *(1) ColumnarToRow\n   +- InMemoryTableScan [col1#220L], [isnotnull(col1#220L), NOT (col1#220L = 2)]\n         +- InMemoryRelation [col1#220L], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- *(1) Project [_1#218L AS col1#220L]\n                  +- *(1) Scan ExistingRDD[_1#218L]\n>>> del df\n>>> gc.collect()\n85\n>>> sc._jvm.System.gc()\n>>> sc._jsc.getPersistentRDDs()\n{86: JavaObject id=o333}"
  },
  {
    "url": "https://stackoverflow.com/questions/60255595/if-i-cache-a-spark-dataframe-and-then-overwrite-the-reference-will-the-original",
    "body": ">>> def func():\n...     data = spark.createDataFrame([[1],[2],[3]]).toDF('col1')\n...     data.cache()\n...     data.count()\n...     return data\n...\n>>> sc._jsc.getPersistentRDDs()\n{}\n>>> df = func()\n>>> sc._jsc.getPersistentRDDs()\n{116: JavaObject id=o398}\n>>> df2 = df.filter('col1 != 2')\n>>> df2.select('*').explain()\n== Physical Plan ==\n*(1) Filter (isnotnull(col1#312L) AND NOT (col1#312L = 2))\n+- *(1) ColumnarToRow\n   +- InMemoryTableScan [col1#312L], [isnotnull(col1#312L), NOT (col1#312L = 2)]\n         +- InMemoryRelation [col1#312L], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- *(1) Project [_1#310L AS col1#312L]\n                  +- *(1) Scan ExistingRDD[_1#310L]\n>>> df.unpersist()\nDataFrame[col1: bigint]\n>>> sc._jsc.getPersistentRDDs()\n{}\n>>> df2.select('*').explain()\n== Physical Plan ==\n*(1) Project [_1#310L AS col1#312L]\n+- *(1) Filter (isnotnull(_1#310L) AND NOT (_1#310L = 2))\n   +- *(1) Scan ExistingRDD[_1#310L]"
  },
  {
    "url": "https://stackoverflow.com/questions/55180829/python-if-else-code-style-for-reduced-code-for-rounding-floats",
    "body": "`\n    if value < -0.95:        ts_folder = ''\n    elif value < -0.85:      ts_folder = r'\\-0.9'\n    elif value < -0.75:      ts_folder = r'\\-0.8'\n    elif value < -0.65:      ts_folder = r'\\-0.7'\n    elif value < -0.55:      ts_folder = r'\\-0.6'\n    elif value < -0.45:      ts_folder = r'\\-0.5'\n    elif value < -0.35:      ts_folder = r'\\-0.4'\n    elif value < -0.25:      ts_folder = r'\\-0.3'\n    elif value < -0.15:      ts_folder = r'\\-0.2'\n    elif value < -0.05:      ts_folder = r'\\-0.1'\n    elif value < 0.05:       ts_folder = r'\\0.0'\n    elif value < 0.15:       ts_folder = r'\\0.1'\n    elif value < 0.25:       ts_folder = r'\\0.2'\n    elif value < 0.35:       ts_folder = r'\\0.3'\n    elif value < 0.45:       ts_folder = r'\\0.4'\n    elif value < 0.55:       ts_folder = r'\\0.5'\n    elif value < 0.65:       ts_folder = r'\\0.6'\n    elif value < 0.75:       ts_folder = r'\\0.7'\n    elif value < 0.85:       ts_folder = r'\\0.8'\n    elif value < 0.95:       ts_folder = r'\\0.9'\n    else:                    ts_folder = ''"
  },
  {
    "url": "https://stackoverflow.com/questions/70680363/structural-pattern-matching-using-regex",
    "body": "# noinspection PyPep8Naming\n@dataclass\nclass regex_in:\n    string: str\n    match: re.Match = None\n    def __eq__(self, other: str | re.Pattern):\n        if isinstance(other, str):\n            other = re.compile(other)\n        assert isinstance(other, re.Pattern)\n        # TODO extend for search and match variants\n        self.match = other.fullmatch(self.string)\n        return self.match is not None\n    def __getitem__(self, group):\n        return self.match[group]\n# Note the `as m` in in the case specification\nmatch regex_in(validated_string):\n    case r'\\d(\\d)' as m:\n        print(f'The second digit is {m[1]}')\n        print(f'The whole match is {m.match}')"
  },
  {
    "url": "https://stackoverflow.com/questions/10085996/shutdown-socketserver-serve-forever-in-one-thread-python-application",
    "body": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\nimport SimpleHTTPServer\nimport SocketServer\nimport time\nimport thread\nclass MyHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n    def do_POST(self):\n        if self.path.startswith('/kill_server'):\n            print \"Server is going down, run it again manually!\"\n            def kill_me_please(server):\n                server.shutdown()\n            thread.start_new_thread(kill_me_please, (httpd,))\n            self.send_error(500)\nclass MyTCPServer(SocketServer.TCPServer):\n    def server_bind(self):\n        import socket\n        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.socket.bind(self.server_address)\nserver_address = ('', 8000)\nhttpd = MyTCPServer(server_address, MyHandler)\ntry:\n    httpd.serve_forever()\nexcept KeyboardInterrupt:\n    pass\nhttpd.server_close()"
  },
  {
    "url": "https://stackoverflow.com/questions/61514887/how-to-trigger-a-dag-on-the-success-of-a-another-dag-in-airflow-using-python",
    "body": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.sensors import ExternalTaskSensor\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2020, 4, 29),\n}\ndag = DAG('Child_dag', default_args=default_args, schedule_interval='@daily')\n# Use ExternalTaskSensor to listen to the Parent_dag and cook_dinner task\n# when cook_dinner is finished, Child_dag will be triggered\nwait_for_dinner = ExternalTaskSensor(\n    task_id='wait_for_dinner',\n    external_dag_id='Parent_dag',\n    external_task_id='cook_dinner',\n    start_date=datetime(2020, 4, 29),\n    execution_delta=timedelta(hours=1),\n    timeout=3600,\n)\nhave_dinner = DummyOperator(\n    task_id='have_dinner',\n    dag=dag,\n)\nplay_with_food = DummyOperator(\n    task_id='play_with_food',\n    dag=dag,\n)\nwait_for_dinner >> have_dinner\nwait_for_dinner >> play_with_food"
  },
  {
    "url": "https://stackoverflow.com/questions/60050586/pytorch-change-the-learning-rate-based-on-number-of-epochs",
    "body": "Epoch-1 lr: 0.1\nEpoch-2 lr: 0.1\nEpoch-3 lr: 0.1\nEpoch-4 lr: 0.1\nEpoch-5 lr: 0.1\nEpoch-6 lr: 0.010000000000000002\nEpoch-7 lr: 0.010000000000000002\nEpoch-8 lr: 0.010000000000000002\nEpoch-9 lr: 0.010000000000000002\nEpoch-10 lr: 0.010000000000000002\nEpoch-11 lr: 0.0010000000000000002\nEpoch-12 lr: 0.0010000000000000002\nEpoch-13 lr: 0.0010000000000000002\nEpoch-14 lr: 0.0010000000000000002\nEpoch-15 lr: 0.0010000000000000002\nEpoch-16 lr: 0.00010000000000000003\nEpoch-17 lr: 0.00010000000000000003\nEpoch-18 lr: 0.00010000000000000003\nEpoch-19 lr: 0.00010000000000000003\nEpoch-20 lr: 0.00010000000000000003"
  },
  {
    "url": "https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier",
    "body": "from sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nmnist = fetch_openml(\"mnist_784\")\ny = mnist.target\ny = y.astype(np.uint8)\nn_classes = len(set(y))\nY = label_binarize(mnist.target, classes=[*range(n_classes)])\nX_train, X_test, y_train, y_test = train_test_split(mnist.data,\n                                                    Y,\n                                                    random_state = 42)\nclf = OneVsRestClassifier(RandomForestClassifier(n_estimators=50,\n                             max_depth=3,\n                             random_state=0))\nclf.fit(X_train, y_train)\ny_score = clf.predict_proba(X_test)"
  },
  {
    "url": "https://stackoverflow.com/questions/53968770/how-to-set-up-local-file-references-in-python-jsonschema-document",
    "body": "import json\nfrom jsonschema import RefResolver, Draft7Validator\naddress=\"\"\"\n{\n  \"$id\": \"https://example.com/schemas/address\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"street_address\": { \"type\": \"string\" },\n    \"city\": { \"type\": \"string\" },\n    \"state\": { \"type\": \"string\" }\n  },\n  \"required\": [\"street_address\", \"city\", \"state\"],\n  \"additionalProperties\": false\n}\n\"\"\"\ncustomer=\"\"\"\n{\n  \"$id\": \"https://example.com/schemas/customer\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"first_name\": { \"type\": \"string\" },\n    \"last_name\": { \"type\": \"string\" },\n    \"shipping_address\": { \"$ref\": \"/schemas/address\" },\n    \"billing_address\": { \"$ref\": \"/schemas/address\" }\n  },\n  \"required\": [\"first_name\", \"last_name\", \"shipping_address\", \"billing_address\"],\n  \"additionalProperties\": false\n}\n\"\"\"\ndata = \"\"\"\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Doe\",\n  \"shipping_address\": {\n    \"street_address\": \"1600 Pennsylvania Avenue NW\",\n    \"city\": \"Washington\",\n    \"state\": \"DC\"\n  },\n  \"billing_address\": {\n    \"street_address\": \"1st Street SE\",\n    \"city\": \"Washington\",\n    \"state\": \"DC\"\n  }\n}\n\"\"\"\naddress_schema = json.loads(address)\ncustomer_schema = json.loads(customer)\nschema_store = {\n    address_schema['$id'] : address_schema,\n    customer_schema['$id'] : customer_schema,\n}\nresolver = RefResolver.from_schema(customer_schema, store=schema_store)\nvalidator = Draft7Validator(customer_schema, resolver=resolver)\njsonData = json.loads(data)\nvalidator.validate(jsonData)"
  },
  {
    "url": "https://stackoverflow.com/questions/40179593/how-to-get-the-coordinates-of-the-maximum-in-xarray",
    "body": "In [8]: da = xr.DataArray(\n   ...:     np.random.rand(2,3),\n   ...:     dims=list('ab'),\n   ...:     coords=dict(a=list('xy'), b=list('ijk'))\n   ...: )\nIn [14]: da\nOut[14]:\n<xarray.DataArray (a: 2, b: 3)>\narray([[0.63059257, 0.00155463, 0.60763418],\n       [0.19680788, 0.43953352, 0.05602777]])\nCoordinates:\n  * a        (a) <U1 'x' 'y'\n  * b        (b) <U1 'i' 'j' 'k'\nIn [13]: da.idxmax('a')\nOut[13]:\n<xarray.DataArray 'a' (b: 3)>\narray(['x', 'y', 'x'], dtype=object)\nCoordinates:\n  * b        (b) <U1 'i' 'j' 'k'"
  },
  {
    "url": "https://stackoverflow.com/questions/73599970/how-to-solve-wkhtmltopdf-reported-an-error-exit-with-code-1-due-to-network-err",
    "body": "download_view.py\nimport os\nimport pdfkit\nfrom django.http import FileResponse\nfrom django.template.loader import render_to_string\nfrom paypal.models import Invoice\nfrom website import settings\ndef download_as_pdf_view(request, pk):\n    # create PDF from HTML template file with context.\n    invoice = Invoice.objects.get(pk=pk)\n    context = {\n        # please set your contexts as dict.\n    }\n    _html = render_to_string('paypal/card_invoice_detail.html', context)\n     # remove header\n    _html = _html[_html.find('<body>'):]\n    # create new header\n    new_header = '''<!DOCTYPE html>\n    <html lang=\"ja\">\n    <head>\n    <meta charset=\"utf-8\"/>\n    </head>\n    <style>\n'''\n    # add style from css file. please change to your css file path.\n    css_path = os.path.join(settings.BASE_DIR, 'paypal', 'static', 'paypal', 'css', 'invoice.css')\n    with open(css_path, 'r') as f:\n        new_header += f.read()\n    new_header += '\\n</style>'\n    print(new_header)\n    # add head to html\n    _html = new_header + _html[_html.find('<body>'):]\n    with open('paypal/sample.html', 'w') as f: f.write(_html)  # for debug\n    # convert html to pdf\n    file_name = 'invoice.pdf'\n    pdf_path = os.path.join(settings.BASE_DIR, 'static', 'pdf', file_name)\n    pdfkit.from_string(_html, pdf_path, options={\"enable-local-file-access\": \"\"})\n    return FileResponse(open(pdf_path, 'rb'), filename=file_name, content_type='application/pdf')"
  },
  {
    "url": "https://stackoverflow.com/questions/66488807/pytorch-model-input-shape",
    "body": "class NetWidth(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n        self.fc2 = nn.Linear(32, 2)\n\n    def forward(self, x):\n        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n        out = out.view(-1, 16 * 8 * 8)\n        out = torch.tanh(self.fc1(out))\n        out = self.fc2(out)\n        return out"
  },
  {
    "url": "https://stackoverflow.com/questions/48258008/n-and-r-arguments-to-ipythons-timeit-magic",
    "body": ">>> r = %timeit -o ...\n7.46 ns ± 0.0788 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n>>> r.loops  # the \"number\" is called \"loops\" on the result\n100000000\n>>> r.repeat\n7\n>>> r.all_runs\n[0.7445439999999905,\n 0.7611092000000212,\n 0.7249667000000102,\n 0.7238135999999997,\n 0.7385598000000186,\n 0.7338551999999936,\n 0.7277425999999991]\n>>> r.best\n7.238135999999997e-09\n>>> r.average\n7.363701571428618e-09\n>>> min(r.all_runs) / r.loops  # calculated best by hand\n7.238135999999997e-09\n>>> from statistics import mean\n>>> mean(r.all_runs) / r.loops  # calculated average by hand\n7.363701571428619e-09"
  },
  {
    "url": "https://stackoverflow.com/questions/48258008/n-and-r-arguments-to-ipythons-timeit-magic",
    "body": "import timeit\nr = timeit.repeat('sum(1 for _ in range(10000))', number=1, repeat=1_000)\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.title('measuring summation of 10_000 1s')\nplt.ylabel('number of measurements')\nplt.xlabel('measured time [s]')\nplt.yscale('log')\nplt.hist(r, bins='auto', color='black', label='measurements')\nplt.tight_layout()\nplt.axvline(np.min(r), c='lime', label='min')\nplt.axvline(np.mean(r), c='red', label='mean')\nplt.axvline(np.median(r), c='blue', label='median')\nplt.legend()"
  },
  {
    "url": "https://stackoverflow.com/questions/68417319/initialize-python-dataclass-from-dictionary",
    "body": "from dataclasses import dataclass, fields\nclass DataClassUnpack:\n    classFieldCache = {}\n    @classmethod\n    def instantiate(cls, classToInstantiate, argDict):\n        if classToInstantiate not in cls.classFieldCache:\n            cls.classFieldCache[classToInstantiate] = {f.name for f in fields(classToInstantiate) if f.init}\n        fieldSet = cls.classFieldCache[classToInstantiate]\n        filteredArgDict = {k : v for k, v in argDict.items() if k in fieldSet}\n        return classToInstantiate(**filteredArgDict)\n@dataclass\nclass Req:\n    id: int\n    description: str\nreq = DataClassUnpack.instantiate(Req, {\"id\": 123, \"description\": \"hello\", \"data_a\": \"\"})\nprint(req)\nreq = DataClassUnpack.instantiate(Req, {\"id\": 456, \"description\": \"goodbye\", \"data_a\": \"my\", \"data_b\": \"friend\"})\nprint(req)\n@dataclass\nclass Req2:\n    id: int\n    description: str\n    data_a: str\nreq2 = DataClassUnpack.instantiate(Req2, {\"id\": 123, \"description\": \"hello\", \"data_a\": \"world\"})\nprint(req2)\nprint(\"\\nHere's a peek at the internals of DataClassUnpack:\")\nprint(DataClassUnpack.classFieldCache)"
  },
  {
    "url": "https://stackoverflow.com/questions/58642528/displaying-of-fastapi-validation-errors-to-end-users",
    "body": "from collections import defaultdict\nfrom fastapi import status\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import JSONResponse\n@app.exception_handler(RequestValidationError)\nasync def custom_form_validation_error(request, exc):\n    reformatted_message = defaultdict(list)\n    for pydantic_error in exc.errors():\n        loc, msg = pydantic_error[\"loc\"], pydantic_error[\"msg\"]\n        filtered_loc = loc[1:] if loc[0] in (\"body\", \"query\", \"path\") else loc\n        field_string = \".\".join(filtered_loc)  # nested fields with dot-notation\n        reformatted_message[field_string].append(msg)\n    return JSONResponse(\n        status_code=status.HTTP_400_BAD_REQUEST,\n        content=jsonable_encoder(\n            {\"detail\": \"Invalid request\", \"errors\": reformatted_message}\n        ),\n    )"
  },
  {
    "url": "https://stackoverflow.com/questions/55301343/how-to-define-the-structure-of-a-sankey-diagram-using-a-pandas-dataframe",
    "body": "import pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nnodes = [\n    ['ID', 'Label', 'Color'],\n    [0,'Remain+No – 28','#F27420'],\n    [1,'Leave+No – 16','#4994CE'],\n    [2,'Remain+Yes – 21','#FABC13'],\n    [3,'Leave+Yes – 14','#7FC241'],\n    [4,'Didn’t vote in at least one referendum – 21','#D3D3D3'],\n    [5,'46 – No','#8A5988']\n]\nlinks = [\n    ['Source','Target','Value','Link Color'],\n    [0,3,20,'rgba(253, 227, 212, 0.5)'],\n    [0,4,3,'rgba(242, 116, 32, 1)'],\n    [0,2,5,'rgba(253, 227, 212, 0.5)'],\n    [1,5,14,'rgba(219, 233, 246, 0.5)'],\n    [1,3,1,'rgba(73, 148, 206, 1)'],\n    [1,4,1,'rgba(219, 233, 246,0.5)'],\n    [1,2,10,'rgba(8, 233, 246,0.5)'],\n    [1,3,5,'rgba(219, 77, 246,0.5)'],\n    [1,5,12,'rgba(219, 4, 246,0.5)']\n]\nnodes_headers = nodes.pop(0)\nnodes_df = pd.DataFrame(nodes, columns = nodes_headers)\nlinks_headers = links.pop(0)\nlinks_df = pd.DataFrame(links, columns = links_headers)\ndata_trace = dict(\n    type='sankey',\n    domain = dict(\n      x =  [0,1],\n      y =  [0,1]\n    ),\n    orientation = \"h\",\n    valueformat = \".0f\",\n    node = dict(\n      pad = 10,\n      thickness = 30,\n      line = dict(\n        color = \"black\",\n        width = 0\n      ),\n      label =  nodes_df['Label'].dropna(axis=0, how='any'),\n      color = nodes_df['Color']\n    ),\n    link = dict(\n      source = links_df['Source'].dropna(axis=0, how='any'),\n      target = links_df['Target'].dropna(axis=0, how='any'),\n      value = links_df['Value'].dropna(axis=0, how='any'),\n      color = links_df['Link Color'].dropna(axis=0, how='any'),\n  )\n)\nlayout =  dict(\n    title = \"Scottish Referendum Voters who now want Independence\",\n    height = 772,\n    font = dict(\n      size = 10\n    ),\n)\nfig = dict(data=[data_trace], layout=layout)\niplot(fig, validate=False)"
  },
  {
    "url": "https://stackoverflow.com/questions/55301343/how-to-define-the-structure-of-a-sankey-diagram-using-a-pandas-dataframe",
    "body": "nodes = [\n    ['ID', 'Label', 'Color'],\n    [0,'Remain+No – 28','#F27420'],\n    [1,'Leave+No – 16','#4994CE'],\n    [2,'Remain+Yes – 21','#FABC13'],\n    [3,'Leave+Yes – 14','#7FC241'],\n    [4,'Didn’t vote in at least one referendum – 21','#D3D3D3'],\n    [5,'46 – No','#8A5988'],\n    [6,'WAKA1','#8A5988'],\n    [7,'WAKA2','#8A5988'],\n    [8,'WAKA3','#8A5988'],\n    [9,'WAKA4','#8A5988'],\n    [10,'WAKA5','#8A5988'],\n    [11,'WAKA6','#8A5988'],\n\n]\nlinks = [\n    ['Source','Target','Value','Link Color'],\n    [0,3,20,'rgba(253, 227, 212, 0.5)'],\n    [0,4,3,'rgba(242, 116, 32, 1)'],\n    [0,2,5,'rgba(253, 227, 212, 0.5)'],\n    [1,5,14,'rgba(219, 233, 246, 0.5)'],\n    [1,3,1,'rgba(73, 148, 206, 1)'],\n    [1,4,1,'rgba(219, 233, 246,0.5)'],\n    [1,2,10,'rgba(8, 233, 246,0.5)'],\n    [1,3,5,'rgba(219, 77, 246,0.5)'],\n    [1,5,12,'rgba(219, 4, 246,0.5)']\n]"
  },
  {
    "url": "https://stackoverflow.com/questions/40949746/how-to-display-flashing-message-without-reloading-the-page-in-flask",
    "body": "@app.route('/', methods=['GET', 'POST'])\ndef index():\n    form = Nameform()\n    if form.validate_on_submit():\n        old_name = session.get('name')\n        if old_name is not None and old_name != form.name.data:\n            flash('Looks like you have changed your name!')\n        session['name'] = form.name.data\n        form.name.data = ''\n        return redirect(url_for('index'))\n    return render_template('index.html', form=form, name=session.get('name'))\n        form = form, name = session.get('name'))"
  },
  {
    "url": "https://stackoverflow.com/questions/69970147/how-do-i-resolve-the-pygraphviz-error-on-mac-os",
    "body": "Collecting pygraphviz\n  Using cached pygraphviz-1.11.zip (120 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nBuilding wheels for collected packages: pygraphviz\n  Building wheel for pygraphviz (pyproject.toml) ... done\n  Created wheel for pygraphviz: filename=pygraphviz-1.11-cp311-cp311-macosx_13_0_arm64.whl size=100586 sha256=0d6e56168321d335706a6c6789a006b8d2831bbd32b1cd28bd8fb2238122f73b\n  Stored in directory: .../pip/wheels/c8/03/73/b754941d55845a8b326f6de528bc70e65774838c76effa6d51\nSuccessfully built pygraphviz\nInstalling collected packages: pygraphviz\nSuccessfully installed pygraphviz-1.11"
  },
  {
    "url": "https://stackoverflow.com/questions/22785849/drawing-multiple-edges-between-two-nodes-with-networkx",
    "body": "import matplotlib.pyplot as plt\nimport networkx as nx\nG = nx.DiGraph()\nedge_list = [(1, 2, {'w': 'A1'}), (2, 1, {'w': 'A2'}), (2, 3, {'w': 'B'}),\n             (3, 1, {'w': 'C'}),\n             (3, 4, {'w': 'D1'}), (4, 3, {'w': 'D2'}), (1, 5, {'w': 'E1'}),\n             (5, 1, {'w': 'E2'}),\n             (3, 5, {'w': 'F'}), (5, 4, {'w': 'G'})]\nG.add_edges_from(edge_list)\npos = nx.spring_layout(G, seed=5)\nfig, ax = plt.subplots()\nnx.draw_networkx_nodes(G, pos, ax=ax)\nnx.draw_networkx_labels(G, pos, ax=ax)\nfig.savefig(\"1.png\", bbox_inches='tight', pad_inches=0)"
  },
  {
    "url": "https://stackoverflow.com/questions/22785849/drawing-multiple-edges-between-two-nodes-with-networkx",
    "body": "import my_networkx as my_nx\nedge_weights = nx.get_edge_attributes(G, 'w')\ncurved_edge_labels = {edge: edge_weights[edge] for edge in curved_edges}\nstraight_edge_labels = {edge: edge_weights[edge] for edge in straight_edges}\nmy_nx.my_draw_networkx_edge_labels(G, pos, ax=ax,\n                                   edge_labels=curved_edge_labels, rotate=False,\n                                   rad=arc_rad)\nnx.draw_networkx_edge_labels(G, pos, ax=ax, edge_labels=straight_edge_labels,\n                             rotate=False)\nfig.savefig(\"3.png\", bbox_inches='tight', pad_inches=0)"
  },
  {
    "url": "https://stackoverflow.com/questions/22785849/drawing-multiple-edges-between-two-nodes-with-networkx",
    "body": "def my_draw_networkx_edge_labels(\n    G,\n    pos,\n    edge_labels=None,\n    label_pos=0.5,\n    font_size=10,\n    font_color=\"k\",\n    font_family=\"sans-serif\",\n    font_weight=\"normal\",\n    alpha=None,\n    bbox=None,\n    horizontalalignment=\"center\",\n    verticalalignment=\"center\",\n    ax=None,\n    rotate=True,\n    clip_on=True,\n    rad=0\n):\n    \"\"\"Draw edge labels.\n    Parameters\n    ----------\n    G : graph\n        A networkx graph\n    pos : dictionary\n        A dictionary with nodes as keys and positions as values.\n        Positions should be sequences of length 2.\n    edge_labels : dictionary (default={})\n        Edge labels in a dictionary of labels keyed by edge two-tuple.\n        Only labels for the keys in the dictionary are drawn.\n    label_pos : float (default=0.5)\n        Position of edge label along edge (0=head, 0.5=center, 1=tail)\n    font_size : int (default=10)\n        Font size for text labels\n    font_color : string (default='k' black)\n        Font color string\n    font_weight : string (default='normal')\n        Font weight\n    font_family : string (default='sans-serif')\n        Font family\n    alpha : float or None (default=None)\n        The text transparency\n    bbox : Matplotlib bbox, optional\n        Specify text box properties (e.g. shape, color etc.) for edge labels.\n        Default is {boxstyle='round', ec=(1.0, 1.0, 1.0), fc=(1.0, 1.0, 1.0)}.\n    horizontalalignment : string (default='center')\n        Horizontal alignment {'center', 'right', 'left'}\n    verticalalignment : string (default='center')\n        Vertical alignment {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n    ax : Matplotlib Axes object, optional\n        Draw the graph in the specified Matplotlib axes.\n    rotate : bool (deafult=True)\n        Rotate edge labels to lie parallel to edges\n    clip_on : bool (default=True)\n        Turn on clipping of edge labels at axis boundaries\n    Returns\n    -------\n    dict\n        `dict` of labels keyed by edge\n    Examples\n    --------\n    >>> G = nx.dodecahedral_graph()\n    >>> edge_labels = nx.draw_networkx_edge_labels(G, pos=nx.spring_layout(G))\n    Also see the NetworkX drawing examples at\n    https://networkx.org/documentation/latest/auto_examples/index.html\n    See Also\n    --------\n    draw\n    draw_networkx\n    draw_networkx_nodes\n    draw_networkx_edges\n    draw_networkx_labels\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    if ax is None:\n        ax = plt.gca()\n    if edge_labels is None:\n        labels = {(u, v): d for u, v, d in G.edges(data=True)}\n    else:\n        labels = edge_labels\n    text_items = {}\n    for (n1, n2), label in labels.items():\n        (x1, y1) = pos[n1]\n        (x2, y2) = pos[n2]\n        (x, y) = (\n            x1 * label_pos + x2 * (1.0 - label_pos),\n            y1 * label_pos + y2 * (1.0 - label_pos),\n        )\n        pos_1 = ax.transData.transform(np.array(pos[n1]))\n        pos_2 = ax.transData.transform(np.array(pos[n2]))\n        linear_mid = 0.5*pos_1 + 0.5*pos_2\n        d_pos = pos_2 - pos_1\n        rotation_matrix = np.array([(0,1), (-1,0)])\n        ctrl_1 = linear_mid + rad*rotation_matrix@d_pos\n        ctrl_mid_1 = 0.5*pos_1 + 0.5*ctrl_1\n        ctrl_mid_2 = 0.5*pos_2 + 0.5*ctrl_1\n        bezier_mid = 0.5*ctrl_mid_1 + 0.5*ctrl_mid_2\n        (x, y) = ax.transData.inverted().transform(bezier_mid)\n        if rotate:\n            # in degrees\n            angle = np.arctan2(y2 - y1, x2 - x1) / (2.0 * np.pi) * 360\n            # make label orientation \"right-side-up\"\n            if angle > 90:\n                angle -= 180\n            if angle < -90:\n                angle += 180\n            # transform data coordinate angle to screen coordinate angle\n            xy = np.array((x, y))\n            trans_angle = ax.transData.transform_angles(\n                np.array((angle,)), xy.reshape((1, 2))\n            )[0]\n        else:\n            trans_angle = 0.0\n        # use default box of white with white border\n        if bbox is None:\n            bbox = dict(boxstyle=\"round\", ec=(1.0, 1.0, 1.0), fc=(1.0, 1.0, 1.0))\n        if not isinstance(label, str):\n            label = str(label)  # this makes \"1\" and 1 labeled the same\n        t = ax.text(\n            x,\n            y,\n            label,\n            size=font_size,\n            color=font_color,\n            family=font_family,\n            weight=font_weight,\n            alpha=alpha,\n            horizontalalignment=horizontalalignment,\n            verticalalignment=verticalalignment,\n            rotation=trans_angle,\n            transform=ax.transData,\n            bbox=bbox,\n            zorder=1,\n            clip_on=clip_on,\n        )\n        text_items[(n1, n2)] = t\n    ax.tick_params(\n        axis=\"both\",\n        which=\"both\",\n        bottom=False,\n        left=False,\n        labelbottom=False,\n        labelleft=False,\n    )\n    return text_items"
  },
  {
    "url": "https://stackoverflow.com/questions/50443411/how-to-load-a-tflite-model-in-script",
    "body": "import numpy as np\nimport tensorflow as tf\n# Load TFLite model and allocate tensors.\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\ninterpreter.allocate_tensors()\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n# Test model on random input data.\ninput_shape = input_details[0]['shape']\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\ninterpreter.set_tensor(input_details[0]['index'], input_data)\ninterpreter.invoke()\n# The function `get_tensor()` returns a copy of the tensor data.\n# Use `tensor()` in order to get a pointer to the tensor.\noutput_data = interpreter.get_tensor(output_details[0]['index'])\nprint(output_data)"
  },
  {
    "url": "https://stackoverflow.com/questions/21104664/extract-all-bounding-boxes-using-opencv-python",
    "body": "import cv2\nimport numpy as np\n# Load image, grayscale, Otsu's threshold\nimage = cv2.imread('1.png')\noriginal = image.copy()\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n# Find contours, obtain bounding box, extract and save ROI\nROI_number = 0\ncnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    x,y,w,h = cv2.boundingRect(c)\n    cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n    ROI = original[y:y+h, x:x+w]\n    cv2.imwrite('ROI_{}.png'.format(ROI_number), ROI)\n    ROI_number += 1\ncv2.imshow('image', image)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/3114786/python-library-to-extract-epub-information",
    "body": "import zipfile\nfrom lxml import etree\ndef epub_info(fname):\n    def xpath(element, path):\n        return element.xpath(\n            path,\n            namespaces={\n                \"n\": \"urn:oasis:names:tc:opendocument:xmlns:container\",\n                \"pkg\": \"http://www.idpf.org/2007/opf\",\n                \"dc\": \"http://purl.org/dc/elements/1.1/\",\n            },\n        )[0]\n    # prepare to read from the .epub file\n    zip_content = zipfile.ZipFile(fname)\n\n    # find the contents metafile\n    cfname = xpath(\n        etree.fromstring(zip_content.read(\"META-INF/container.xml\")),\n        \"n:rootfiles/n:rootfile/@full-path\",\n    )\n\n    # grab the metadata block from the contents metafile\n    metadata = xpath(\n        etree.fromstring(zip_content.read(cfname)), \"/pkg:package/pkg:metadata\"\n    )\n\n    # repackage the data\n    return {\n        s: xpath(metadata, f\"dc:{s}/text()\")\n        for s in (\"title\", \"language\", \"creator\", \"date\", \"identifier\")\n    }"
  },
  {
    "url": "https://stackoverflow.com/questions/61316540/how-to-get-python-fastapi-async-await-functionality-to-work-properly",
    "body": "from fastapi import FastAPI\nimport time\nimport asyncio\napp = FastAPI()\nasync def my_func_1():\n    \"\"\"\n    my func 1\n    \"\"\"\n    print('Func1 started..!!')\n    await asyncio.sleep(5)\n    print('Func1 ended..!!')\n    return 'a..!!'\nasync def my_func_2():\n    \"\"\"\n    my func 2\n    \"\"\"\n    print('Func2 started..!!')\n    await asyncio.sleep(5)\n    print('Func2 ended..!!')\n    return 'b..!!'\n@app.get(\"/home\")\nasync def root():\n    \"\"\"\n    my home route\n    \"\"\"\n    start = time.time()\n    futures = [my_func_1(), my_func_2()]\n    a,b = await asyncio.gather(*futures)\n    end = time.time()\n    print('It took {} seconds to finish execution.'.format(round(end-start)))\n    return {\n        'a': a,\n        'b': b\n    }"
  },
  {
    "url": "https://stackoverflow.com/questions/59182827/how-to-get-the-cells-of-a-sudoku-grid-with-opencv",
    "body": "import cv2\nfrom imutils import contours\nimport numpy as np\n# Load image, grayscale, and adaptive threshold\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,57,5)\n# Filter out all numbers and noise to isolate only boxes\ncnts = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    area = cv2.contourArea(c)\n    if area < 1000:\n        cv2.drawContours(thresh, [c], -1, (0,0,0), -1)\n# Fix horizontal and vertical lines\nvertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,5))\nthresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, vertical_kernel, iterations=9)\nhorizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,1))\nthresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, horizontal_kernel, iterations=4)\n# Sort by top to bottom and each row by left to right\ninvert = 255 - thresh\ncnts = cv2.findContours(invert, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\n(cnts, _) = contours.sort_contours(cnts, method=\"top-to-bottom\")\nsudoku_rows = []\nrow = []\nfor (i, c) in enumerate(cnts, 1):\n    area = cv2.contourArea(c)\n    if area < 50000:\n        row.append(c)\n        if i % 9 == 0:\n            (cnts, _) = contours.sort_contours(row, method=\"left-to-right\")\n            sudoku_rows.append(cnts)\n            row = []\n# Iterate through each box\nfor row in sudoku_rows:\n    for c in row:\n        mask = np.zeros(image.shape, dtype=np.uint8)\n        cv2.drawContours(mask, [c], -1, (255,255,255), -1)\n        result = cv2.bitwise_and(image, mask)\n        result[mask==0] = 255\n        cv2.imshow('result', result)\n        cv2.waitKey(175)\ncv2.imshow('thresh', thresh)\ncv2.imshow('invert', invert)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/27386738/per-transaction-isolation-level-in-django-orm",
    "body": "    DATABASES = {\n        'default': {\n            'NAME': 'app_data',\n            'ENGINE': 'django.db.backends.postgresql',\n            'USER': 'postgres_user',\n            'PASSWORD': 's3krit',\n        },\n        'serializable': {\n            'NAME': 'app_data',\n            'ENGINE': 'django.db.backends.postgresql',\n            'USER': 'postgres_user',\n            'PASSWORD': 's3krit',\n            'OPTIONS': {\n                'isolation_level': psycopg2.extensions.ISOLATION_LEVEL_SERIALIZABLE,\n            },\n        },\n    }"
  },
  {
    "url": "https://stackoverflow.com/questions/42844636/what-is-the-difference-between-partial-and-partialmethod",
    "body": "from functools import partialmethod\nclass Live:\n    def __init__(self):\n        self._live = False\n    def set_live(self,state:'bool'):\n        self._live = state\n    def __get_live(self):\n        return self._live\n    def __call__(self):\n        # enable this to be called when the object is made callable.\n        return self.__get_live()\n\n    # partial methods. Freezes the method `set_live` and `set_dead`\n    # with the specific arguments\n    set_alive = partialmethod(set_live, True)\n    set_dead = partialmethod(set_live, False)\nlive = Live() # create object\nprint(live()) # make the object callable. It calls `__call__` under the hood\nlive.set_alive() # Call the partial method\nprint(live())"
  },
  {
    "url": "https://stackoverflow.com/questions/57737610/puppeteer-fingerprint-simulation",
    "body": "const puppeteer = require('puppeteer');\n(async () => {\n    const browser = await puppeteer.launch({ headless: false });\n    const page = await browser.newPage();\n    await page.evaluateOnNewDocument(() => {\n        const originalFunction = HTMLCanvasElement.prototype.toDataURL;\n        HTMLCanvasElement.prototype.toDataURL = function (type) {\n            if (type === 'image/png' && this.width === 220 && this.height === 30) {\n                // this is likely a fingerprint attempt, return fake fingerprint\n                return 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANwAAAAeCAAAAABiES/iAAACeElEQVRYw+2YzUtUURjGf47OmDPh5AyFomUiEeEmyghXtWsh4dcswlYV2KYWfZh/QRBUVLhTCCJXEgmKUCIkFhJREARBkbkyKBlTRmUC82lxZ7z3TjM4whwXwz2ry3vO87znx33Pey4XFfHAg/PgPDgPzoPz4Dy4rFIKscSkAfmnsUY+iTfXFhxue4Zm4QpfaKbg8k+EsZNsGG6iNVzRMrkZeRPmjp6eCgcae5f+3wJIgtWLldG+DUnfzoail1etaVsEa1f2lUqw2hPd3T7nCrkMtlkQ24YDwP8+FZkI+gY3uq2cTcu54GIA/dJCDUAnSE4RdAESdALUxZ0hl4E5OMs49iE528E5a+cj5YFhDVI3vLA2c4K+zLXpvR37tNRDs3STg1OJqXqQSwS14wlJUD+VeHWAW86Qy8BwQ5Ek/WK/JBgqC72UTvJakmY5lAvurTRPSDrMmKRRcIvgeUo2KmmEI86Qy8DwmVu/ezQIBCSBLzwjKZhujv5cZZmUNkAq57ekRXCLYDG12pre5Qy5DAzDXbPfIOB/JqmCzNafCZd+dMA5RfZxdsBlNTAMF+FJfD2eSvSI0iGpmXe5GnbG3qyyHAO3yCZxlGV2uBLWDcJVMZKc7UrnfIBvQI+pHpxbS34ZaNkK7gYN0yvTDSCXyCZxNJTscFFe/DUH1w3QvpnzPiUPdTXfsvxZDdBGmeQU2SQd9lWQHS5m9J6Ln4/suZCwc96D25qM1formq5/3ApOX1uDkZ7P7JXkENkkK5eqQm3flRtuvitSYgCucKOf0zv01bazcG3Tyz8GKukvSjjrlB3/U5Rw42dqAo29yypKOO8figeX1/gH+zX9JqfOeUwAAAAASUVORK5CYII=';\n            }\n            // otherwise, just use the original function\n            return originalFunction.apply(this, arguments);\n        };\n    });\n    await page.goto('https://browserleaks.com/canvas');\n})();"
  },
  {
    "url": "https://stackoverflow.com/questions/31768031/plotting-points-on-the-surface-of-a-sphere",
    "body": "   0.000000000000000\t90.000000000000000    -0.055226399197273\n 180.000000000000000\t90.000000000000000    -0.055226399197273\n  90.000000000000000\t90.000000000000000    -0.055226399197273\n -90.000000000000000\t90.000000000000000    -0.055226399197273\n  90.000000000000000\t 0.000000000000000    -0.055226399197273\n  90.000000000000000   180.000000000000000    -0.055226399197273\n  45.000000000000000\t54.735610317245346     0.004450274607445\n  45.000000000000000   125.264389682754654     0.004450274607445\n -45.000000000000000\t54.735610317245346     0.004450274607445\n -45.000000000000000   125.264389682754654     0.004450274607445\n 135.000000000000000\t54.735610317245346     0.004450274607445\n 135.000000000000000   125.264389682754654     0.004450274607445\n-135.000000000000000\t54.735610317245346     0.004450274607445\n-135.000000000000000   125.264389682754654     0.004450274607445\n  45.000000000000000\t39.440090784780402     0.004496841067921\n  45.000000000000000   140.559909215219591     0.004496841067921\n -45.000000000000000\t39.440090784780402     0.004496841067921\n -45.000000000000000   140.559909215219591     0.004496841067921\n 135.000000000000000\t39.440090784780402     0.004496841067921\n 135.000000000000000   140.559909215219591     0.004496841067921\n-135.000000000000000\t39.440090784780402     0.004496841067921\n-135.000000000000000   140.559909215219591     0.004496841067921\n  59.815442273124063\t63.307345060625650     0.004496841067921\n -59.815442273124063\t63.307345060625650     0.004496841067921\n  59.815442273124063   116.692654939374364     0.004496841067921\n -59.815442273124063   116.692654939374364     0.004496841067921\n 120.184557726875937\t63.307345060625650     0.004496841067921\n-120.184557726875937\t63.307345060625650     0.004496841067921\n 120.184557726875937   116.692654939374364     0.004496841067921\n-120.184557726875937   116.692654939374364     0.004496841067921\n  30.184557726875941\t63.307345060625650     0.004496841067921\n 149.815442273124063\t63.307345060625650     0.004496841067921\n  30.184557726875941   116.692654939374364     0.004496841067921\n 149.815442273124063   116.692654939374364     0.004496841067921\n -30.184557726875941\t63.307345060625650     0.004496841067921\n-149.815442273124063\t63.307345060625650     0.004496841067921\n -30.184557726875941   116.692654939374364     0.004496841067921\n-149.815442273124063   116.692654939374364     0.004496841067921\n  45.000000000000000\t20.881794557261646     0.005049153450479\n  45.000000000000000   159.118205442738343     0.005049153450479\n -45.000000000000000\t20.881794557261646     0.005049153450479\n -45.000000000000000   159.118205442738343     0.005049153450479\n 135.000000000000000\t20.881794557261646     0.005049153450479\n 135.000000000000000   159.118205442738343     0.005049153450479\n-135.000000000000000\t20.881794557261646     0.005049153450479\n-135.000000000000000   159.118205442738343     0.005049153450479\n  74.903220296612005\t75.401622829462283     0.005049153450479\n -74.903220296612005\t75.401622829462283     0.005049153450479\n  74.903220296612005   104.598377170537717     0.005049153450479\n -74.903220296612005   104.598377170537717     0.005049153450479\n 105.096779703387995\t75.401622829462283     0.005049153450479\n-105.096779703387995\t75.401622829462283     0.005049153450479\n 105.096779703387995   104.598377170537717     0.005049153450479\n-105.096779703387995   104.598377170537717     0.005049153450479\n  15.096779703387996\t75.401622829462283     0.005049153450479\n 164.903220296612034\t75.401622829462283     0.005049153450479\n  15.096779703387996   104.598377170537717     0.005049153450479\n 164.903220296612034   104.598377170537717     0.005049153450479\n -15.096779703387996\t75.401622829462283     0.005049153450479\n-164.903220296612034\t75.401622829462283     0.005049153450479\n -15.096779703387996   104.598377170537717     0.005049153450479\n-164.903220296612034   104.598377170537717     0.005049153450479\n  45.000000000000000\t80.891636123006165     0.003976408018052\n  45.000000000000000\t99.108363876993835     0.003976408018052\n -45.000000000000000\t80.891636123006165     0.003976408018052\n -45.000000000000000\t99.108363876993835     0.003976408018052\n 135.000000000000000\t80.891636123006165     0.003976408018052\n 135.000000000000000\t99.108363876993835     0.003976408018052\n-135.000000000000000\t80.891636123006165     0.003976408018052\n-135.000000000000000\t99.108363876993835     0.003976408018052\n  12.774805990014807\t45.717979481517574     0.003976408018052\n -12.774805990014807\t45.717979481517574     0.003976408018052\n  12.774805990014807   134.282020518482426     0.003976408018052\n -12.774805990014807   134.282020518482426     0.003976408018052\n 167.225194009985188\t45.717979481517574     0.003976408018052\n-167.225194009985188\t45.717979481517574     0.003976408018052\n 167.225194009985188   134.282020518482426     0.003976408018052\n-167.225194009985188   134.282020518482426     0.003976408018052\n  77.225194009985188\t45.717979481517574     0.003976408018052\n 102.774805990014812\t45.717979481517574     0.003976408018052\n  77.225194009985188   134.282020518482426     0.003976408018052\n 102.774805990014812   134.282020518482426     0.003976408018052\n -77.225194009985188\t45.717979481517574     0.003976408018052\n-102.774805990014812\t45.717979481517574     0.003976408018052\n -77.225194009985188   134.282020518482426     0.003976408018052\n-102.774805990014812   134.282020518482426     0.003976408018052\n  45.000000000000000\t68.685581154790029     0.004401400650381\n  45.000000000000000   111.314418845209985     0.004401400650381\n -45.000000000000000\t68.685581154790029     0.004401400650381\n -45.000000000000000   111.314418845209985     0.004401400650381\n 135.000000000000000\t68.685581154790029     0.004401400650381\n 135.000000000000000   111.314418845209985     0.004401400650381\n-135.000000000000000\t68.685581154790029     0.004401400650381\n-135.000000000000000   111.314418845209985     0.004401400650381\n  28.889424740291254\t48.796111385350962     0.004401400650381\n -28.889424740291254\t48.796111385350962     0.004401400650381\n  28.889424740291254   131.203888614649060     0.004401400650381\n -28.889424740291254   131.203888614649060     0.004401400650381\n 151.110575259708753\t48.796111385350962     0.004401400650381\n-151.110575259708753\t48.796111385350962     0.004401400650381\n 151.110575259708753   131.203888614649060     0.004401400650381\n-151.110575259708753   131.203888614649060     0.004401400650381\n  61.110575259708753\t48.796111385350962     0.004401400650381\n 118.889424740291247\t48.796111385350962     0.004401400650381\n  61.110575259708753   131.203888614649060     0.004401400650381\n 118.889424740291247   131.203888614649060     0.004401400650381\n -61.110575259708753\t48.796111385350962     0.004401400650381\n-118.889424740291247\t48.796111385350962     0.004401400650381\n -61.110575259708753   131.203888614649060     0.004401400650381\n-118.889424740291247   131.203888614649060     0.004401400650381\n  45.000000000000000\t 3.274152069216487     0.017245443505444\n  45.000000000000000   176.725847930783516     0.017245443505444\n -45.000000000000000\t 3.274152069216487     0.017245443505444\n -45.000000000000000   176.725847930783516     0.017245443505444\n 135.000000000000000\t 3.274152069216487     0.017245443505444\n 135.000000000000000   176.725847930783516     0.017245443505444\n-135.000000000000000\t 3.274152069216487     0.017245443505444\n-135.000000000000000   176.725847930783516     0.017245443505444\n  87.683564415961172\t87.685455250362111     0.017245443505444\n -87.683564415961172\t87.685455250362111     0.017245443505444\n  87.683564415961172\t92.314544749637903     0.017245443505444\n -87.683564415961172\t92.314544749637903     0.017245443505444\n  92.316435584038842\t87.685455250362111     0.017245443505444\n -92.316435584038842\t87.685455250362111     0.017245443505444\n  92.316435584038842\t92.314544749637903     0.017245443505444\n -92.316435584038842\t92.314544749637903     0.017245443505444\n   2.316435584038771\t87.685455250362111     0.017245443505444\n 177.683564415961257\t87.685455250362111     0.017245443505444\n   2.316435584038771\t92.314544749637903     0.017245443505444\n 177.683564415961257\t92.314544749637903     0.017245443505444\n  -2.316435584038771\t87.685455250362111     0.017245443505444\n-177.683564415961257\t87.685455250362111     0.017245443505444\n  -2.316435584038771\t92.314544749637903     0.017245443505444\n-177.683564415961257\t92.314544749637903     0.017245443505444\n  54.381587934584054\t90.000000000000000     0.004231083095357\n -54.381587934584054\t90.000000000000000     0.004231083095357\n 125.618412065415953\t90.000000000000000     0.004231083095357\n-125.618412065415953\t90.000000000000000     0.004231083095357\n  35.618412065415953\t90.000000000000000     0.004231083095357\n -35.618412065415953\t90.000000000000000     0.004231083095357\n 144.381587934584047\t90.000000000000000     0.004231083095357\n-144.381587934584047\t90.000000000000000     0.004231083095357\n   0.000000000000000\t35.618412065415953     0.004231083095357\n   0.000000000000000   144.381587934584047     0.004231083095357\n 180.000000000000000\t35.618412065415953     0.004231083095357\n 180.000000000000000   144.381587934584047     0.004231083095357\n   0.000000000000000\t54.381587934584054     0.004231083095357\n   0.000000000000000   125.618412065415953     0.004231083095357\n 180.000000000000000\t54.381587934584054     0.004231083095357\n 180.000000000000000   125.618412065415953     0.004231083095357\n  90.000000000000000\t35.618412065415953     0.004231083095357\n  90.000000000000000   144.381587934584047     0.004231083095357\n -90.000000000000000\t35.618412065415953     0.004231083095357\n -90.000000000000000   144.381587934584047     0.004231083095357\n  90.000000000000000\t54.381587934584054     0.004231083095357\n  90.000000000000000   125.618412065415953     0.004231083095357\n -90.000000000000000\t54.381587934584054     0.004231083095357\n -90.000000000000000   125.618412065415953     0.004231083095357\n  69.231820019013028\t90.000000000000000     0.005198069864064\n -69.231820019013028\t90.000000000000000     0.005198069864064\n 110.768179980986986\t90.000000000000000     0.005198069864064\n-110.768179980986986\t90.000000000000000     0.005198069864064\n  20.768179980986979\t90.000000000000000     0.005198069864064\n -20.768179980986979\t90.000000000000000     0.005198069864064\n 159.231820019013014\t90.000000000000000     0.005198069864064\n-159.231820019013014\t90.000000000000000     0.005198069864064\n   0.000000000000000\t20.768179980986979     0.005198069864064\n   0.000000000000000   159.231820019013014     0.005198069864064\n 180.000000000000000\t20.768179980986979     0.005198069864064\n 180.000000000000000   159.231820019013014     0.005198069864064\n   0.000000000000000\t69.231820019013028     0.005198069864064\n   0.000000000000000   110.768179980986986     0.005198069864064\n 180.000000000000000\t69.231820019013028     0.005198069864064\n 180.000000000000000   110.768179980986986     0.005198069864064\n  90.000000000000000\t20.768179980986979     0.005198069864064\n  90.000000000000000   159.231820019013014     0.005198069864064\n -90.000000000000000\t20.768179980986979     0.005198069864064\n -90.000000000000000   159.231820019013014     0.005198069864064\n  90.000000000000000\t69.231820019013028     0.005198069864064\n  90.000000000000000   110.768179980986986     0.005198069864064\n -90.000000000000000\t69.231820019013028     0.005198069864064\n -90.000000000000000   110.768179980986986     0.005198069864064\n  64.963704081332708\t32.473856655655446     0.004695720972569\n  64.963704081332708   147.526143344344547     0.004695720972569\n -64.963704081332708\t32.473856655655446     0.004695720972569\n -64.963704081332708   147.526143344344547     0.004695720972569\n 115.036295918667292\t32.473856655655446     0.004695720972569\n 115.036295918667292   147.526143344344547     0.004695720972569\n-115.036295918667292\t32.473856655655446     0.004695720972569\n-115.036295918667292   147.526143344344547     0.004695720972569\n  74.926112157973748\t60.891424466952714     0.004695720972569\n  74.926112157973748   119.108575533047286     0.004695720972569\n -74.926112157973748\t60.891424466952714     0.004695720972569\n -74.926112157973748   119.108575533047286     0.004695720972569\n 105.073887842026252\t60.891424466952714     0.004695720972569\n 105.073887842026252   119.108575533047286     0.004695720972569\n-105.073887842026252\t60.891424466952714     0.004695720972569\n-105.073887842026252   119.108575533047286     0.004695720972569\n  25.036295918667289\t32.473856655655446     0.004695720972569\n  25.036295918667289   147.526143344344547     0.004695720972569\n -25.036295918667289\t32.473856655655446     0.004695720972569\n -25.036295918667289   147.526143344344547     0.004695720972569\n 154.963704081332708\t32.473856655655446     0.004695720972569\n 154.963704081332708   147.526143344344547     0.004695720972569\n-154.963704081332708\t32.473856655655446     0.004695720972569\n-154.963704081332708   147.526143344344547     0.004695720972569\n  60.030959593932515\t76.866650451671518     0.004695720972569\n  60.030959593932515   103.133349548328482     0.004695720972569\n -60.030959593932515\t76.866650451671518     0.004695720972569\n -60.030959593932515   103.133349548328482     0.004695720972569\n 119.969040406067492\t76.866650451671518     0.004695720972569\n 119.969040406067492   103.133349548328482     0.004695720972569\n-119.969040406067492\t76.866650451671518     0.004695720972569\n-119.969040406067492   103.133349548328482     0.004695720972569\n  15.073887842026251\t60.891424466952714     0.004695720972569\n  15.073887842026251   119.108575533047286     0.004695720972569\n -15.073887842026251\t60.891424466952714     0.004695720972569\n -15.073887842026251   119.108575533047286     0.004695720972569\n 164.926112157973762\t60.891424466952714     0.004695720972569\n 164.926112157973762   119.108575533047286     0.004695720972569\n-164.926112157973762\t60.891424466952714     0.004695720972569\n-164.926112157973762   119.108575533047286     0.004695720972569\n  29.969040406067499\t76.866650451671518     0.004695720972569\n  29.969040406067499   103.133349548328482     0.004695720972569\n -29.969040406067499\t76.866650451671518     0.004695720972569\n -29.969040406067499   103.133349548328482     0.004695720972569\n 150.030959593932522\t76.866650451671518     0.004695720972569\n 150.030959593932522   103.133349548328482     0.004695720972569\n-150.030959593932522\t76.866650451671518     0.004695720972569\n-150.030959593932522   103.133349548328482     0.004695720972569"
  },
  {
    "url": "https://stackoverflow.com/questions/44404349/pyqt-showing-video-stream-from-opencv",
    "body": "import cv2\nimport sys\nfrom PyQt5.QtWidgets import  QWidget, QLabel, QApplication\nfrom PyQt5.QtCore import QThread, Qt, pyqtSignal, pyqtSlot\nfrom PyQt5.QtGui import QImage, QPixmap\n\nclass Thread(QThread):\n    changePixmap = pyqtSignal(QImage)\n    def run(self):\n        cap = cv2.VideoCapture(0)\n        while True:\n            ret, frame = cap.read()\n            if ret:\n                # https://stackoverflow.com/a/55468544/6622587\n                rgbImage = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                h, w, ch = rgbImage.shape\n                bytesPerLine = ch * w\n                convertToQtFormat = QImage(rgbImage.data, w, h, bytesPerLine, QImage.Format_RGB888)\n                p = convertToQtFormat.scaled(640, 480, Qt.KeepAspectRatio)\n                self.changePixmap.emit(p)\nclass App(QWidget):\n    def __init__(self):\n        super().__init__()\n        [...]\n        self.initUI()\n    @pyqtSlot(QImage)\n    def setImage(self, image):\n        self.label.setPixmap(QPixmap.fromImage(image))\n    def initUI(self):\n        self.setWindowTitle(self.title)\n        self.setGeometry(self.left, self.top, self.width, self.height)\n        self.resize(1800, 1200)\n        # create a label\n        self.label = QLabel(self)\n        self.label.move(280, 120)\n        self.label.resize(640, 480)\n        th = Thread(self)\n        th.changePixmap.connect(self.setImage)\n        th.start()\n        self.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/64116781/how-do-i-automerge-dependabot-updates-config-version-2",
    "body": "name: Dependabot auto-approve\non: pull_request_target\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  dependabot:\n    runs-on: ubuntu-latest\n    if: ${{ github.actor == 'dependabot[bot]' }}\n    steps:\n      - name: Dependabot metadata\n        id: metadata\n        uses: dependabot/fetch-metadata@v1.1.1\n        with:\n          github-token: \"${{ secrets.GITHUB_TOKEN }}\"\n      - name: Enable auto-merge for Dependabot PRs\n        if: ${{contains(steps.metadata.outputs.dependency-names, 'my-dependency') && steps.metadata.outputs.update-type == 'version-update:semver-patch'}}\n        run: gh pr merge --auto --merge \"$PR_URL\"\n        env:\n          PR_URL: ${{github.event.pull_request.html_url}}\n          GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}}"
  },
  {
    "url": "https://stackoverflow.com/questions/61706535/keras-validation-loss-and-accuracy-stuck-at-0",
    "body": "model.fit(X_train, y_train, validation_data=[X_train.to_numpy(), y_train.to_numpy()],\nepochs=10, batch_size=64)\nEpoch 1/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.7898 - accuracy: 0.6087 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 2/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6710 - accuracy: 0.6500 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 3/10\n8/8 [==============================] - 0s 5ms/step - loss: 0.6748 - accuracy: 0.6500 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 4/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6716 - accuracy: 0.6370 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 5/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6085 - accuracy: 0.6326 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 6/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6744 - accuracy: 0.6326 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 7/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6102 - accuracy: 0.6522 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 8/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.7032 - accuracy: 0.6109 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 9/10\n8/8 [==============================] - 0s 5ms/step - loss: 0.6283 - accuracy: 0.6717 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 10/10\n8/8 [==============================] - 0s 5ms/step - loss: 0.6120 - accuracy: 0.6652 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00"
  },
  {
    "url": "https://stackoverflow.com/questions/61706535/keras-validation-loss-and-accuracy-stuck-at-0",
    "body": "...\n...\n        # Run validation.\n        if validation_data and self._should_eval(epoch, validation_freq):\n          val_x, val_y, val_sample_weight = (\n              data_adapter.unpack_x_y_sample_weight(validation_data))\n          val_logs = self.evaluate(\n              x=val_x,\n              y=val_y,\n              sample_weight=val_sample_weight,\n              batch_size=validation_batch_size or batch_size,\n              steps=validation_steps,\n              callbacks=callbacks,\n              max_queue_size=max_queue_size,\n              workers=workers,\n              use_multiprocessing=use_multiprocessing,\n              return_dict=True)\n          val_logs = {'val_' + name: val for name, val in val_logs.items()}\n          epoch_logs.update(val_logs)"
  },
  {
    "url": "https://stackoverflow.com/questions/61706535/keras-validation-loss-and-accuracy-stuck-at-0",
    "body": "model.fit(X_train, y_train, validation_data=(X_train.to_numpy(), y_train.to_numpy()),\nepochs=10, batch_size=64)\nEpoch 1/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5832 - accuracy: 0.6696 - val_loss: 0.6892 - val_accuracy: 0.6674\nEpoch 2/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.6385 - accuracy: 0.6804 - val_loss: 0.8984 - val_accuracy: 0.5565\nEpoch 3/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.6822 - accuracy: 0.6391 - val_loss: 0.6556 - val_accuracy: 0.6739\nEpoch 4/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6276 - accuracy: 0.6609 - val_loss: 1.0691 - val_accuracy: 0.5630\nEpoch 5/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.7048 - accuracy: 0.6239 - val_loss: 0.6474 - val_accuracy: 0.6326\nEpoch 6/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.6500 - val_loss: 0.6659 - val_accuracy: 0.6043\nEpoch 7/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5796 - accuracy: 0.6913 - val_loss: 0.6891 - val_accuracy: 0.6435\nEpoch 8/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5915 - accuracy: 0.6891 - val_loss: 0.5307 - val_accuracy: 0.7152\nEpoch 9/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5571 - accuracy: 0.7000 - val_loss: 0.5465 - val_accuracy: 0.6957\nEpoch 10/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.7133 - accuracy: 0.6283 - val_loss: 0.7046 - val_accuracy: 0.6413"
  },
  {
    "url": "https://stackoverflow.com/questions/50308812/is-it-possible-to-limit-the-number-of-coroutines-running-corcurrently-in-asyncio",
    "body": "import asyncio\nasync def semaphore_gather(num, coros, return_exceptions=False):\n    semaphore = asyncio.Semaphore(num)\n    async def _wrap_coro(coro):\n        async with semaphore:\n            return await coro\n    return await asyncio.gather(\n        *(_wrap_coro(coro) for coro in coros), return_exceptions=return_exceptions\n    )\n# async def a():\n#     return 1\n# print(asyncio.run(semaphore_gather(10, [a() for _ in range(100)])))\n# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
  },
  {
    "url": "https://stackoverflow.com/questions/71938799/python-asyncio-create-task-really-need-to-keep-a-reference",
    "body": "import asyncio\nimport gc\nasync def coro1():\n    while True:\n        print(\"just printing...\")\n        await asyncio.sleep(1)\n        gc.collect()\nasync def coro2():\n    loop = asyncio.get_running_loop()\n    f = loop.create_future()\n    print(\"inside coro2 - going to wait for future\")\n    await f\n    print(\"inside coro2 - future resolved\")\nasync def main():\n    t1 = asyncio.create_task(coro1()) # This task has a reference.\n    asyncio.create_task(coro2())      # This task doesn't.\n    await asyncio.sleep(5)\nasyncio.run(main())"
  },
  {
    "url": "https://stackoverflow.com/questions/71938799/python-asyncio-create-task-really-need-to-keep-a-reference",
    "body": "import asyncio\nimport socket\nimport gc\nasync def echo(connection, loop):\n    while data := await loop.sock_recv(connection, 512):\n        gc.collect()\n        await loop.sock_sendall(connection, data)\nasync def listen_for_connections(server_socket, loop):\n    while True:\n        gc.collect()\n        client_socket, client_address = await loop.sock_accept(server_socket)\n        client_socket.setblocking(False)\n        print(f\"received a connection from {client_address}\")\n        asyncio.create_task(echo(client_socket, loop))  # no reference to this task\nasync def main():\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_address = (\"127.0.0.1\", 8000)\n    server_socket.setblocking(False)\n    server_socket.bind(server_address)\n    server_socket.listen()\n    await listen_for_connections(server_socket, asyncio.get_running_loop())\nasyncio.run(main())"
  },
  {
    "url": "https://stackoverflow.com/questions/71938799/python-asyncio-create-task-really-need-to-keep-a-reference",
    "body": "async def sleep(delay, result=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n    ### Reaching this line means the `delay` is a positive integer\n    loop = events.get_running_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,        # <------------\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()"
  },
  {
    "url": "https://stackoverflow.com/questions/64234214/how-to-generate-a-blob-signed-url-in-google-cloud-run",
    "body": "def sign_url():\n    from google.cloud import storage\n    from datetime import datetime, timedelta\n    import google.auth\n    credentials, project_id = google.auth.default()\n    # Perform a refresh request to get the access token of the current credentials (Else, it's None)\n    from google.auth.transport import requests\n    r = requests.Request()\n    credentials.refresh(r)\n    client = storage.Client()\n    bucket = client.get_bucket('EXAMPLE_BUCKET')\n    blob = bucket.get_blob('libraries/image_1.png')\n    expires = datetime.now() + timedelta(seconds=86400)\n    # In case of user credential use, define manually the service account to use (for development purpose only)\n    service_account_email = \"YOUR DEV SERVICE ACCOUNT\"\n    # If you use a service account credential, you can use the embedded email\n    if hasattr(credentials, \"service_account_email\"):\n        service_account_email = credentials.service_account_email\n    url = blob.generate_signed_url(expiration=expires,service_account_email=service_account_email, access_token=credentials.token)\n    return url, 200"
  },
  {
    "url": "https://stackoverflow.com/questions/62398231/building-docs-fails-due-to-missing-pandoc",
    "body": "from inspect import getsourcefile\n# Get path to directory containing this file, conf.py.\nDOCS_DIRECTORY = os.path.dirname(os.path.abspath(getsourcefile(lambda: 0)))\ndef ensure_pandoc_installed(_):\n    import pypandoc\n    # Download pandoc if necessary. If pandoc is already installed and on\n    # the PATH, the installed version will be used. Otherwise, we will\n    # download a copy of pandoc into docs/bin/ and add that to our PATH.\n    pandoc_dir = os.path.join(DOCS_DIRECTORY, \"bin\")\n    # Add dir containing pandoc binary to the PATH environment variable\n    if pandoc_dir not in os.environ[\"PATH\"].split(os.pathsep):\n        os.environ[\"PATH\"] += os.pathsep + pandoc_dir\n    pypandoc.ensure_pandoc_installed(\n        quiet=True,\n        targetfolder=pandoc_dir,\n        delete_installer=True,\n    )\ndef setup(app):\n    app.connect(\"builder-inited\", ensure_pandoc_installed)"
  },
  {
    "url": "https://stackoverflow.com/questions/58562928/how-do-i-update-a-python-virtual-environment-with-venv-in-python-3-3-to-use",
    "body": "python -m venv --help\nusage: venv [-h] [--system-site-packages] [--symlinks | --copies] [--clear]\n            [--upgrade] [--without-pip] [--prompt PROMPT]\n            ENV_DIR [ENV_DIR ...]\nCreates virtual Python environments in one or more target directories.\npositional arguments:\n  ENV_DIR               A directory to create the environment in.\noptional arguments:\n  -h, --help            show this help message and exit\n  --system-site-packages\n                        Give the virtual environment access to the system\n                        site-packages dir.\n  --symlinks            Try to use symlinks rather than copies, when symlinks\n                        are not the default for the platform.\n  --copies              Try to use copies rather than symlinks, even when\n                        symlinks are the default for the platform.\n  --clear               Delete the contents of the environment directory if it\n                        already exists, before environment creation.\n  --upgrade             Upgrade the environment directory to use this version\n                        of Python, assuming Python has been upgraded in-place.\n  --without-pip         Skips installing or upgrading pip in the virtual\n                        environment (pip is bootstrapped by default)\n  --prompt PROMPT       Provides an alternative prompt prefix for this\n                        environment."
  },
  {
    "url": "https://stackoverflow.com/questions/56656493/what-is-the-difference-between-anaconda-prompt-and-anaconda-powershell-prompt",
    "body": "> $PSVersionTable\nName                           Value\n----                           -----\nPSVersion                      5.1.18362.752\nPSEdition                      Desktop\nPSCompatibleVersions           {1.0, 2.0, 3.0, 4.0...}\nBuildVersion                   10.0.18362.752\nCLRVersion                     4.0.30319.42000\nWSManStackVersion              3.0\nPSRemotingProtocolVersion      2.3\nSerializationVersion           1.1.0.1\n> $env:PATH\nC:\\Users\\user-name\\anaconda3;C:\\Users\\user-name\\anaconda3\\Library\\mingw-w64\\bin;..."
  },
  {
    "url": "https://stackoverflow.com/questions/28680896/how-can-i-get-the-3rd-friday-of-a-month-in-python",
    "body": "import pandas as pd\npd.date_range('2017-12-02','2020-08-31',freq='WOM-3FRI')\nOutput:\nDatetimeIndex(['2017-12-15', '2018-01-19', '2018-02-16', '2018-03-16',\n               '2018-04-20', '2018-05-18', '2018-06-15', '2018-07-20',\n               '2018-08-17', '2018-09-21', '2018-10-19', '2018-11-16',\n               '2018-12-21', '2019-01-18', '2019-02-15', '2019-03-15',\n               '2019-04-19', '2019-05-17', '2019-06-21', '2019-07-19',\n               '2019-08-16', '2019-09-20', '2019-10-18', '2019-11-15',\n               '2019-12-20', '2020-01-17', '2020-02-21', '2020-03-20',\n               '2020-04-17', '2020-05-15', '2020-06-19', '2020-07-17',\n               '2020-08-21'],\n              dtype='datetime64[ns]', freq='WOM-3FRI')"
  },
  {
    "url": "https://stackoverflow.com/questions/69796358/repeating-triangle-pattern-in-python",
    "body": "                        *\n                       ***\n                      *****\n                     *******\n                 *      *      *\n                ***    ***    ***\n               *****  *****  *****\n              *********************\n          *      *      *      *      *\n         ***    ***    ***    ***    ***\n        *****  *****  *****  *****  *****\n       ***********************************\n   *      *      *      *      *      *      *\n  ***    ***    ***    ***    ***    ***    ***\n *****  *****  *****  *****  *****  *****  *****\n*************************************************"
  },
  {
    "url": "https://stackoverflow.com/questions/67887138/how-to-install-packages-in-airflow-docker-compose",
    "body": "version: '3'\nx-airflow-common:\n  &airflow-common\n  build: .\n  # REPLACED # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.0}\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'\n  volumes:\n    - ./dags:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}\"\n  depends_on:\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n# ..."
  },
  {
    "url": "https://stackoverflow.com/questions/55169952/delete-all-items-dynamodb-using-python",
    "body": "import boto3\ndynamo = boto3.resource('dynamodb')\ndef truncateTable(tableName):\n    table = dynamo.Table(tableName)\n\n    #get the table keys\n    tableKeyNames = [key.get(\"AttributeName\") for key in table.key_schema]\n    #Only retrieve the keys for each item in the table (minimize data transfer)\n    projectionExpression = \", \".join('#' + key for key in tableKeyNames)\n    expressionAttrNames = {'#'+key: key for key in tableKeyNames}\n\n    counter = 0\n    page = table.scan(ProjectionExpression=projectionExpression, ExpressionAttributeNames=expressionAttrNames)\n    with table.batch_writer() as batch:\n        while page[\"Count\"] > 0:\n            counter += page[\"Count\"]\n            # Delete items in batches\n            for itemKeys in page[\"Items\"]:\n                batch.delete_item(Key=itemKeys)\n            # Fetch the next page\n            if 'LastEvaluatedKey' in page:\n                page = table.scan(\n                    ProjectionExpression=projectionExpression, ExpressionAttributeNames=expressionAttrNames,\n                    ExclusiveStartKey=page['LastEvaluatedKey'])\n            else:\n                break\n    print(f\"Deleted {counter}\")\n\ntruncateTable(\"YOUR_TABLE_NAME\")"
  },
  {
    "url": "https://stackoverflow.com/questions/62994795/how-to-secure-fastapi-api-endpoint-with-jwt-token-based-authorization",
    "body": "# dependency.py script\nfrom jose import jwt\nfrom jose.exceptions import JOSEError\nfrom fastapi import HTTPException, Depends\nfrom fastapi.security import HTTPAuthorizationCredentials, HTTPBearer\nsecurity = HTTPBearer()\nasync def has_access(credentials: HTTPAuthorizationCredentials= Depends(security)):\n    \"\"\"\n        Function that is used to validate the token in the case that it requires it\n    \"\"\"\n    token = credentials.credentials\n    try:\n        payload = jwt.decode(token, key='secret', options={\"verify_signature\": False,\n                                                           \"verify_aud\": False,\n                                                           \"verify_iss\": False})\n        print(\"payload => \", payload)\n    except JOSEError as e:  # catches any exception\n        raise HTTPException(\n            status_code=401,\n            detail=str(e))"
  },
  {
    "url": "https://stackoverflow.com/questions/59330863/cant-import-dll-module-in-python",
    "body": "    #!/usr/bin/env python\n    import argparse\n    import ctypes as cts\n    import os\n    import sys\n    DLL_NAME = \"./dll00.{:s}\".format(\"dll\" if sys.platform[:3].lower() == \"win\" else \"so\")\n    METH_ADDLLDIR = \"a\"\n    METH_PATH = \"p\"\n    METHS = (\n        METH_ADDLLDIR,\n        METH_PATH,\n    )\n    def parse_args(*argv):\n        parser = argparse.ArgumentParser(description=\"Python .dll search path (Win) example\")\n        parser.add_argument(\"--path\", \"-p\", choices=METHS)\n        parser.add_argument(\"--winmode\", \"-w\", type=int)\n        args, unk = parser.parse_known_args()\n        if unk:\n            print(\"Warning: Ignoring unknown arguments: {:}\".format(unk))\n        return args.path, args.winmode\n    def main(*argv):\n        meth, wmod = parse_args()\n        print(\"PATH (original): {:}\\n\".format(os.environ.get(\"PATH\")))\n        print(\"Using winmode={:}\".format(wmod))\n        if meth is not None:\n            subdir = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"subdir00\")\n            if meth == METH_ADDLLDIR:\n                add_dll_directory = getattr(os, \"add_dll_directory\", None)\n                if add_dll_directory:\n                    os.add_dll_directory(subdir)\n                    print(\"Using AddDllDirectory()\\n\")\n            elif meth == METH_PATH:\n                os.environ[\"PATH\"] = os.pathsep.join((os.environ.get(\"PATH\", \"\"), subdir))\n                print(\"Using %PATH%\\n\")\n        dll00 = cts.CDLL(DLL_NAME, winmode=wmod)\n        print(\"Dll: {:}\".format(dll00))\n        if False:  # No need to actually call the function\n            dll00Func00 = dll00.dll00Func00\n            dll00Func00.argtypes = ()\n            dll00Func00.restype = cts.c_int\n            res = dll00Func00()\n            print(\"\\n{0:s} returned: {1:d}\".format(dll00Func00.__name__, res))\n    if __name__ == \"__main__\":\n        print(\"Python {:s} {:03d}bit on {:s}\\n\".format(\" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n                                                       64 if sys.maxsize > 0x100000000 else 32, sys.platform))\n        rc = main(*sys.argv[1:])\n        print(\"\\nDone.\\n\")\n        sys.exit(rc)"
  },
  {
    "url": "https://stackoverflow.com/questions/59330863/cant-import-dll-module-in-python",
    "body": ">    [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q059330863]> sopr.bat\n>    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n>\n>    [prompt]> \"c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 > nul\n>\n>    [prompt]> tree /a /f\n>    Folder PATH listing for volume SSD0-WORK\n>    Volume serial number is AE9E-72AC\n>    E:.\n>    |   code00.py\n>    |   dll00.c\n>    |   dll01.c\n>    |   test.bat\n>    |\n>    \\---subdir00\n>\n>    [prompt]> :: Build .dlls\n>    [prompt]> cl /nologo /DDLL /MD dll01.c  /link /NOLOGO /DLL /OUT:subdir00\\dll01.dll\n>    dll01.c\n>       Creating library subdir00\\dll01.lib and object subdir00\\dll01.exp\n>\n>    [prompt]>\n>    [prompt]> cl /nologo /DDLL /MD dll00.c  /link /NOLOGO /DLL /OUT:dll00.dll subdir00\\dll01.lib\n>    dll00.c\n>       Creating library dll00.lib and object dll00.exp\n>\n>    [prompt]>\n>    [prompt]> tree /a /f\n>    Folder PATH listing for volume SSD0-WORK\n>    Volume serial number is AE9E-72AC\n>    E:.\n>    |   code00.py\n>    |   dll00.c\n>    |   dll00.dll\n>    |   dll00.exp\n>    |   dll00.lib\n>    |   dll00.obj\n>    |   dll01.c\n>    |   dll01.obj\n>    |   test.bat\n>    |\n>    \\---subdir00\n>            dll01.dll\n>            dll01.exp\n>            dll01.lib\n>\n>\n>    [prompt]>\n>    [prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\\python.exe\" code00.py -h\n>    Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)] 064bit on win32\n>\n>    usage: code00.py [-h] [--path {a,p}] [--winmode WINMODE]\n>\n>    Python .dll search path (Win) example\n>\n>    options:\n>      -h, --help            show this help message and exit\n>      --path {a,p}, -p {a,p}\n>      --winmode WINMODE, -w WINMODE\n>\n>    [prompt]>\n>    [prompt]> :: Going through combinations. When an argument is not passed, its default value is None\n>    [prompt]>\n>    [prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\\python.exe\" code00.py\n>    Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)] 064bit on win32\n>\n>    PATH (original): c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\VCPackages;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\MSBuild\\Current\\bin\\Roslyn;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4\n>    .8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\dotnet\\;C:\\Users\\cfati\\AppData\\Local\\Programs\\Python\\Launcher\\;e:\\Work\\Dev\\Utils\\current\\Win;e:\\Work\\Dev\\VEnvs\\py_pc064_03.09_test0\\Scripts;C:\\Us\n>    ers\\cfati\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cfati\\.dotnet\\tools;;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\n>\n>    Using winmode=None\n>    Traceback (most recent call last):\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 57, in <module>\n>        rc = main(*sys.argv[1:])\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 44, in main\n>        dll00 = cts.CDLL(DLL_NAME, winmode=wmod)\n>      File \"c:\\Install\\pc064\\Python\\Python\\03.10\\lib\\ctypes\\__init__.py\", line 374, in __init__\n>        self._handle = _dlopen(self._name, mode)\n>    FileNotFoundError: Could not find module 'e:\\Work\\Dev\\StackOverflow\\q059330863\\dll00.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n>\n>    [prompt]>\n>    [prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\\python.exe\" code00.py -w 0\n>    Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)] 064bit on win32\n>\n>    PATH (original): c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\VCPackages;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\MSBuild\\Current\\bin\\Roslyn;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4\n>    .8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\dotnet\\;C:\\Users\\cfati\\AppData\\Local\\Programs\\Python\\Launcher\\;e:\\Work\\Dev\\Utils\\current\\Win;e:\\Work\\Dev\\VEnvs\\py_pc064_03.09_test0\\Scripts;C:\\Us\n>    ers\\cfati\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cfati\\.dotnet\\tools;;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\n>\n>    Using winmode=0\n>    Traceback (most recent call last):\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 57, in <module>\n>        rc = main(*sys.argv[1:])\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 44, in main\n>        dll00 = cts.CDLL(DLL_NAME, winmode=wmod)\n>      File \"c:\\Install\\pc064\\Python\\Python\\03.10\\lib\\ctypes\\__init__.py\", line 374, in __init__\n>        self._handle = _dlopen(self._name, mode)\n>    FileNotFoundError: Could not find module './dll00.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n>\n>    [prompt]>\n>    [prompt]>\n>    [prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\\python.exe\" code00.py -p a\n>    Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)] 064bit on win32\n>\n>    PATH (original): c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\VCPackages;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\MSBuild\\Current\\bin\\Roslyn;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4\n>    .8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\dotnet\\;C:\\Users\\cfati\\AppData\\Local\\Programs\\Python\\Launcher\\;e:\\Work\\Dev\\Utils\\current\\Win;e:\\Work\\Dev\\VEnvs\\py_pc064_03.09_test0\\Scripts;C:\\Us\n>    ers\\cfati\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cfati\\.dotnet\\tools;;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\n>\n>    Using winmode=None\n>    Using AddDllDirectory()\n>\n>    Dll: <CDLL 'e:\\Work\\Dev\\StackOverflow\\q059330863\\dll00.dll', handle 7ffe6aaf0000 at 0x1f896d9ffd0>\n>\n>    Done.\n>\n>\n>    [prompt]>\n>    [prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\\python.exe\" code00.py -p a -w 0\n>    Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)] 064bit on win32\n>\n>    PATH (original): c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\VCPackages;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\MSBuild\\Current\\bin\\Roslyn;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4\n>    .8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\dotnet\\;C:\\Users\\cfati\\AppData\\Local\\Programs\\Python\\Launcher\\;e:\\Work\\Dev\\Utils\\current\\Win;e:\\Work\\Dev\\VEnvs\\py_pc064_03.09_test0\\Scripts;C:\\Us\n>    ers\\cfati\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cfati\\.dotnet\\tools;;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\n>\n>    Using winmode=0\n>    Using AddDllDirectory()\n>\n>    Traceback (most recent call last):\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 57, in <module>\n>        rc = main(*sys.argv[1:])\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 44, in main\n>        dll00 = cts.CDLL(DLL_NAME, winmode=wmod)\n>      File \"c:\\Install\\pc064\\Python\\Python\\03.10\\lib\\ctypes\\__init__.py\", line 374, in __init__\n>        self._handle = _dlopen(self._name, mode)\n>    FileNotFoundError: Could not find module './dll00.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n>\n>    [prompt]>\n>    [prompt]>\n>    [prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\\python.exe\" code00.py -p p\n>    Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)] 064bit on win32\n>\n>    PATH (original): c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\VCPackages;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\MSBuild\\Current\\bin\\Roslyn;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4\n>    .8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\dotnet\\;C:\\Users\\cfati\\AppData\\Local\\Programs\\Python\\Launcher\\;e:\\Work\\Dev\\Utils\\current\\Win;e:\\Work\\Dev\\VEnvs\\py_pc064_03.09_test0\\Scripts;C:\\Us\n>    ers\\cfati\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cfati\\.dotnet\\tools;;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\n>\n>    Using winmode=None\n>    Using %PATH%\n>\n>    Traceback (most recent call last):\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 57, in <module>\n>        rc = main(*sys.argv[1:])\n>      File \"e:\\Work\\Dev\\StackOverflow\\q059330863\\code00.py\", line 44, in main\n>        dll00 = cts.CDLL(DLL_NAME, winmode=wmod)\n>      File \"c:\\Install\\pc064\\Python\\Python\\03.10\\lib\\ctypes\\__init__.py\", line 374, in __init__\n>        self._handle = _dlopen(self._name, mode)\n>    FileNotFoundError: Could not find module 'e:\\Work\\Dev\\StackOverflow\\q059330863\\dll00.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n>\n>    [prompt]>\n>    [prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\\python.exe\" code00.py -p p -w 0\n>    Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)] 064bit on win32\n>\n>    PATH (original): c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\VCPackages;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\MSBuild\\Current\\bin\\Roslyn;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4\n>    .8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\Tools;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.22000.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\Tools\\;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files\\dotnet\\;C:\\Users\\cfati\\AppData\\Local\\Programs\\Python\\Launcher\\;e:\\Work\\Dev\\Utils\\current\\Win;e:\\Work\\Dev\\VEnvs\\py_pc064_03.09_test0\\Scripts;C:\\Us\n>    ers\\cfati\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cfati\\.dotnet\\tools;;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;c:\\Install\\pc032\\Microsoft\\VisualStudioCommunity\\2019\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\n>\n>    Using winmode=0\n>    Using %PATH%\n>\n>    Dll: <CDLL './dll00.dll', handle 7ffe6aaf0000 at 0x142a7a73cd0>\n>\n>    Done.\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/29967487/get-progress-back-from-shutil-file-copy-thread",
    "body": "import os\nimport shutil\ndef copyfileobj(fsrc, fdst, callback, length=0):\n    try:\n        # check for optimisation opportunity\n        if \"b\" in fsrc.mode and \"b\" in fdst.mode and fsrc.readinto:\n            return _copyfileobj_readinto(fsrc, fdst, callback, length)\n    except AttributeError:\n        # one or both file objects do not support a .mode or .readinto attribute\n        pass\n    if not length:\n        length = shutil.COPY_BUFSIZE\n\n    fsrc_read = fsrc.read\n    fdst_write = fdst.write\n    copied = 0\n    while True:\n        buf = fsrc_read(length)\n        if not buf:\n            break\n        fdst_write(buf)\n        copied += len(buf)\n        callback(copied)\n# differs from shutil.COPY_BUFSIZE on platforms != Windows\nREADINTO_BUFSIZE = 1024 * 1024\ndef _copyfileobj_readinto(fsrc, fdst, callback, length=0):\n    \"\"\"readinto()/memoryview() based variant of copyfileobj().\n    *fsrc* must support readinto() method and both files must be\n    open in binary mode.\n    \"\"\"\n    fsrc_readinto = fsrc.readinto\n    fdst_write = fdst.write\n    if not length:\n        try:\n            file_size = os.stat(fsrc.fileno()).st_size\n        except OSError:\n            file_size = READINTO_BUFSIZE\n        length = min(file_size, READINTO_BUFSIZE)\n    copied = 0\n    with memoryview(bytearray(length)) as mv:\n        while True:\n            n = fsrc_readinto(mv)\n            if not n:\n                break\n            elif n < length:\n                with mv[:n] as smv:\n                    fdst.write(smv)\n            else:\n                fdst_write(mv)\n            copied += n\n            callback(copied)"
  },
  {
    "url": "https://stackoverflow.com/questions/4699605/why-doesn-t-sqlite-require-a-commit-call-to-save-data",
    "body": "import sqlite3\nconnection = sqlite3.connect(':memory:', isolation_level='DEFERRED')\n# No transaction is explicitly initiated here by a start transaction statement.\nassert connection.in_transaction is False\nstatements = []\nconnection.set_trace_callback(statements.append)\ncursor = connection.cursor()\n# Transaction 1 is implicitly initiated here.\ncursor.execute('CREATE TABLE t (i INT)')\n# Transaction 1 is implicitly committed here.\n# Transaction 2 is explicitly initiated here by a start transaction statement.\ncursor.execute('INSERT INTO t VALUES (?)', (1,))\ncursor.execute('CREATE TABLE u (j INT)')\ncursor.execute('INSERT INTO u VALUES (?)', (2,))\ncursor.close()\nconnection.close()\n# Transaction 2 is implicitly rolled back here.\nassert statements == [\n    'CREATE TABLE t (i INT)',\n    'BEGIN DEFERRED',\n    'INSERT INTO t VALUES (1)',\n    'CREATE TABLE u (j INT)',\n    'INSERT INTO u VALUES (2)',\n]"
  },
  {
    "url": "https://stackoverflow.com/questions/4699605/why-doesn-t-sqlite-require-a-commit-call-to-save-data",
    "body": "import sqlite3\nconnection = sqlite3.connect(':memory:', autocommit=False)\n# Transaction 1 is explicitly initiated here by a start transaction statement.\nassert connection.in_transaction is True\nstatements = []\nconnection.set_trace_callback(statements.append)\ncursor = connection.cursor()\ncursor.execute('CREATE TABLE t (i INT)')\ncursor.execute('INSERT INTO t VALUES (?)', (1,))\ncursor.execute('CREATE TABLE u (j INT)')\ncursor.execute('INSERT INTO u VALUES (?)', (2,))\ncursor.close()\nconnection.close()\n# Transaction 1 is explicitly rolled back here by a rollback statement.\nassert statements == [\n    'CREATE TABLE t (i INT)',\n    'INSERT INTO t VALUES (1)',\n    'CREATE TABLE u (j INT)',\n    'INSERT INTO u VALUES (2)',\n    'ROLLBACK',\n]"
  },
  {
    "url": "https://stackoverflow.com/questions/4699605/why-doesn-t-sqlite-require-a-commit-call-to-save-data",
    "body": "import sqlite3\nconnection = sqlite3.connect(':memory:', isolation_level=None)\n# No transaction is explicitly initiated here by a start transaction statement.\nassert connection.in_transaction is False\nstatements = []\nconnection.set_trace_callback(statements.append)\ncursor = connection.cursor()\n# Transaction 1 is implicitly initiated here.\ncursor.execute('CREATE TABLE t (i INT)')\n# Transaction 1 is implicitly committed here.\n# Transaction 2 is implicitly initiated here.\ncursor.execute('INSERT INTO t VALUES (?)', (1,))\n# Transaction 2 is implicitly committed here.\n# Transaction 3 is implicitly initiated here.\ncursor.execute('CREATE TABLE u (j INT)')\n# Transaction 3 is implicitly committed here.\n# Transaction 4 is implicitly initiated here.\ncursor.execute('INSERT INTO u VALUES (?)', (2,))\n# Transaction 4 is implicitly committed here.\ncursor.close()\nconnection.close()\nassert statements == [\n    'CREATE TABLE t (i INT)',\n    'INSERT INTO t VALUES (1)',\n    'CREATE TABLE u (j INT)',\n    'INSERT INTO u VALUES (2)',\n]"
  },
  {
    "url": "https://stackoverflow.com/questions/4699605/why-doesn-t-sqlite-require-a-commit-call-to-save-data",
    "body": "import sqlite3\nconnection = sqlite3.connect(':memory:', autocommit=True)\n# No transaction is explicitly initiated here by a start transaction statement.\nassert connection.in_transaction is False\nstatements = []\nconnection.set_trace_callback(statements.append)\ncursor = connection.cursor()\n# Transaction 1 is implicitly initiated here.\ncursor.execute('CREATE TABLE t (i INT)')\n# Transaction 1 is implicitly committed here.\n# Transaction 2 is implicitly initiated here.\ncursor.execute('INSERT INTO t VALUES (?)', (1,))\n# Transaction 2 is implicitly committed here.\n# Transaction 3 is implicitly initiated here.\ncursor.execute('CREATE TABLE u (j INT)')\n# Transaction 3 is implicitly committed here.\n# Transaction 4 is implicitly initiated here.\ncursor.execute('INSERT INTO u VALUES (?)', (2,))\n# Transaction 4 is implicitly committed here.\ncursor.close()\nconnection.close()\nassert statements == [\n    'CREATE TABLE t (i INT)',\n    'INSERT INTO t VALUES (1)',\n    'CREATE TABLE u (j INT)',\n    'INSERT INTO u VALUES (2)',\n]"
  },
  {
    "url": "https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model",
    "body": "from transformers import BertModel\nclass CustomBERTModel(nn.Module):\n    def __init__(self):\n          super(CustomBERTModel, self).__init__()\n          self.bert = BertModel.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n          ### New layers:\n          self.linear1 = nn.Linear(768, 256)\n          self.linear2 = nn.Linear(256, 3) ## 3 is the number of classes in this example\n    def forward(self, ids, mask):\n          sequence_output, pooled_output = self.bert(\n               ids,\n               attention_mask=mask)\n          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n          linear1_output = self.linear1(sequence_output[:,0,:].view(-1,768)) ## extract the 1st token's embeddings\n          linear2_output = self.linear2(linear1_output)\n          return linear2_output\ntokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\nmodel = CustomBERTModel() # You can pass the parameters if required to have more flexible model\nmodel.to(torch.device(\"cpu\")) ## can be gpu\ncriterion = nn.CrossEntropyLoss() ## If required define your own criterion\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\nfor epoch in epochs:\n    for batch in data_loader: ## If you have a DataLoader()  object to get the data.\n        data = batch[0]\n        targets = batch[1] ## assuming that data loader returns a tuple of data and its targets\n\n        optimizer.zero_grad()\n        encoding = tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True, truncation=True,max_length=50, add_special_tokens = True)\n        outputs = model(input_ids, attention_mask=attention_mask)\n        outputs = F.log_softmax(outputs, dim=1)\n        input_ids = encoding['input_ids']\n        attention_mask = encoding['attention_mask']\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()"
  },
  {
    "url": "https://stackoverflow.com/questions/44467828/what-techniques-can-be-used-to-measure-performance-of-pandas-numpy-solutions",
    "body": "           sum_pd    sum_fc    sum_nb\n16       0.000796  0.000515  0.000502\n32       0.000702  0.000453  0.000454\n64       0.000702  0.000454  0.000456\n128      0.000711  0.000456  0.000458\n256      0.000714  0.000461  0.000462\n512      0.000728  0.000471  0.000473\n1024     0.000746  0.000512  0.000513\n2048     0.000825  0.000515  0.000514\n4096     0.000902  0.000609  0.000640\n8192     0.001056  0.000731  0.000755\n16384    0.001381  0.001012  0.000936\n32768    0.001885  0.001465  0.001328\n65536    0.003404  0.002957  0.002585\n131072   0.008076  0.005668  0.005159\n262144   0.015532  0.011059  0.010988\n524288   0.032517  0.023336  0.018608\n1048576  0.055144  0.040367  0.035487\n2097152  0.112333  0.080407  0.072154"
  },
  {
    "url": "https://stackoverflow.com/questions/76322463/how-to-initialize-a-global-object-or-variable-and-reuse-it-in-every-fastapi-endp",
    "body": "from fastapi import FastAPI, Request\nfrom contextlib import asynccontextmanager\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    ''' Run at startup\n        Initialize the Client and add it to app.state\n    '''\n    app.state.n_client = NotificationClient()\n    yield\n    ''' Run on shutdown\n        Close the connection\n        Clear variables and release the resources\n    '''\n    app.state.n_client.close()\napp = FastAPI(lifespan=lifespan)\n@app.get('/')\nasync def main(request: Request):\n    n_client = request.app.state.n_client\n    # ..."
  },
  {
    "url": "https://stackoverflow.com/questions/76322463/how-to-initialize-a-global-object-or-variable-and-reuse-it-in-every-fastapi-endp",
    "body": "from fastapi import FastAPI, Request\nfrom contextlib import asynccontextmanager\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    ''' Run at startup\n        Initialize the Client and add it to request.state\n    '''\n    n_client = NotificationClient()\n    yield {'n_client': n_client}\n    ''' Run on shutdown\n        Close the connection\n        Clear variables and release the resources\n    '''\n    n_client.close()\napp = FastAPI(lifespan=lifespan)\n@app.get('/')\nasync def main(request: Request):\n    n_client = request.state.n_client\n    # ..."
  },
  {
    "url": "https://stackoverflow.com/questions/76322463/how-to-initialize-a-global-object-or-variable-and-reuse-it-in-every-fastapi-endp",
    "body": "from fastapi import FastAPI, Request\nfrom contextlib import asynccontextmanager\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    yield {\"data\": {\"val\": 1}}\n    #yield {\"val\": 1}  # changes to `val` would not take effect globally\napp = FastAPI(lifespan=lifespan)\n@app.get('/set')\nasync def set(request: Request):\n    #request.state.val = 2  # changes to `val` would not take effect globally\n    request.state.data[\"val\"] = 2\n    return request.state\n@app.get(\"/get\")\nasync def get(request: Request):\n    return request.state"
  },
  {
    "url": "https://stackoverflow.com/questions/63492123/how-do-add-an-assembled-field-to-a-pydantic-model",
    "body": "`python\nfrom typing import Optional\nfrom pydantic import BaseModel, validator\nclass UserDB(BaseModel):\n    first_name: Optional[str] = None\n    last_name: Optional[str] = None\nclass User_1(BaseModel):\n    location: str  # for a change\n    full_name: Optional[str] = None\n    def __init__(self, user_db: UserDB, **data):\n        super().__init__(full_name=f\"{user_db.first_name} {user_db.last_name}\", **data)\nuser_db = UserDB(first_name=\"John\", last_name=\"Stark\")\nuser = User_1(user_db, location=\"Mars\")\nprint(user)\nclass User_2(BaseModel):\n    first_name: Optional[str] = None\n    last_name: Optional[str] = None\n    full_name: Optional[str] = None\n    @validator('full_name', always=True)\n    def ab(cls, v, values) -> str:\n        return f\"{values['first_name']} {values['last_name']}\"\nuser = User_2(**user_db.dict())\nprint(user)"
  },
  {
    "url": "https://stackoverflow.com/questions/74605279/python-3-11-worse-optimized-than-3-10",
    "body": "CPython 3.10 loop:\n        >>   28 FOR_ITER                 6 (to 42)\n             30 STORE_NAME               4 (_)\n  6          32 LOAD_NAME                1 (a)\n             34 LOAD_CONST               2 ('a')\n             36 INPLACE_ADD                             <----------\n             38 STORE_NAME               1 (a)\n             40 JUMP_ABSOLUTE           14 (to 28)\nCPython 3.11 loop:\n        >>   66 FOR_ITER                 7 (to 82)\n             68 STORE_NAME               4 (_)\n  6          70 LOAD_NAME                1 (a)\n             72 LOAD_CONST               2 ('a')\n             74 BINARY_OP               13 (+=)         <----------\n             78 STORE_NAME               1 (a)\n             80 JUMP_BACKWARD            8 (to 66)"
  },
  {
    "url": "https://stackoverflow.com/questions/74605279/python-3-11-worse-optimized-than-3-10",
    "body": "        // In CPython 3.10.8\n        case TARGET(INPLACE_ADD): {\n            PyObject *right = POP();\n            PyObject *left = TOP();\n            PyObject *sum;\n            if (PyUnicode_CheckExact(left) && PyUnicode_CheckExact(right)) {\n                sum = unicode_concatenate(tstate, left, right, f, next_instr); // <-----\n                /* unicode_concatenate consumed the ref to left */\n            }\n            else {\n                sum = PyNumber_InPlaceAdd(left, right);\n                Py_DECREF(left);\n            }\n            Py_DECREF(right);\n            SET_TOP(sum);\n            if (sum == NULL)\n                goto error;\n            DISPATCH();\n        }\n//----------------------------------------------------------------------------\n        // In CPython 3.11.0\n        TARGET(BINARY_OP_ADD_UNICODE) {\n            assert(cframe.use_tracing == 0);\n            PyObject *left = SECOND();\n            PyObject *right = TOP();\n            DEOPT_IF(!PyUnicode_CheckExact(left), BINARY_OP);\n            DEOPT_IF(Py_TYPE(right) != Py_TYPE(left), BINARY_OP);\n            STAT_INC(BINARY_OP, hit);\n            PyObject *res = PyUnicode_Concat(left, right); // <-----\n            STACK_SHRINK(1);\n            SET_TOP(res);\n            _Py_DECREF_SPECIALIZED(left, _PyUnicode_ExactDealloc);\n            _Py_DECREF_SPECIALIZED(right, _PyUnicode_ExactDealloc);\n            if (TOP() == NULL) {\n                goto error;\n            }\n            JUMPBY(INLINE_CACHE_ENTRIES_BINARY_OP);\n            DISPATCH();\n        }"
  },
  {
    "url": "https://stackoverflow.com/questions/55481355/python-abstract-class-shall-force-derived-classes-to-initialize-variable-in-in",
    "body": "from abc import ABCMeta, abstractmethod\n# our version of ABCMeta with required attributes\nclass MyMeta(ABCMeta):\n    required_attributes = []\n    def __call__(self, *args, **kwargs):\n        obj = super(MyMeta, self).__call__(*args, **kwargs)\n        for attr_name in obj.required_attributes:\n            if not getattr(obj, attr_name):\n                raise ValueError('required attribute (%s) not set' % attr_name)\n        return obj\n# similar to the above example, but inheriting MyMeta now\nclass Quadrature(object, metaclass=MyMeta):\n    required_attributes = ['xyz', 'weights']\n    @abstractmethod\n    def __init__(self, order):\n        pass\nclass QuadratureWhichWorks(Quadrature):\n    # This shall work because we initialize xyz and weights in __init__\n    def __init__(self,order):\n        self.xyz = 123\n        self.weights = 456\nq = QuadratureWhichWorks('foo')\nclass QuadratureWhichShallNotWork(Quadrature):\n    def __init__(self, order):\n        self.xyz = 123\nq2 = QuadratureWhichShallNotWork('bar')"
  },
  {
    "url": "https://stackoverflow.com/questions/55481355/python-abstract-class-shall-force-derived-classes-to-initialize-variable-in-in",
    "body": ">>> class Joker(object):\n>>>     # a class attribute\n>>>     setup = 'Wenn ist das Nunstück git und Slotermeyer?'\n>>>\n>>>     # a read-only property\n>>>     @property\n>>>     def warning(self):\n>>>         return 'Joke Warfare is explicitly banned bythe Geneva Conventions'\n>>>\n>>>     def __init__(self):\n>>>         self.punchline = 'Ja! Beiherhund das Oder die Flipperwaldt gersput!'\n>>> j = Joker()\n>>> # we can access the class attribute via class or instance\n>>> Joker.setup == j.setup\n>>> # we can get the property but cannot set it\n>>> j.warning\n'Joke Warfare is explicitly banned bythe Geneva Conventions'\n>>> j.warning = 'Totally safe joke...'\nAttributeError: cant set attribute\n>>> # instance attribute set in __init__ is only accessible to that instance\n>>> j.punchline != Joker.punchline\nAttributeError: type object 'Joker' has no attribute 'punchline'"
  },
  {
    "url": "https://stackoverflow.com/questions/55481355/python-abstract-class-shall-force-derived-classes-to-initialize-variable-in-in",
    "body": ">>> from abc import ABCMeta, abstractmethod\n>>> class Quadrature(object, metaclass=ABCMeta):\n>>>\n>>>     @property\n>>>     @abstractmethod\n>>>     def xyz(self):\n>>>         pass\n>>>\n>>>     @property\n>>>     @abstractmethod\n>>>     def weights(self):\n>>>         pass\n>>>\n>>>     @abstractmethod\n>>>     def __init__(self, order):\n>>>         pass\n>>>\n>>>     def someStupidFunctionDefinedHere(self, n):\n>>>         return self.xyz+self.weights+n\n>>>\n>>>\n>>> class QuadratureWhichWorks(Quadrature):\n>>>     # This shall work because we initialize xyz and weights in __init__\n>>>     def __init__(self,order):\n>>>         self._xyz = 123\n>>>         self._weights = 456\n>>>\n>>>     @property\n>>>     def xyz(self):\n>>>         return self._xyz\n>>>\n>>>     @property\n>>>     def weights(self):\n>>>         return self._weights\n>>>\n>>> q = QuadratureWhichWorks('foo')\n>>> q.xyz\n123\n>>> q.weights\n456"
  },
  {
    "url": "https://stackoverflow.com/questions/42697933/colormap-with-maximum-distinguishable-colours",
    "body": "import math\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.cm import hsv\ndef generate_colormap(number_of_distinct_colors: int = 80):\n    if number_of_distinct_colors == 0:\n        number_of_distinct_colors = 80\n    number_of_shades = 7\n    number_of_distinct_colors_with_multiply_of_shades = int(math.ceil(number_of_distinct_colors / number_of_shades) * number_of_shades)\n    # Create an array with uniformly drawn floats taken from <0, 1) partition\n    linearly_distributed_nums = np.arange(number_of_distinct_colors_with_multiply_of_shades) / number_of_distinct_colors_with_multiply_of_shades\n    # We are going to reorganise monotonically growing numbers in such way that there will be single array with saw-like pattern\n    #     but each saw tooth is slightly higher than the one before\n    # First divide linearly_distributed_nums into number_of_shades sub-arrays containing linearly distributed numbers\n    arr_by_shade_rows = linearly_distributed_nums.reshape(number_of_shades, number_of_distinct_colors_with_multiply_of_shades // number_of_shades)\n    # Transpose the above matrix (columns become rows) - as a result each row contains saw tooth with values slightly higher than row above\n    arr_by_shade_columns = arr_by_shade_rows.T\n    # Keep number of saw teeth for later\n    number_of_partitions = arr_by_shade_columns.shape[0]\n    # Flatten the above matrix - join each row into single array\n    nums_distributed_like_rising_saw = arr_by_shade_columns.reshape(-1)\n    # HSV colour map is cyclic (https://matplotlib.org/tutorials/colors/colormaps.html#cyclic), we'll use this property\n    initial_cm = hsv(nums_distributed_like_rising_saw)\n    lower_partitions_half = number_of_partitions // 2\n    upper_partitions_half = number_of_partitions - lower_partitions_half\n    # Modify lower half in such way that colours towards beginning of partition are darker\n    # First colours are affected more, colours closer to the middle are affected less\n    lower_half = lower_partitions_half * number_of_shades\n    for i in range(3):\n        initial_cm[0:lower_half, i] *= np.arange(0.2, 1, 0.8/lower_half)\n    # Modify second half in such way that colours towards end of partition are less intense and brighter\n    # Colours closer to the middle are affected less, colours closer to the end are affected more\n    for i in range(3):\n        for j in range(upper_partitions_half):\n            modifier = np.ones(number_of_shades) - initial_cm[lower_half + j * number_of_shades: lower_half + (j + 1) * number_of_shades, i]\n            modifier = j * modifier / upper_partitions_half\n            initial_cm[lower_half + j * number_of_shades: lower_half + (j + 1) * number_of_shades, i] += modifier\n    return ListedColormap(initial_cm)"
  },
  {
    "url": "https://stackoverflow.com/questions/24872527/combine-word-document-using-python-docx",
    "body": "#Importing the required packages\nfrom docxcompose.composer import Composer\nfrom docx import Document as Document_compose\n#filename_master is name of the file you want to merge the docx file into\nmaster = Document_compose(filename_master)\ncomposer = Composer(master)\n#filename_second_docx is the name of the second docx file\ndoc2 = Document_compose(filename_second_docx)\n#append the doc2 into the master using composer.append function\ncomposer.append(doc2)\n#Save the combined docx with a name\ncomposer.save(\"combined.docx\")"
  },
  {
    "url": "https://stackoverflow.com/questions/24872527/combine-word-document-using-python-docx",
    "body": "#Filename_master is the name of the file you want to merge all the document into\n#files_list is a list containing all the filename of the docx file to be merged\ndef combine_all_docx(filename_master,files_list):\n    number_of_sections=len(files_list)\n    master = Document_compose(filename_master)\n    composer = Composer(master)\n    for i in range(0, number_of_sections):\n        doc_temp = Document_compose(files_list[i])\n        composer.append(doc_temp)\n    composer.save(\"combined_file.docx\")\n#For Example\n#filename_master=\"file1.docx\"\n#files_list=[\"file2.docx\",\"file3.docx\",\"file4.docx\",file5.docx\"]\n#Calling the function\n#combine_all_docx(filename_master,files_list)\n#This function will combine all the document in the array files_list into the file1.docx and save the merged document into combined_file.docx"
  },
  {
    "url": "https://stackoverflow.com/questions/54653356/case-when-function-from-r-to-python",
    "body": "conditions = [\n    (df[\"age\"].lt(10)),\n    (df[\"age\"].ge(10) & df[\"age\"].lt(20)),\n    (df[\"age\"].ge(20) & df[\"age\"].lt(30)),\n    (df[\"age\"].ge(30) & df[\"age\"].lt(50)),\n    (df[\"age\"].ge(50)),\n]\nchoices = [\"baby\", \"kid\", \"young\", \"mature\", \"grandpa\"]\ndf[\"elderly\"] = np.select(conditions, choices)\n# Results in:\n#      name  age  preTestScore  postTestScore  elderly\n#  0  Jason   42             4             25   mature\n#  1  Molly   52            24             94  grandpa\n#  2   Tina   36            31             57   mature\n#  3   Jake   24             2             62    young\n#  4    Amy   73             3             70  grandpa"
  },
  {
    "url": "https://stackoverflow.com/questions/71514124/find-near-duplicate-and-faked-images",
    "body": "from sentence_transformers import SentenceTransformer, util\nfrom PIL import Image\nimport glob\nimport os\n# Load the OpenAI CLIP Model\nprint('Loading CLIP Model...')\nmodel = SentenceTransformer('clip-ViT-B-32')\n# Next we compute the embeddings\n# To encode an image, you can use the following code:\n# from PIL import Image\n# encoded_image = model.encode(Image.open(filepath))\nimage_names = list(glob.glob('./*.jpg'))\nprint(\"Images:\", len(image_names))\nencoded_image = model.encode([Image.open(filepath) for filepath in image_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n# Now we run the clustering algorithm. This function compares images aganist\n# all other images and returns a list with the pairs that have the highest\n# cosine similarity score\nprocessed_images = util.paraphrase_mining_embeddings(encoded_image)\nNUM_SIMILAR_IMAGES = 10\n# =================\n# DUPLICATES\n# =================\nprint('Finding duplicate images...')\n# Filter list for duplicates. Results are triplets (score, image_id1, image_id2) and is scorted in decreasing order\n# A duplicate image will have a score of 1.00\nduplicates = [image for image in processed_images if image[0] >= 1]\n# Output the top X duplicate images\nfor score, image_id1, image_id2 in duplicates[0:NUM_SIMILAR_IMAGES]:\n    print(\"\\nScore: {:.3f}%\".format(score * 100))\n    print(image_names[image_id1])\n    print(image_names[image_id2])\n# =================\n# NEAR DUPLICATES\n# =================\nprint('Finding near duplicate images...')\n# Use a threshold parameter to identify two images as similar. By setting the threshold lower,\n# you will get larger clusters which have less similar images in it. Threshold 0 - 1.00\n# A threshold of 1.00 means the two images are exactly the same. Since we are finding near\n# duplicate images, we can set it at 0.99 or any number 0 < X < 1.00.\nthreshold = 0.99\nnear_duplicates = [image for image in processed_images if image[0] < threshold]\nfor score, image_id1, image_id2 in near_duplicates[0:NUM_SIMILAR_IMAGES]:\n    print(\"\\nScore: {:.3f}%\".format(score * 100))\n    print(image_names[image_id1])\n    print(image_names[image_id2])"
  },
  {
    "url": "https://stackoverflow.com/questions/65575796/why-does-the-flask-bool-query-parameter-always-evaluate-to-true",
    "body": "$ curl -XGET http://localhost:5000/test?fullInfo=false\n{\"full_info\":false}\n$ curl -XGET http://localhost:5000/test?fullInfo=adasdasd\n{\"full_info\":false}\n$ curl -XGET http://localhost:5000/test?fullInfo=11431423\n{\"full_info\":false}\n$ curl -XGET http://localhost:5000/test?fullInfo=\n{\"full_info\":false}\n$ curl -XGET http://localhost:5000/test?fullInfo=true\n{\"full_info\":true}\n$ curl -XGET http://localhost:5000/test?fullInfo=TRUE\n{\"full_info\":true}\n$ curl -XGET http://localhost:5000/test\n{\"full_info\":false}"
  },
  {
    "url": "https://stackoverflow.com/questions/65465555/how-to-use-values-from-list-as-pydantic-validator",
    "body": ">>> uf = UserForm(fruits=['apple','banana'],name='hello')\n>>> uf\nUserForm(fruits=[<Fruit.APPLE: 'apple'>, <Fruit.BANANA: 'banana'>], name='hello')\n>>> af = UserForm(fruits=['monkey','apple'],name='hello')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"pydantic/main.py\", line 400, in pydantic.main.BaseModel.__init__\npydantic.error_wrappers.ValidationError: 1 validation error for UserForm\nfruits -> 0\n  value is not a valid enumeration member; permitted: 'apple', 'banana', 'melon' (type=type_error.enum; enum_values=[<Fruit.APPLE: 'apple'>, <Fruit.BANANA: 'banana'>, <Fruit.MELON: 'melon'>])\n>>>"
  },
  {
    "url": "https://stackoverflow.com/questions/43434020/black-and-white-boxplots-in-seaborn",
    "body": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n_to_plot = pd.DataFrame(\n    {\n     0: np.random.normal(0, 1, 100),\n     1: np.random.normal(0, 2, 100),\n     2: np.random.normal(-1, 1, 100),\n     3: np.random.normal(-2, 2, 100)\n     }\n).melt()\nPROPS = {\n    'boxprops':{'facecolor':'none', 'edgecolor':'red'},\n    'medianprops':{'color':'green'},\n    'whiskerprops':{'color':'blue'},\n    'capprops':{'color':'magenta'}\n}\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.boxplot(x='variable',y='value',\n            data=_to_plot,\n            showfliers=False,\n            linewidth=1,\n            ax=ax,\n            **PROPS)"
  },
  {
    "url": "https://stackoverflow.com/questions/4066202/resizing-pictures-in-pil-in-tkinter",
    "body": "im_temp = Image.open(Image_Location)\nim_temp = im_temp.resize((250, 250), Image.ANTIALIAS)\nim_temp.save(\"ArtWrk.ppm\", \"ppm\") ## The only reason I included this was to convert\n## The image into a format that Tkinter woulden't complain about\nself.photo = PhotoImage(file=\"ArtWrk.ppm\") ## Open the image as a tkinter.PhotoImage class()\nself.Artwork.destroy() ## Erase the last drawn picture (in the program the picture I used was changing)\nself.Artwork = Label(self.frame, image=self.photo) ## Sets the image too the label\nself.Artwork.photo = self.photo ## Make the image actually display (If I don't include this it won't display an image)\nself.Artwork.pack() ## Repack the image"
  },
  {
    "url": "https://stackoverflow.com/questions/385572/typecasting-in-python",
    "body": "s8 = (i + 2**7) % 2**8 - 2**7      # convert to signed 8-bit\nu8 = i % 2**8                      # convert to unsigned 8-bit\ns16 = (i + 2**15) % 2**16 - 2**15  # convert to signed 16-bit\nu16 = i % 2**16                    # convert to unsigned 16-bit\ns32 = (i + 2**31) % 2**32 - 2**31  # convert to signed 32-bit\nu32 = i % 2**32                    # convert to unsigned 32-bit\ns64 = (i + 2**63) % 2**64 - 2**63  # convert to signed 64-bit\nu64 = i % 2**64                    # convert to unsigned 64-bit"
  },
  {
    "url": "https://stackoverflow.com/questions/56206038/how-to-loop-through-paginated-api-using-python",
    "body": "import requests #to make TMDB API calls\n#Discover API url filtered to movies >= 2004 and containing Drama genre_ID: 18\ndiscover_api_url = 'https://api.themoviedb.org/3/discover/movie?\napi_key=['my api key']&language=en-US&sort_by=popularity.desc&include_adult=false&include_video=false&primary_release_year=>%3D2004&with_genres=18'\nmost_popular_films = []\nnew_results = True\npage = 1\nwhile new_results:\n    discover_api = requests.get(discover_api_url + f\"&page={page}\").json()\n    new_results = discover_api.get(\"results\", [])\n    most_popular_films.extend(new_results)\n    page += 1\n#printing movie_id and movie_title by popularity desc\nfor i, film in enumerate(most_popular_films):\n    print(i, film['id'], film['title'])"
  },
  {
    "url": "https://stackoverflow.com/questions/56206038/how-to-loop-through-paginated-api-using-python",
    "body": "import requests #to make TMDB API calls\n#Discover API url filtered to movies >= 2004 and containing Drama genre_ID: 18\ndiscover_api_url = 'https://api.themoviedb.org/3/discover/movie?\napi_key=['my api key']&language=en-US&sort_by=popularity.desc&include_adult=false&include_video=false&primary_release_year=>%3D2004&with_genres=18'\ndiscover_api = requests.get(discover_api_url).json()\nmost_popular_films = discover_api[\"results\"]\nfor page in range(2, discover_api[\"total_pages\"]+1):\n    discover_api = requests.get(discover_api_url + f\"&page={page}\").json()\n    most_popular_films.extend(discover_api[\"results\"])\n#printing movie_id and movie_title by popularity desc\nfor i, film in enumerate(most_popular_films):\n    print(i, film['id'], film['title'])"
  },
  {
    "url": "https://stackoverflow.com/questions/56206038/how-to-loop-through-paginated-api-using-python",
    "body": "import requests #to make TMDB API calls\n#Discover API url filtered to movies >= 2004 and containing Drama genre_ID: 18\ndiscover_api = 'https://api.themoviedb.org/3/discover/movie?\napi_key=['my api key']&language=en-US&sort_by=popularity.desc&include_adult=false&include_video=false&primary_release_year=>%3D2004&with_genres=18'\ndiscover_api = requests.get(discover_api).json()\nmost_popular_films = discover_api[\"results\"]\nwhile discover_api[\"next_url\"]:\n    discover_api = requests.get(discover_api[\"next_url\"]).json()\n    most_popular_films.extend(discover_api[\"results\"])\n#printing movie_id and movie_title by popularity desc\nfor i, film in enumerate(most_popular_films):\n    print(i, film['id'], film['title'])"
  },
  {
    "url": "https://stackoverflow.com/questions/45425896/install-tensorflow-with-specific-version-on-anaconda",
    "body": "Loading channels: done\n# Name                       Version           Build  Channel\ntensorflow-gpu                 1.4.1               0  pkgs/main\ntensorflow-gpu                 1.5.0               0  pkgs/main\ntensorflow-gpu                 1.6.0               0  pkgs/main\ntensorflow-gpu                 1.7.0               0  pkgs/main\ntensorflow-gpu                 1.8.0      h7b35bdc_0  pkgs/main\ntensorflow-gpu                 1.9.0      hf154084_0  pkgs/main\ntensorflow-gpu                1.10.0      hf154084_0  pkgs/main\ntensorflow-gpu                1.11.0      h0d30ee6_0  pkgs/main\ntensorflow-gpu                1.12.0      h0d30ee6_0  pkgs/main\ntensorflow-gpu                1.13.1      h0d30ee6_0  pkgs/main\ntensorflow-gpu                1.14.0      h0d30ee6_0  pkgs/main\ntensorflow-gpu                1.15.0      h0d30ee6_0  pkgs/main\ntensorflow-gpu                 2.0.0      h0d30ee6_0  pkgs/main\ntensorflow-gpu                 2.1.0      h0d30ee6_0  pkgs/main\ntensorflow-gpu                 2.2.0      h0d30ee6_0  pkgs/main"
  },
  {
    "url": "https://stackoverflow.com/questions/63105799/understanding-python-contextvars",
    "body": "`\nimport asyncio\nimport contextvars\n# declare context var\ncurrent_request_id_ctx = contextvars.ContextVar('')\ncurrent_request_id_global = ''\nasync def some_inner_coroutine():\n    global current_request_id_global\n    # simulate some async work\n    await asyncio.sleep(0.1)\n    # get value\n    print('Processed inner coroutine of request: {}'.format(current_request_id_ctx.get()))\n    if current_request_id_global != current_request_id_ctx.get():\n        print(f\"ERROR! global var={current_request_id_global}\")\nasync def some_outer_coroutine(req_id):\n    global current_request_id_global\n    # set value\n    current_request_id_ctx.set(req_id)\n    current_request_id_global = req_id\n    await some_inner_coroutine()\n    # get value\n    print('Processed outer coroutine of request: {}\\n'.format(current_request_id_ctx.get()))\nasync def main():\n    tasks = []\n    for req_id in range(1, 10000):\n        tasks.append(asyncio.create_task(some_outer_coroutine(req_id)))\n    await asyncio.gather(*tasks)\nif __name__ == '__main__':\n    asyncio.run(main())"
  },
  {
    "url": "https://stackoverflow.com/questions/1408272/get-file-creation-time-with-python-on-linux",
    "body": "> 3.1)  How do I find the creation time of a file?\n>\n>       You can't - it isn't stored anywhere.  Files have a last-modified\n>       time (shown by \"ls -l\"), a last-accessed time (shown by \"ls -lu\")\n>       and an inode change time (shown by \"ls -lc\"). The latter is often\n>       referred to as the \"creation time\" - even in some man pages -\n>       but that's wrong; it's also set by such operations as mv, ln,\n>       chmod, chown and chgrp.\n>\n>       The man page for \"stat(2)\" discusses this.\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/67921192/5bad-argument-in-function-rectangle-cant-parse-pt1-sequence-item-wit",
    "body": "Traceback (most recent call last):\n  File \"C:/Users/User/Desktop/temp.py\", line 9, in <module>\n    cv2.rectangle(img, c1, c2, (255, 0, 0), -1)\ncv2.error: OpenCV(4.5.2) :-1: error: (-5:Bad argument) in function 'rectangle'\n> Overload resolution failed:\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'pt1'. Sequence item with index 0 has a wrong type\n>  - Can't parse 'rec'. Expected sequence length 4, got 2\n>  - Can't parse 'rec'. Expected sequence length 4, got 2"
  },
  {
    "url": "https://stackoverflow.com/questions/62280161/saving-keras-models-with-custom-layers",
    "body": "import tensorflow as tf\n@tf.keras.utils.register_keras_serializable()\nclass CustomLayer(tf.keras.layers.Layer):\n    def __init__(self, k, **kwargs):\n        self.k = k\n        super(CustomLayer, self).__init__(**kwargs)\n    def get_config(self):\n        config = super().get_config()\n        config[\"k\"] = self.k\n        return config\n    def call(self, input):\n        return tf.multiply(input, 2)\ndef main():\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.Input(name='input_layer', shape=(10,)),\n            CustomLayer(10, name='custom_layer'),\n            tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')\n        ]\n    )\n    print(\"SUMMARY OF THE MODEL CREATED\")\n    print(\"-\" * 60)\n    print(model.summary())\n    model.save('model.h5')\n    del model\n    print()\n    print()\n    model = tf.keras.models.load_model('model.h5')\n    print(\"SUMMARY OF THE MODEL LOADED\")\n    print(\"-\" * 60)\n    print(model.summary())\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/62280161/saving-keras-models-with-custom-layers",
    "body": "SUMMARY OF THE MODEL CREATED\n------------------------------------------------------------\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ncustom_layer (CustomLayer)   (None, 10)                0\n_________________________________________________________________\noutput_layer (Dense)         (None, 1)                 11\n=================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n_________________________________________________________________\nNone\nWARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\nSUMMARY OF THE MODEL LOADED\n------------------------------------------------------------\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ncustom_layer (CustomLayer)   (None, 10)                0\n_________________________________________________________________\noutput_layer (Dense)         (None, 1)                 11\n=================================================================\nTotal params: 11\nTrainable params: 11\nNon-trainable params: 0\n_________________________________________________________________\nNone"
  },
  {
    "url": "https://stackoverflow.com/questions/795190/how-to-perform-common-post-initialization-tasks-in-inherited-classes",
    "body": "class PostInitCaller(type):\n    def __call__(cls, *args, **kwargs):\n        obj = type.__call__(cls, *args, **kwargs)\n        obj.__post_init__()\n        return obj\nclass BaseClass(metaclass=PostInitCaller):\n    def __init__(self):\n        print('base __init__')\n        self.common1()\n    def common1(self):\n        print('common 1')\n    def finalizeInitialization(self):\n        print('finalizeInitialization [common2]')\n    def __post_init__(self): # this is called at the end of __init__\n        self.finalizeInitialization()\nclass Subclass1(BaseClass):\n    def __init__(self):\n        super().__init__()\n        self.specific()\n    def specific(self):\n        print('specific')\ns = Subclass1()"
  },
  {
    "url": "https://stackoverflow.com/questions/48996494/send-http-request-through-specific-network-interface",
    "body": "import requests\ndef session_for_src_addr(addr: str) -> requests.Session:\n    \"\"\"\n    Create `Session` which will bind to the specified local address\n    rather than auto-selecting it.\n    \"\"\"\n    session = requests.Session()\n    for prefix in ('http://', 'https://'):\n        session.get_adapter(prefix).init_poolmanager(\n            # those are default values from HTTPAdapter's constructor\n            connections=requests.adapters.DEFAULT_POOLSIZE,\n            maxsize=requests.adapters.DEFAULT_POOLSIZE,\n            # This should be a tuple of (address, port). Port 0 means auto-selection.\n            source_address=(addr, 0),\n        )\n    return session\n# usage example:\ns = session_for_src_addr('192.168.1.12')\ns.get('https://httpbin.org/ip')"
  },
  {
    "url": "https://stackoverflow.com/questions/42771110/fastest-way-to-left-cycle-a-numpy-array-like-pop-push-for-a-queue",
    "body": "# benchmark_circular_buffer.py\nimport numpy as np\n# all operations are O(1) and don't require copying the array\n# except to_array which has to copy the array and is O(n)\nclass RecordingQueue1D:\n    def __init__(self, object: object, maxlen: int):\n        #allocate the memory we need ahead of time\n        self.max_length: int = maxlen\n        self.queue_tail: int = maxlen - 1\n        o_len = len(object)\n        if (o_len == maxlen):\n            self.rec_queue = np.array(object, dtype=np.int64)\n        elif (o_len > maxlen):\n            self.rec_queue = np.array(object[o_len-maxlen:], dtype=np.int64)\n        else:\n            self.rec_queue = np.append(np.array(object, dtype=np.int64), np.zeros(maxlen-o_len, dtype=np.int64))\n            self.queue_tail = o_len - 1\n    def to_array(self) -> np.array:\n        head = (self.queue_tail + 1) % self.max_length\n        return np.roll(self.rec_queue, -head) # this will force a copy\n    def enqueue(self, new_data: np.array) -> None:\n        # move tail pointer forward then insert at the tail of the queue\n        # to enforce max length of recording\n        self.queue_tail = (self.queue_tail + 1) % self.max_length\n        self.rec_queue[self.queue_tail] = new_data\n    def peek(self) -> int:\n        queue_head = (self.queue_tail + 1) % self.max_length\n        return self.rec_queue[queue_head]\n    def replace_item_at(self, index: int, new_value: int):\n        loc = (self.queue_tail + 1 + index) % self.max_length\n        self.rec_queue[loc] = new_val\n    def item_at(self, index: int) -> int:\n        # the item we want will be at head + index\n        loc = (self.queue_tail + 1 + index) % self.max_length\n        return self.rec_queue[loc]\n    def __repr__(self):\n        return \"tail: \" + str(self.queue_tail) + \"\\narray: \" + str(self.rec_queue)\n    def __str__(self):\n        return \"tail: \" + str(self.queue_tail) + \"\\narray: \" + str(self.rec_queue)\n        # return str(self.to_array())\nrnd_arr = np.random.randint(0, 1e6, 10**8)\nnew_val = -100\nslice_arr = rnd_arr.copy()\nc_buf_arr = RecordingQueue1D(rnd_arr.copy(), len(rnd_arr))\n# Test speed for queuing new a new item\n# swapping items 100 and 1000\n# swapping items 10000 and 100000\ndef slice_and_copy():\n    slice_arr[:-1] = slice_arr[1:]\n    slice_arr[-1] = new_val\n    old = slice_arr[100]\n    slice_arr[100] = slice_arr[1000]\n    old = slice_arr[10000]\n    slice_arr[10000] = slice_arr[100000]\ndef circular_buffer():\n    c_buf_arr.enqueue(new_val)\n    old = c_buf_arr.item_at(100)\n    slice_arr[100] = slice_arr[1000]\n    old = slice_arr[10000]\n    slice_arr[10000] = slice_arr[100000]\n# lets add copying the array to a new numpy.array\n# this will take O(N) time for the circular buffer because we use numpy.roll()\n# which copies the array.\ndef slice_and_copy_assignemnt():\n    slice_and_copy()\n    my_throwaway_arr = slice_arr.copy()\n    return my_throwaway_arr\ndef circular_buffer_assignment():\n    circular_buffer()\n    my_throwaway_arr = c_buf_arr.to_array().copy()\n    return my_throwaway_arr\n# test using\n# python -m timeit -s \"import benchmark_circular_buffer as bcb\" \"bcb.slice_and_copy()\"\n# python -m timeit -s \"import benchmark_circular_buffer as bcb\" \"bcb.circular_buffer()\"\n# python -m timeit -r 5 -n 4 -s \"import benchmark_circular_buffer as bcb\" \"bcb.slice_and_copy_assignemnt()\"\n# python -m timeit -r 5 -n 4 -s \"import benchmark_circular_buffer as bcb\" \"bcb.circular_buffer_assignment()\""
  },
  {
    "url": "https://stackoverflow.com/questions/42771110/fastest-way-to-left-cycle-a-numpy-array-like-pop-push-for-a-queue",
    "body": "(thermal_venv) PS X:\\win10\\repos\\thermal> python -m timeit -s \"import benchmark_circular_buffer as bcb\" \"bcb.slice_and_copy()\"\n10 loops, best of 5: 36.7 msec per loop\n(thermal_venv) PS X:\\win10\\repos\\thermal> python -m timeit -s \"import benchmark_circular_buffer as bcb\" \"bcb.circular_buffer()\"\n200000 loops, best of 5: 1.04 usec per loop\n(thermal_venv) PS X:\\win10\\repos\\thermal> python -m timeit -s \"import benchmark_circular_buffer as bcb\" \"bcb.slice_and_copy_assignemnt()\"\n2 loops, best of 5: 166 msec per loop\n(thermal_venv) PS X:\\win10\\repos\\thermal> python -m timeit -r 5 -n 4 -s \"import benchmark_circular_buffer as bcb\" \"bcb.slice_and_copy_assignemnt()\"\n4 loops, best of 5: 159 msec per loop\n(thermal_venv) PS X:\\win10\\repos\\thermal> python -m timeit -r 5 -n 4 -s \"import benchmark_circular_buffer as bcb\" \"bcb.circular_buffer_assignment()\"\n4 loops, best of 5: 511 msec per loop"
  },
  {
    "url": "https://stackoverflow.com/questions/49581104/sklearn-gridsearchcv-not-using-sample-weight-in-score-function",
    "body": "import numpy as np\nfrom sklearn import set_config\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import GridSearchCV, RepeatedKFold\nfrom sklearn.metrics import make_scorer\nset_config(enable_metadata_routing=True)\nif __name__ == \"__main__\":\n    RANDOM_STATE = 1337\n    X, y = load_iris(return_X_y=True)\n    sample_weight = np.array([1 + 100 * (i % 25) for i in range(len(X))])\n    search_params = {\"max_features\": [1, 2, 3, 4]}\n    cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=RANDOM_STATE)\n    rfc_weighted = RandomForestClassifier(\n        n_estimators=256,\n        criterion=\"entropy\",\n        warm_start=False,\n        n_jobs=1,\n        random_state=RANDOM_STATE,\n    ).set_fit_request(sample_weight=True)\n    rfc_unweighted = RandomForestClassifier(\n        n_estimators=256,\n        criterion=\"entropy\",\n        warm_start=False,\n        n_jobs=1,\n        random_state=RANDOM_STATE,\n    ).set_fit_request(sample_weight=False)\n    scorer_weighted = make_scorer(\n        log_loss, greater_is_better=False, needs_proba=True, needs_threshold=False\n    ).set_score_request(sample_weight=True)\n    scorer_unweighted = make_scorer(\n        log_loss, greater_is_better=False, needs_proba=True, needs_threshold=False\n    ).set_score_request(sample_weight=False)\n    for rfc, rfc_is_weighted in zip([rfc_weighted, rfc_unweighted], [True, False]):\n        for scorer, scorer_is_weighted in zip(\n            [scorer_weighted, scorer_unweighted], [True, False]\n        ):\n            grid_clf = GridSearchCV(\n                estimator=rfc,\n                scoring=scorer,\n                cv=cv,\n                param_grid=search_params,\n                refit=True,\n                return_train_score=False,\n            )\n            if rfc_is_weighted or scorer_is_weighted:\n                grid_clf.fit(X, y, sample_weight=sample_weight)\n            else:\n                grid_clf.fit(X, y)\n            print(\n                \"This is the best out-of-sample score using GridSearchCV with \"\n                f\"(is scorer weighted: {scorer_is_weighted}), (is rfc weighted: \"\n                f\"{rfc_is_weighted}): {-grid_clf.best_score_}\"\n            )"
  },
  {
    "url": "https://stackoverflow.com/questions/49581104/sklearn-gridsearchcv-not-using-sample-weight-in-score-function",
    "body": "This is the best out-of-sample score using GridSearchCV with (is scorer weighted: True), (is rfc weighted: True): 0.09180030568650309\nThis is the best out-of-sample score using GridSearchCV with (is scorer weighted: False), (is rfc weighted: True): 0.1225297422810374\nThis is the best out-of-sample score using GridSearchCV with (is scorer weighted: True), (is rfc weighted: False): 0.09064253271691491\nThis is the best out-of-sample score using GridSearchCV with (is scorer weighted: False), (is rfc weighted: False): 0.12187958644498716"
  },
  {
    "url": "https://stackoverflow.com/questions/15857838/modify-object-in-python-multiprocessing",
    "body": "import numpy as np\nimport multiprocessing as mp\nclass Tester:\n    num = 0.0\n    name = 'none'\n    def __init__(self,tnum=num, tname=name):\n        self.num  = tnum\n        self.name = tname\n    def __str__(self):\n        return '%f %s' % (self.num, self.name)\ndef mod(test, nn, out_queue):\n    print test.num\n    test.num = np.random.randn()\n    print test.num\n    test.name = nn\n    out_queue.put(test)\nif __name__ == '__main__':\n    num = 10\n    out_queue = mp.Queue()\n    tests = np.empty(num, dtype=object)\n    for it in range(num):\n        tests[it] = Tester(tnum=it*1.0)\n\n\n    print '\\n'\n    workers = [ mp.Process(target=mod, args=(test, 'some', out_queue) ) for test in tests ]\n\n    for work in workers: work.start()\n\n    for work in workers: work.join()\n\n    res_lst = []\n    for j in range(len(workers)):\n        res_lst.append(out_queue.get())\n\n    for test in res_lst: print test"
  },
  {
    "url": "https://stackoverflow.com/questions/57193597/mocking-a-sqlalchemy-session-for-pytest",
    "body": "import typing\nfrom flask import jsonify\nclass LegendsPostService:\n    def __init__(self, json_args, _session=None) -> None:\n        self.json_args = json_args\n        self.session = _session or db.session\n    def _get_legends(self) -> Legend:\n        return schemas.Legends(many=True).load(self.json_args)\n    def post(self) -> typing.List[typing.Dict[str, typing.Any]]:\n        legends = self._get_legends()\n        for legend in legends:\n            self.session.add(legend)\n        self.session.commit()\n        return schemas.Legends(many=True).dump(legends)\ndef post(cls):\n    service = LegendsPostService(json_args=request.get_json())\n    service.post()\n    return jsonify({'message': 'legends saved'})"
  },
  {
    "url": "https://stackoverflow.com/questions/57193597/mocking-a-sqlalchemy-session-for-pytest",
    "body": "from factory.alchemy import SQLAlchemyModelFactory\nclass ModelFactory(SQLAlchemyModelFactory):\n    class Meta:\n        abstract = True\n        sqlalchemy_session = db.session\n# setup your factory for Legends:\nclass LegendsFactory(ModelFactory):\n    logo_url = factory.Faker('image_url')\n    class Meta(ModelFactory.Meta):\n        model = Legends\nfrom unittest.mock import MagicMock, patch\n# neither of these tests even need a database connection!\n# so you should be able to write HUNDREDS of similar tests\n# and you should be able to run hundreds of them in seconds (not minutes)\ndef test_LegendsPostService_can_init():\n    session = MagicMock()\n    service = LegendsPostService(json_args={'foo': 'bar'}, _session=session)\n    assert service.session is session\n    assert service.json_args['foo'] == 'bar'\ndef test_LegendsPostService_can_post():\n    session = MagicMock()\n    service = LegendsPostService(json_args={'foo': 'bar'}, _session=session)\n    # let's make some fake Legends for our service!\n    legends = LegendsFactory.build_batch(2)\n    with patch.object(service, '_get_legends') as _get_legends:\n        _get_legends.return_value = legends\n        legends_post_json = service.post()\n    # look, Ma! No database connection!\n    assert legends_post_json[0]['image_url'] == legends[0].image_url"
  },
  {
    "url": "https://stackoverflow.com/questions/10625865/how-does-pyarg-parsetupleandkeywords-work",
    "body": "static PyObject *keywords(PyObject *self, PyObject *args, PyObject *kwargs)\n{\n    char *a;\n    char *b;\n    char *foo = NULL;\n    char *bar = NULL;\n    char *baz = NULL;\n    // Note how \"a\" and \"b\" are included in this\n    // even though they aren't supposed to be in kwargs like in python\n    static char *kwlist[] = {\"a\", \"b\", \"foo\", \"bar\", \"baz\", NULL};\n    if (!PyArg_ParseTupleAndKeywords(args, kwargs, \"ss|sss\", kwlist,\n                                     &a, &b, &foo, &bar, &baz))\n    {\n        return NULL;\n    }\n    printf(\"a is %s\\n\", a);\n    printf(\"b is %s\\n\", b);\n    printf(\"foo is %s\\n\", foo);\n    printf(\"bar is %s\\n\", bar);\n    printf(\"baz is %s\\n\", baz);\n    Py_RETURN_NONE;\n}\n// ...\nstatic PyMethodDef SpamMethods[] =\n{\n    // ...\n    {\"keywords\", (PyCFunction) keywords, METH_VARARGS | METH_KEYWORDS, \"practice kwargs\"},\n    {NULL, NULL, 0, NULL}\n    // ...\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/55912363/how-to-analyze-dependency-tree-for-conda",
    "body": "# version\n$ conda-tree --version\nconda-tree 0.0.4\n# packages that no other package depends on\n$ conda-tree leaves\n['samtools','bcftools',...]\n# dependencies of a specific package\n$ conda-tree depends samtools\n['curl', 'xz', 'libgcc', 'zlib']\n# which packages depend on a specific package\n$ conda-tree whoneeds xz\n['samtools', 'bcftools', 'htslib', 'python']\n# dependency cycles\n$ conda-tree cycles\npip -> python -> pip\npip -> wheel -> python -> pip\n# query a different conda prefix/env\n$ conda-tree -p /conda/envs/trinity leaves\n['trinity']\n# query by name\n$ conda-tree -n trinity leaves\n['trinity']"
  },
  {
    "url": "https://stackoverflow.com/questions/64901945/how-to-send-a-progress-of-operation-in-a-fastapi-app",
    "body": "import asyncio\nfrom http import HTTPStatus\nfrom fastapi import BackgroundTasks\nfrom typing import Dict, List\nfrom uuid import UUID, uuid4\nimport uvicorn\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel, Field\nclass Job(BaseModel):\n    uid: UUID = Field(default_factory=uuid4)\n    status: str = \"in_progress\"\n    progress: int = 0\n    result: int = None\napp = FastAPI()\njobs: Dict[UUID, Job] = {}  # Dict as job storage\nasync def long_task(queue: asyncio.Queue, param: int):\n    for i in range(1, param):  # do work and return our progress\n        await asyncio.sleep(1)\n        await queue.put(i)\n    await queue.put(None)\nasync def start_new_task(uid: UUID, param: int) -> None:\n    queue = asyncio.Queue()\n    task = asyncio.create_task(long_task(queue, param))\n    while progress := await queue.get():  # monitor task progress\n        jobs[uid].progress = progress\n    jobs[uid].status = \"complete\"\n@app.post(\"/new_task/{param}\", status_code=HTTPStatus.ACCEPTED)\nasync def task_handler(background_tasks: BackgroundTasks, param: int):\n    new_task = Job()\n    jobs[new_task.uid] = new_task\n    background_tasks.add_task(start_new_task, new_task.uid, param)\n    return new_task\n@app.get(\"/task/{uid}/status\")\nasync def status_handler(uid: UUID):\n    return jobs[uid]"
  },
  {
    "url": "https://stackoverflow.com/questions/64901945/how-to-send-a-progress-of-operation-in-a-fastapi-app",
    "body": "import time\nfrom http import HTTPStatus\nfrom fastapi import BackgroundTasks, UploadFile, File\nfrom typing import Dict, List\nfrom uuid import UUID, uuid4\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel, Field\nclass Job(BaseModel):\n    uid: UUID = Field(default_factory=uuid4)\n    status: str = \"in_progress\"\n    processed_files: List[str] = Field(default_factory=list)\napp = FastAPI()\njobs: Dict[UUID, Job] = {}\ndef process_files(task_id: UUID, files: List[UploadFile]):\n    for i in files:\n        time.sleep(5)  # pretend long task\n        # ...\n        # do a lot of operations on each file\n        # then append the processed file to a list\n        # ...\n        jobs[task_id].processed_files.append(i.filename)\n    jobs[task_id].status = \"completed\"\n@app.post('/work/test', status_code=HTTPStatus.ACCEPTED)\nasync def work(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...)):\n    new_task = Job()\n    jobs[new_task.uid] = new_task\n    background_tasks.add_task(process_files, new_task.uid, files)\n    return new_task\n@app.get(\"/work/{uid}/status\")\nasync def status_handler(uid: UUID):\n    return jobs[uid]"
  },
  {
    "url": "https://stackoverflow.com/questions/71031816/how-do-you-properly-reuse-an-httpx-asyncclient-within-a-fastapi-application",
    "body": "import logging\nfrom fastapi import FastAPI\nimport httpx\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)-9s %(asctime)s - %(name)s - %(message)s\")\nLOGGER = logging.getLogger(__name__)\nclass HTTPXClientWrapper:\n    async_client = None\n    def start(self):\n        \"\"\" Instantiate the client. Call from the FastAPI startup hook.\"\"\"\n        self.async_client = httpx.AsyncClient()\n        LOGGER.info(f'httpx AsyncClient instantiated. Id {id(self.async_client)}')\n    async def stop(self):\n        \"\"\" Gracefully shutdown. Call from FastAPI shutdown hook.\"\"\"\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed} - Now close it. Id (will be unchanged): {id(self.async_client)}')\n        await self.async_client.aclose()\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')\n        self.async_client = None\n        LOGGER.info('httpx AsyncClient closed')\n    def __call__(self):\n        \"\"\" Calling the instantiated HTTPXClientWrapper returns the wrapped singleton.\"\"\"\n        # Ensure we don't use it if not started / running\n        assert self.async_client is not None\n        LOGGER.info(f'httpx async_client.is_closed(): {self.async_client.is_closed}. Id (will be unchanged): {id(self.async_client)}')\n        return self.async_client\nhttpx_client_wrapper = HTTPXClientWrapper()\napp = FastAPI()\n@app.get('/test-call-external')\nasync def call_external_api(url: str = 'https://stackoverflow.com'):\n    async_client = httpx_client_wrapper()\n    res = await async_client.get(url)\n    result = res.text\n    return {\n        'result': result,\n        'status': res.status_code\n    }\n@app.on_event(\"startup\")\nasync def startup_event():\n    httpx_client_wrapper.start()\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    await httpx_client_wrapper.stop()\nif __name__ == '__main__':\n    import uvicorn\n    LOGGER.info(f'starting...')\n    uvicorn.run(f\"{__name__}:app\", host=\"127.0.0.1\", port=8000)"
  },
  {
    "url": "https://stackoverflow.com/questions/71031816/how-do-you-properly-reuse-an-httpx-asyncclient-within-a-fastapi-application",
    "body": "from fastapi import FastAPI, Depends\nimport httpx\napp = FastAPI()\nasync def get_client():\n    # create a new client for each request\n    async with httpx.AsyncClient() as client:\n        # yield the client to the endpoint function\n        yield client\n        # close the client when the request is done\n@app.get(\"/foo\")\nasync def foo(client: httpx.AsyncClient = Depends(get_client)):\n    # use the client to make some http requests, e.g.,\n    response = await client.get(\"http://example.it\")\n    return response.json()"
  },
  {
    "url": "https://stackoverflow.com/questions/16673778/python-regex-match-in-multiline-but-still-want-to-get-the-line-number",
    "body": "def finditer_with_line_numbers(pattern, string, flags=0):\n    \"\"\"\n    A version of ``re.finditer`` that returns ``(match, line_number)`` pairs.\n    \"\"\"\n    import re\n    matches = list(re.finditer(pattern, string, flags))\n    if matches:\n        end = matches[-1].start()\n        # -1 so a failed `rfind` maps to the first line.\n        newline_table = {-1: 0}\n        for i, m in enumerate(re.finditer(\"\\\\n\", string), 1):\n            # Don't find newlines past our last match.\n            offset = m.start()\n            if offset > end:\n                break\n            newline_table[offset] = i\n        # Failing to find the newline is OK, -1 maps to 0.\n        for m in matches:\n            newline_offset = string.rfind(\"\\n\", 0, m.start())\n            line_number = newline_table[newline_offset]\n            yield (m, line_number)"
  },
  {
    "url": "https://stackoverflow.com/questions/24418449/pretty-output-with-pyyaml",
    "body": "import sys\nimport ruamel.yaml\nfrom pathlib import Path\nLT = ruamel.yaml.scalarstring.LiteralScalarString\nfile_org = Path('org.yaml')\nfile_plain = Path('plain.yaml')\nfile_block = Path('block.yaml')\ndef normalise(d):\n    if isinstance(d, dict):\n        for k, v in d.items():\n             d[k] = normalise(v)\n        return d\n    if isinstance(d, list):\n        for idx, elem in enumerate(d):\n            d[idx] = normalise(elem)\n        return d\n    if not isinstance(d, str):\n        return d\n    if '\\n' in d:\n        if isinstance(d, LT):\n            return d     # already a block style literal scalar\n        return LT(d)\n    return str(d)\nyaml = ruamel.yaml.YAML()\nfor fn in [file_org, file_plain, file_block]:\n    data = normalise(yaml.load(file_org))\n    yaml.dump(data, fn)\nassert file_org.read_bytes() == file_plain.read_bytes()\nassert file_org.read_bytes() == file_block.read_bytes()\nprint(file_block.read_text())"
  },
  {
    "url": "https://stackoverflow.com/questions/65184035/alembic-ignore-specific-tables",
    "body": "def run_migrations_online():\n    ...\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n            # THE FOLLOWING LINE WAS MISSING FROM MY ORIGINAL CODE\n            include_object=include_object, # <----------------------- THIS!\n        )\n    ..."
  },
  {
    "url": "https://stackoverflow.com/questions/51772493/understanding-output-from-statsmodels-grangercausalitytests",
    "body": "Granger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=19.8998 , p=0.0000  , df_denom=221, df_num=1\nssr based chi2 test:   chi2=20.1700 , p=0.0000  , df=1\nlikelihood ratio test: chi2=19.3129 , p=0.0000  , df=1\nparameter F test:         F=19.8998 , p=0.0000  , df_denom=221, df_num=1\nGranger Causality\nnumber of lags (no zero) 25\nssr based F test:         F=6.9970  , p=0.0000  , df_denom=149, df_num=25\nssr based chi2 test:   chi2=234.7975, p=0.0000  , df=25\nlikelihood ratio test: chi2=155.3126, p=0.0000  , df=25\nparameter F test:         F=6.9970  , p=0.0000  , df_denom=149, df_num=25"
  },
  {
    "url": "https://stackoverflow.com/questions/51772493/understanding-output-from-statsmodels-grangercausalitytests",
    "body": "Granger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.1279  , p=0.7210  , df_denom=219, df_num=1\nssr based chi2 test:   chi2=0.1297  , p=0.7188  , df=1\nlikelihood ratio test: chi2=0.1296  , p=0.7188  , df=1\nparameter F test:         F=0.1279  , p=0.7210  , df_denom=219, df_num=1\nGranger Causality\nnumber of lags (no zero) 25\nssr based F test:         F=6.2471  , p=0.0000  , df_denom=147, df_num=25\nssr based chi2 test:   chi2=210.3621, p=0.0000  , df=25\nlikelihood ratio test: chi2=143.3297, p=0.0000  , df=25\nparameter F test:         F=6.2471  , p=0.0000  , df_denom=147, df_num=25"
  },
  {
    "url": "https://stackoverflow.com/questions/74057367/how-to-get-rid-of-the-in-place-futurewarning-when-setting-an-entire-column-from",
    "body": "import numpy as np\nimport pandas as pd\nimport warnings\ndf = pd.DataFrame({\"price\": [11.1, 12.2]}, index=[\"book1\", \"book2\"])\noriginal_prices = df[\"price\"]\nnew_prices = np.array([98, 99])\nwith warnings.catch_warnings():\n    # Setting values in-place is fine, ignore the warning in Pandas >= 1.5.0\n    # This can be removed, if Pandas 1.5.0 does not need to be supported any longer.\n    # See also: https://stackoverflow.com/q/74057367/859591\n    warnings.filterwarnings(\n        \"ignore\",\n        category=FutureWarning,\n        message=(\n            \".*will attempt to set the values inplace instead of always setting a new array. \"\n            \"To retain the old behavior, use either.*\"\n        ),\n    )\n    df.iloc[:, 0] = new_prices\ndf.iloc[:, 0]"
  },
  {
    "url": "https://stackoverflow.com/questions/55105045/python-invalid-base64-encoded-string-number-of-data-characters-5-cannot-be-1",
    "body": ">>> import base64\n>>> import os\n>>> from cryptography.fernet import Fernet\n>>> from cryptography.hazmat.backends import default_backend\n>>> from cryptography.hazmat.primitives import hashes\n>>> from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n>>> password = b\"password\"\n>>> hkdf = HKDF(\n...     algorithm=hashes.SHA256(),  # You can swap this out for hashes.MD5()\n...     length=32,\n...     salt=None,    # You may be able to remove this line but I'm unable to test\n...     info=None,    # You may also be able to remove this line\n...     backend=default_backend()\n... )\n>>> key = base64.urlsafe_b64encode(hkdf.derive(password))\n>>> f = Fernet(key)\n>>> token = f.encrypt(b\"Secret message!\")\n>>> token\nb'...'\n>>> f.decrypt(token)  # Process the key in the exact same manner to decode an encoded message\nb'Secret message!'"
  },
  {
    "url": "https://stackoverflow.com/questions/53581589/matplotlib-can-i-use-a-secondary-font-for-missing-glyphs",
    "body": "import matplotlib.pyplot as plt\nfrom matplotlib.textpath import TextPath\nfrom fontTools.ttLib import TTFont\nfig = plt.figure()\nplt.axis([0, 8, 0, 6])\nt = u'abcde♥'\nplt.text(4.5, 4, 'DejaVu Sans:', horizontalalignment='right')\nplt.text(5, 4, t, {'family':'DejaVu Sans'})\nplt.text(4.5, 3, 'Noto Sans:', horizontalalignment='right')\nplt.text(5, 3, t, {'family':'Noto Sans'})\nplt.text(4.5, 2, 'Noto Sans Symbols2:', horizontalalignment='right')\nplt.text(5, 2, t, {'family':'Noto Sans Symbols2'})\ndef doesContain(fontPath, unicode_char):  # Helper function, the only issue being it takes font paths instead of names\n    font = TTFont(fontPath)  # Use helper library to go through all characters\n    for cmap in font['cmap'].tables:\n        if cmap.isUnicode():\n            if ord(unicode_char) in cmap.cmap:  # If the character table contains our character return True\n                return True\n    # Otherwise, return False.\n    return False\n\ndef renderText(x, y, text, fontSize, fallback_list, spacingSize):\n    xPosNow = x\n\n    for char in text:  # For each character...\n        fontId = 0\n        while not doesContain(fallback_list[fontId]['path'], char):  # find a font that works\n            if fontId < len(fallback_list) - 1:\n                fontId += 1\n            else:  # Or just go with the first font, if nothing seems to match\n                fontId = 0\n                break\n        print(fontId)\n\n        t = plt.text(xPosNow, y, char, {'family':fallback_list[fontId]['name']})\n        r = fig.canvas.get_renderer()\n        xPosNow += t.get_window_extent(renderer=r).width/100 + spacingSize"
  },
  {
    "url": "https://stackoverflow.com/questions/44603119/how-to-display-a-pandas-data-frame-with-pyqt5-pyside2",
    "body": "class DataFrameModel(QtCore.QAbstractTableModel):\n    DtypeRole = QtCore.Qt.UserRole + 1000\n    ValueRole = QtCore.Qt.UserRole + 1001\n    def __init__(self, df=pd.DataFrame(), parent=None):\n        super(DataFrameModel, self).__init__(parent)\n        self._dataframe = df\n    def setDataFrame(self, dataframe):\n        self.beginResetModel()\n        self._dataframe = dataframe.copy()\n        self.endResetModel()\n    def dataFrame(self):\n        return self._dataframe\n    dataFrame = QtCore.pyqtProperty(pd.DataFrame, fget=dataFrame, fset=setDataFrame)\n    @QtCore.pyqtSlot(int, QtCore.Qt.Orientation, result=str)\n    def headerData(self, section: int, orientation: QtCore.Qt.Orientation, role: int = QtCore.Qt.DisplayRole):\n        if role == QtCore.Qt.DisplayRole:\n            if orientation == QtCore.Qt.Horizontal:\n                return self._dataframe.columns[section]\n            else:\n                return str(self._dataframe.index[section])\n        return QtCore.QVariant()\n    def rowCount(self, parent=QtCore.QModelIndex()):\n        if parent.isValid():\n            return 0\n        return len(self._dataframe.index)\n    def columnCount(self, parent=QtCore.QModelIndex()):\n        if parent.isValid():\n            return 0\n        return self._dataframe.columns.size\n    def data(self, index, role=QtCore.Qt.DisplayRole):\n        if not index.isValid() or not (0 <= index.row() < self.rowCount() \\\n            and 0 <= index.column() < self.columnCount()):\n            return QtCore.QVariant()\n        row = self._dataframe.index[index.row()]\n        col = self._dataframe.columns[index.column()]\n        dt = self._dataframe[col].dtype\n        val = self._dataframe.iloc[row][col]\n        if role == QtCore.Qt.DisplayRole:\n            return str(val)\n        elif role == DataFrameModel.ValueRole:\n            return val\n        if role == DataFrameModel.DtypeRole:\n            return dt\n        return QtCore.QVariant()\n    def roleNames(self):\n        roles = {\n            QtCore.Qt.DisplayRole: b'display',\n            DataFrameModel.DtypeRole: b'dtype',\n            DataFrameModel.ValueRole: b'value'\n        }\n        return roles"
  },
  {
    "url": "https://stackoverflow.com/questions/53995171/anaconda-conda-error-argument-command-invalid-choice-when-trying-to-update-pa",
    "body": "no change     /opt/homebrew/anaconda3/condabin/conda\nno change     /opt/homebrew/anaconda3/bin/conda\nno change     /opt/homebrew/anaconda3/bin/conda-env\nno change     /opt/homebrew/anaconda3/bin/activate\nno change     /opt/homebrew/anaconda3/bin/deactivate\nno change     /opt/homebrew/anaconda3/etc/profile.d/conda.sh\nno change     /opt/homebrew/anaconda3/etc/fish/conf.d/conda.fish\nno change     /opt/homebrew/anaconda3/shell/condabin/Conda.psm1\nno change     /opt/homebrew/anaconda3/shell/condabin/conda-hook.ps1\nno change     /opt/homebrew/anaconda3/lib/python3.11/site-packages/xontrib/conda.xsh\nno change     /opt/homebrew/anaconda3/etc/profile.d/conda.csh\nmodified /Users/<username>/.zshrc # <--- ADDITONAL CHANGES"
  },
  {
    "url": "https://stackoverflow.com/questions/55099243/python3-dataclass-with-kwargsasterisk",
    "body": "from dataclasses import dataclass\nfrom inspect import signature\n@dataclass\nclass Container:\n    user_id: int\n    body: str\n    @classmethod\n    def from_kwargs(cls, **kwargs):\n        # fetch the constructor's signature\n        cls_fields = {field for field in signature(cls).parameters}\n        # split the kwargs into native ones and new ones\n        native_args, new_args = {}, {}\n        for name, val in kwargs.items():\n            if name in cls_fields:\n                native_args[name] = val\n            else:\n                new_args[name] = val\n        # use the native ones to create the class ...\n        ret = cls(**native_args)\n        # ... and add the new ones by hand\n        for new_name, new_val in new_args.items():\n            setattr(ret, new_name, new_val)\n        return ret"
  },
  {
    "url": "https://stackoverflow.com/questions/11782147/python-opencv-contour-tree-hierarchy-structure",
    "body": "0:\t[ 6 -1  1 -1]\t18:\t[19 -1 -1 17]\n1:\t[ 2 -1 -1  0]\t19:\t[20 18 -1 17]\n2:\t[ 3  1 -1  0]\t20:\t[21 19 -1 17]\n3:\t[ 4  2 -1  0]\t21:\t[22 20 -1 17]\n4:\t[ 5  3 -1  0]\t22:\t[-1 21 -1 17]\n5:\t[-1  4 -1  0]\t23:\t[27 17 24 -1]\n6:\t[11  0  7 -1]\t24:\t[25 -1 -1 23]\n7:\t[ 8 -1 -1  6]\t25:\t[26 24 -1 23]\n8:\t[ 9  7 -1  6]\t26:\t[-1 25 -1 23]\n9:\t[10  8 -1  6]\t27:\t[32 23 28 -1]\n10:\t[-1  9 -1  6]\t28:\t[29 -1 -1 27]\n11:\t[17  6 12 -1]\t29:\t[30 28 -1 27]\n12:\t[15 -1 13 11]\t30:\t[31 29 -1 27]\n13:\t[14 -1 -1 12]\t31:\t[-1 30 -1 27]\n14:\t[-1 13 -1 12]\t32:\t[-1 27 33 -1]\n15:\t[16 12 -1 11]\t33:\t[34 -1 -1 32]\n16:\t[-1 15 -1 11]\t34:\t[35 33 -1 32]\n17:\t[23 11 18 -1]\t35:\t[-1 34 -1 32]"
  },
  {
    "url": "https://stackoverflow.com/questions/76734333/pydantic-v2-field-validator-values-argument-equivalent",
    "body": "from typing import Any\nfrom pydantic import BaseModel, ValidationError, validator\nclass UserModel(BaseModel):\n    ...\n    password1: str\n    password2: str\n    @validator(\"password2\")\n    def passwords_match(cls, v: str, values: dict[str, Any]) -> str:\n        if \"password1\" in values and v != values[\"password1\"]:\n            raise ValueError(\"passwords do not match\")\n        return v\ntry:\n    UserModel(password1=\"abc\", password2=\"xyz\")\nexcept ValidationError as err:\n    print(err.json(indent=4))"
  },
  {
    "url": "https://stackoverflow.com/questions/76734333/pydantic-v2-field-validator-values-argument-equivalent",
    "body": "from pydantic import BaseModel, ValidationError, ValidationInfo, field_validator\nclass UserModel(BaseModel):\n    ...\n    password1: str\n    password2: str\n    @field_validator(\"password2\")\n    def passwords_match(cls, v: str, info: ValidationInfo) -> str:\n        if \"password1\" in info.data and v != info.data[\"password1\"]:\n            raise ValueError(\"passwords do not match\")\n        return v\ntry:\n    UserModel(password1=\"abc\", password2=\"xyz\")\nexcept ValidationError as err:\n    print(err.json(indent=4))"
  },
  {
    "url": "https://stackoverflow.com/questions/76734333/pydantic-v2-field-validator-values-argument-equivalent",
    "body": "from typing import Annotated\nfrom pydantic import AfterValidator, BaseModel, ValidationError, ValidationInfo\ndef ensure_passwords_match(v: str, info: ValidationInfo) -> str:\n    if \"password1\" in info.data and v != info.data[\"password1\"]:\n        raise ValueError(\"passwords do not match\")\n    return v\nclass UserModel(BaseModel):\n    ...\n    password1: str\n    password2: Annotated[str, AfterValidator(ensure_passwords_match)]\ntry:\n    UserModel(password1=\"abc\", password2=\"xyz\")\nexcept ValidationError as err:\n    print(err.json(indent=4))"
  },
  {
    "url": "https://stackoverflow.com/questions/64501193/fastapi-how-to-use-httpexception-in-responses",
    "body": "from fastapi import FastAPI\nfrom fastapi.exceptions import HTTPException\nfrom pydantic import BaseModel\nclass Dummy(BaseModel):\n    name: str\nclass HTTPError(BaseModel):\n    detail: str\n    class Config:\n        schema_extra = {\n            \"example\": {\"detail\": \"HTTPException raised.\"},\n        }\napp = FastAPI()\n@app.get(\n    \"/test\",\n    responses={\n        200: {\"model\": Dummy},\n        409: {\n            \"model\": HTTPError,\n            \"description\": \"This endpoint always raises an error\",\n        },\n    },\n)\ndef raises_error():\n    raise HTTPException(409, detail=\"Error raised\")"
  },
  {
    "url": "https://stackoverflow.com/questions/51525691/realtime-offline-speech-recognition-in-python",
    "body": "    from vosk import Model, KaldiRecognizer\n    import pyaudio\n\n    model = Model(r\"C:\\\\Users\\User\\Desktop\\python practice\\ai\\vosk-model-small-en-us-0.15\")\n    recognizer = KaldiRecognizer(model, 16000)\n\n    mic = pyaudio.PyAudio()\n    stream = mic.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=8192)\n    stream.start_stream()\n\n    while True:\n        data = stream.read(4096)\n\n\n        if recognizer.AcceptWaveform(data):\n            text = recognizer.Result()\n            print(f\"' {text[14:-3]} '\")"
  },
  {
    "url": "https://stackoverflow.com/questions/29800749/delaunay-triangulation-of-points-from-2d-surface-in-3d-with-python",
    "body": "from matplotlib import path as mpath\nfrom mayavi import mlab\nimport numpy as np\ndef make_star(amplitude=1.0, rotation=0.0):\n    \"\"\" Make a star shape\n    \"\"\"\n    t = np.linspace(0, 2*np.pi, 6) + rotation\n    star = np.zeros((12, 2))\n    star[::2] = np.c_[np.cos(t), np.sin(t)]\n    star[1::2] = 0.5*np.c_[np.cos(t + np.pi / 5), np.sin(t + np.pi / 5)]\n    return amplitude * star\ndef make_stars(n_stars=51, z_diff=0.05):\n    \"\"\" Make `2*n_stars-1` stars stacked in 3D\n    \"\"\"\n    amps = np.linspace(0.25, 1, n_stars)\n    amps = np.r_[amps, amps[:-1][::-1]]\n    rots = np.linspace(0, 2*np.pi, len(amps))\n    zamps = np.linspace\n    stars = []\n    for i, (amp, rot) in enumerate(zip(amps, rots)):\n        star = make_star(amplitude=amp, rotation=rot)\n        height = i*z_diff\n        z = np.full(len(star), height)\n        star3d = np.c_[star, z]\n        stars.append(star3d)\n    return stars\ndef polygon_to_boolean(points, xvals, yvals):\n    \"\"\" Convert `points` to a boolean indicator mask\n    over the specified domain\n    \"\"\"\n    x, y = np.meshgrid(xvals, yvals)\n    xy = np.c_[x.flatten(), y.flatten()]\n    mask = mpath.Path(points).contains_points(xy).reshape(x.shape)\n    return x, y, mask\ndef plot_contours(stars):\n    \"\"\" Plot a list of stars in 3D\n    \"\"\"\n    n = len(stars)\n    for i, star in enumerate(stars):\n        x, y, z = star.T\n        mlab.plot3d(*star.T)\n        #ax.plot3D(x, y, z, '-o', c=(0, 1-i/n, i/n))\n        #ax.set_xlim(-1, 1)\n        #ax.set_ylim(-1, 1)\n    mlab.show()\nif __name__ == '__main__':\n    # Make and plot the 2D contours\n    stars3d = make_stars()\n    plot_contours(stars3d)\n    xvals = np.linspace(-1, 1, 101)\n    yvals = np.linspace(-1, 1, 101)\n    volume = np.dstack([\n        polygon_to_boolean(star[:,:2], xvals, yvals)[-1]\n        for star in stars3d\n    ]).astype(float)\n    mlab.contour3d(volume, contours=[0.5])\n    mlab.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/37094419/python-requests-retry-request-after-re-authentication",
    "body": "session = requests.Session()\nsession.headers.update({\"Authorization\": f\"Bearer deliberate-wrong-token\"})\ndef refresh_token(r, *args, **kwargs):\n    if r.status_code == 401:\n        logger.info(\"Fetching new token as the previous token expired\")\n        token = get_token()\n        session.headers.update({\"Authorization\": f\"Bearer {token}\"})\n        r.request.headers[\"Authorization\"] = session.headers[\"Authorization\"]\n        return session.send(r.request, verify=False)\nsession.hooks['response'].append(refresh_token)"
  },
  {
    "url": "https://stackoverflow.com/questions/53290302/how-to-handle-optional-arguments-in-logging-format-strings",
    "body": "import logging\nimport re\nclass CustomFormatter(logging.Formatter):\n    def format(self, record: logging.LogRecord) -> str:\n        arg_pattern = re.compile(r'%\\((\\w+)\\)')\n        arg_names = [x.group(1) for x in arg_pattern.finditer(self._fmt)]\n        for field in arg_names:\n            if field not in record.__dict__:\n                record.__dict__[field] = None\n        return super().format(record)\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nformatter = CustomFormatter('{\"message\": \"%(message)s\", \"user\": \"%(user)s\"}')\nhandler.setFormatter(formatter)\nlogger.setLevel(logging.INFO)\nlogger.addHandler(handler)\nlogger.info('hi')\nlogger.info('hi', extra={\"user\": \"asmith\"})"
  },
  {
    "url": "https://stackoverflow.com/questions/53290302/how-to-handle-optional-arguments-in-logging-format-strings",
    "body": "import logging\nclass ExtraFormatter(logging.Formatter):\n    def format(self, record: logging.LogRecord) -> str:\n        default_attrs = logging.LogRecord(None, None, None, None, None, None, None).__dict__.keys()\n        extras = set(record.__dict__.keys()) - default_attrs\n        log_items = ['\"message\": \"%(message)s\"']\n        for attr in extras:\n            log_items.append(f'\"{attr}\": \"%({attr})s\"')\n        format_str = f'{{{\", \".join(log_items)}}}'\n        self._style._fmt = format_str\n        return super().format(record)\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nformatter = ExtraFormatter()\nhandler.setFormatter(formatter)\nlogger.setLevel(logging.INFO)\nlogger.addHandler(handler)\nlogger.info('hi')\nlogger.info('hi', extra={\"user\": \"asmith\", \"number\": \"42\"})"
  },
  {
    "url": "https://stackoverflow.com/questions/63110848/how-do-i-send-list-of-dictionary-as-body-parameter-together-with-files-in-fastap",
    "body": "from fastapi import FastAPI, File, UploadFile, Body, status\nfrom pydantic import BaseModel\nfrom typing import Optional, List\nimport json\napp = FastAPI()\nclass DataModelOut(BaseModel):\n    message: str = None\n    id: str = None\n    input_data: dict = None\n    result: List[dict] = []\n    statusCode: int\n\n\nclass DataModelIn(BaseModel):\n    countryId: str\n    policyDetails: List[dict]\n    leaveTypeId: str\n    branchIds: List[str]\n    cityIds: List[str]\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate_to_json\n    @classmethod\n    def validate_to_json(cls, value):\n        if isinstance(value, str):\n            return cls(**json.loads(value))\n        return value\n\n@app.post('/', response_model=DataModelOut)\ndef create_policy_details(data: DataModelIn = Body(...), files: Optional[List[UploadFile]] = File(None)):\n    print('Files received: ', [f.filename for f in files])\n    return {'input_data':data, 'statusCode': status.HTTP_201_CREATED}"
  },
  {
    "url": "https://stackoverflow.com/questions/63110848/how-do-i-send-list-of-dictionary-as-body-parameter-together-with-files-in-fastap",
    "body": "from fastapi import FastAPI, File, UploadFile, Body, status\nfrom pydantic import BaseModel, model_validator\nfrom typing import Optional, List\nimport json\napp = FastAPI()\nclass DataModelOut(BaseModel):\n    message: str = None\n    id: str = None\n    input_data: dict = None\n    result: List[dict] = []\n    statusCode: int\n\n\nclass DataModelIn(BaseModel):\n    countryId: str\n    policyDetails: List[dict]\n    leaveTypeId: str\n    branchIds: List[str]\n    cityIds: List[str]\n    @model_validator(mode='before')\n    @classmethod\n    def validate_to_json(cls, value):\n        if isinstance(value, str):\n            return cls(**json.loads(value))\n        return value\n\n@app.post('/', response_model=DataModelOut)\ndef create_policy_details(data: DataModelIn = Body(...), files: List[UploadFile] = File(...)):\n    print('Files received: ', [f.filename for f in files])\n    return {'input_data': data.model_dump(), 'statusCode': status.HTTP_201_CREATED}"
  },
  {
    "url": "https://stackoverflow.com/questions/54358287/pass-and-use-input-parameters-to-a-lambda-task-from-a-step-function",
    "body": "{\n  \"Comment\": \"A Hello World example of the Amazon States Language using an AWS Lambda function\",\n  \"StartAt\": \"HelloWorld\",\n  \"States\": {\n    \"HelloWorld\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:ap-southeast-2:XXXXXXX:function:fields_sync\",\n      \"Next\": \"HelloWorld2\"\n    },\n    \"HelloWorld2\": {\n      \"Type\": \"Task\",\n      \"InputPath\": \"$\",\n      \"ResultPath\": \"$.taskresult\"\n      \"Resource\": \"arn:aws:lambda:ap-southeast-2:XXXXXXX:function:fields_sync_2\",\n      \"End\": true\n    }\n  }\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/6567724/set-minor-tick-label-spacing-on-a-log-axis-and-change-colorbar-tick-label-size",
    "body": "import numpy as np\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(111)\nx = np.arange(10,3000,100)\ny = np.arange(10,3000,100)\nX,Y = np.meshgrid(x,y)\nZ = np.random.random(X.shape)*8000000\nsurf = ax.contourf(X,Y,Z, 8, cmap=plt.cm.jet)\nax.set_ylabel('Log Frequency (Hz)')\nax.set_xlabel('Log Frequency (Hz)')\nax.set_xscale('log')\nax.set_yscale('log')\nax.xaxis.set_minor_formatter(plt.FormatStrFormatter('%d'))\n# defining custom minor tick locations:\nax.xaxis.set_minor_locator(plt.FixedLocator([50,500,2000]))\nax.yaxis.set_ticks_position('left')\nax.xaxis.set_ticks_position('bottom')\nax.tick_params(axis='both',reset=False,which='both',length=8,width=2)\ncbar = fig.colorbar(surf, shrink=0.5, aspect=20, fraction=.12,pad=.02)\ncbar.set_label('Activation',size=18)\n# access to cbar tick labels:\ncbar.ax.tick_params(labelsize=5)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/72854116/selenium-attributeerror-webdriver-object-has-no-attribute-find-element-by-cs",
    "body": ".find_element_by_class_name(\n.find_element(By.CLASS_NAME,\n.find_element_by_css_selector(\n.find_element(By.CSS_SELECTOR,\n.find_element_by_id(\n.find_element(By.ID,\n.find_element_by_link_text(\n.find_element(By.LINK_TEXT,\n.find_element_by_name(\n.find_element(By.NAME,\n.find_element_by_partial_link_text(\n.find_element(By.PARTIAL_LINK_TEXT,\n.find_element_by_tag_name(\n.find_element(By.TAG_NAME,\n.find_element_by_xpath(\n.find_element(By.XPATH,\n.find_elements_by_class_name(\n.find_elements(By.CLASS_NAME,\n.find_elements_by_css_selector(\n.find_elements(By.CSS_SELECTOR,\n.find_elements_by_id(\n.find_elements(By.ID,\n.find_elements_by_link_text(\n.find_elements(By.LINK_TEXT,\n.find_elements_by_name(\n.find_elements(By.NAME,\n.find_elements_by_partial_link_text(\n.find_elements(By.PARTIAL_LINK_TEXT,\n.find_elements_by_tag_name(\n.find_elements(By.TAG_NAME,\n.find_elements_by_xpath(\n.find_elements(By.XPATH,"
  },
  {
    "url": "https://stackoverflow.com/questions/45440900/throttling-async-functions-in-python-asyncio",
    "body": "import asyncio\nfrom contextlib import AbstractAsyncContextManager\nfrom functools import partial\nfrom heapq import heappop, heappush\nfrom itertools import count\nfrom types import TracebackType\nfrom typing import List, Optional, Tuple, Type\nclass AsyncLimiter(AbstractAsyncContextManager):\n    \"\"\"A leaky bucket rate limiter.\n    This is an :ref:`asynchronous context manager <async-context-managers>`;\n    when used with :keyword:`async with`, entering the context acquires\n    capacity::\n        limiter = AsyncLimiter(10)\n        for foo in bar:\n            async with limiter:\n                # process foo elements at 10 items per minute\n    :param max_rate: Allow up to `max_rate` / `time_period` acquisitions before\n       blocking.\n    :param time_period: duration, in seconds, of the time period in which to\n       limit the rate. Note that up to `max_rate` acquisitions are allowed\n       within this time period in a burst.\n    \"\"\"\n    __slots__ = (\n        \"max_rate\",\n        \"time_period\",\n        \"_rate_per_sec\",\n        \"_level\",\n        \"_last_check\",\n        \"_event_loop\",\n        \"_waiters\",\n        \"_next_count\",\n        \"_waker_handle\",\n    )\n    max_rate: float  #: The configured `max_rate` value for this limiter.\n    time_period: float  #: The configured `time_period` value for this limiter.\n    def __init__(self, max_rate: float, time_period: float = 60) -> None:\n        self.max_rate = max_rate\n        self.time_period = time_period\n        self._rate_per_sec = max_rate / time_period\n        self._level = 0.0\n        self._last_check = 0.0\n        # timer until next waiter can resume\n        self._waker_handle: asyncio.TimerHandle | None = None\n        # min-heap with (amount requested, order, future) for waiting tasks\n        self._waiters: List[Tuple[float, int, \"asyncio.Future[None]\"]] = []\n        # counter used to order waiting tasks\n        self._next_count = partial(next, count())\n    @property\n    def _loop(self) -> asyncio.AbstractEventLoop:\n        self._event_loop: asyncio.AbstractEventLoop\n        try:\n            loop = self._event_loop\n        except AttributeError:\n            loop = self._event_loop = asyncio.get_running_loop()\n        return loop\n    def _leak(self) -> None:\n        \"\"\"Drip out capacity from the bucket.\"\"\"\n        now = self._loop.time()\n        if self._level:\n            # drip out enough level for the elapsed time since\n            # we last checked\n            elapsed = now - self._last_check\n            decrement = elapsed * self._rate_per_sec\n            self._level = max(self._level - decrement, 0)\n        self._last_check = now\n    def has_capacity(self, amount: float = 1) -> bool:\n        \"\"\"Check if there is enough capacity remaining in the limiter\n        :param amount: How much capacity you need to be available.\n        \"\"\"\n        self._leak()\n        return self._level + amount <= self.max_rate\n    async def acquire(self, amount: float = 1) -> None:\n        \"\"\"Acquire capacity in the limiter.\n        If the limit has been reached, blocks until enough capacity has been\n        freed before returning.\n        :param amount: How much capacity you need to be available.\n        :exception: Raises :exc:`ValueError` if `amount` is greater than\n           :attr:`max_rate`.\n        \"\"\"\n        if amount > self.max_rate:\n            raise ValueError(\"Can't acquire more than the maximum capacity\")\n        loop = self._loop\n        while not self.has_capacity(amount):\n            # Add a future to the _waiters heapq to be notified when capacity\n            # has come up. The future callback uses call_soon so other tasks\n            # are checked *after* completing capacity acquisition in this task.\n            fut = loop.create_future()\n            fut.add_done_callback(partial(loop.call_soon, self._wake_next))\n            heappush(self._waiters, (amount, self._next_count(), fut))\n            self._wake_next()\n            await fut\n        self._level += amount\n        # reset the waker to account for the new, lower level.\n        self._wake_next()\n        return None\n    def _wake_next(self, *_args: object) -> None:\n        \"\"\"Wake the next waiting future or set a timer\"\"\"\n        # clear timer and any cancelled futures at the top of the heap\n        heap, handle, self._waker_handle = self._waiters, self._waker_handle, None\n        if handle is not None:\n            handle.cancel()\n        while heap and heap[0][-1].done():\n            heappop(heap)\n        if not heap:\n            # nothing left waiting\n            return\n        amount, _, fut = heap[0]\n        self._leak()\n        needed = amount - self.max_rate + self._level\n        if needed <= 0:\n            heappop(heap)\n            fut.set_result(None)\n            # fut.set_result triggers another _wake_next call\n            return\n        wake_next_at = self._last_check + (1 / self._rate_per_sec * needed)\n        self._waker_handle = self._loop.call_at(wake_next_at, self._wake_next)\n    def __repr__(self) -> str:  # pragma: no cover\n        args = f\"max_rate={self.max_rate!r}, time_period={self.time_period!r}\"\n        state = f\"level: {self._level:f}, waiters: {len(self._waiters)}\"\n        if (handle := self._waker_handle) and not handle.cancelled():\n            microseconds = int((handle.when() - self._loop.time()) * 10**6)\n            if microseconds > 0:\n                state += f\", waking in {microseconds} \\N{MICRO SIGN}s\"\n        return f\"<AsyncLimiter({args}) at {id(self):#x} [{state}]>\"\n    async def __aenter__(self) -> None:\n        await self.acquire()\n        return None\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        return None"
  },
  {
    "url": "https://stackoverflow.com/questions/69774921/im-using-wsl-how-i-upgrade-python-to-the-last-version-through-the-console",
    "body": "# Update package lists{\nsudo apt update\n# Install dependent libraries:\nsudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev libffi-dev curl libbz2-dev\n# Download Python binary package:\nwget https://www.python.org/ftp/python/3.10.8/Python-3.10.8.tgz\n# Unzip the package:\ntar -xzf Python-3.10.8.tgz\n# Execute configure script\ncd Python-3.10.8\n./configure --enable-optimizations\n# Build Python 3.10\nmake -j 2\n# Install Python 3.10\nsudo make install\n# Verify the installation\npython3.10"
  },
  {
    "url": "https://stackoverflow.com/questions/43367805/pandas-read-excel-multiple-tables-on-the-same-sheet",
    "body": "def parse_excel_sheet(file, sheet_name=0, threshold=5):\n    '''parses multiple tables from an excel sheet into multiple data frame objects. Returns [dfs, df_mds], where dfs is a list of data frames and df_mds their potential associated metadata'''\n    xl = pd.ExcelFile(file)\n    entire_sheet = xl.parse(sheet_name=sheet_name)\n    # count the number of non-Nan cells in each row and then the change in that number between adjacent rows\n    n_values = np.logical_not(entire_sheet.isnull()).sum(axis=1)\n    n_values_deltas = n_values[1:] - n_values[:-1].values\n    # define the beginnings and ends of tables using delta in n_values\n    table_beginnings = n_values_deltas > threshold\n    table_beginnings = table_beginnings[table_beginnings].index\n    table_endings = n_values_deltas < -threshold\n    table_endings = table_endings[table_endings].index\n    if len(table_beginnings) < len(table_endings) or len(table_beginnings) > len(table_endings)+1:\n        raise BaseException('Could not detect equal number of beginnings and ends')\n    # look for metadata before the beginnings of tables\n    md_beginnings = []\n    for start in table_beginnings:\n        md_start = n_values.iloc[:start][n_values==0].index[-1] + 1\n        md_beginnings.append(md_start)\n    # make data frames\n    dfs = []\n    df_mds = []\n    for ind in range(len(table_beginnings)):\n        start = table_beginnings[ind]+1\n        if ind < len(table_endings):\n            stop = table_endings[ind]\n        else:\n            stop = entire_sheet.shape[0]\n        df = xl.parse(sheet_name=sheet_name, skiprows=start, nrows=stop-start)\n        dfs.append(df)\n        md = xl.parse(sheet_name=sheet_name, skiprows=md_beginnings[ind], nrows=start-md_beginnings[ind]-1).dropna(axis=1)\n        df_mds.append(md)\n    return dfs, df_mds"
  },
  {
    "url": "https://stackoverflow.com/questions/62287001/how-to-overlay-two-plots-in-same-figure-in-plotly-create-pareto-chart-in-plotl",
    "body": "import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\ntrace1 = go.Bar(\n    x=df[cat],\n    y=df[num],\n    name=num,\n    marker=dict(\n        color='rgb(34,163,192)'\n               )\n)\ntrace2 = go.Scatter(\n    x=df[cat],\n    y=df['cumulative_perc'],\n    name='Cumulative Percentage',\n    yaxis='y2'\n)\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(trace1)\nfig.add_trace(trace2,secondary_y=True)\nfig['layout'].update(height = 600, width = 800, title = title,xaxis=dict(\n      tickangle=-90\n    ))\niplot(fig)"
  },
  {
    "url": "https://stackoverflow.com/questions/49851280/showing-a-simple-matplotlib-plot-in-plotly-dash",
    "body": "import io\nimport base64\n...\n\napp.layout = html.Div(children=[\n    ...,\n    html.Img(id='example') # img element\n])\n@app.callback(\n    dash.dependencies.Output('example', 'src'), # src attribute\n    [dash.dependencies.Input('n_points', 'value')]\n)\ndef update_figure(n_points):\n    #create some matplotlib graph\n    x = np.random.rand(n_points)\n    y = np.random.rand(n_points)\n    buf = io.BytesIO() # in-memory files\n    plt.scatter(x, y)\n    plt.savefig(buf, format = \"png\")\n    plt.close()\n    data = base64.b64encode(buf.getbuffer()).decode(\"utf8\") # encode to html elements\n    buf.close()\n    return \"data:image/png;base64,{}\".format(data)"
  },
  {
    "url": "https://stackoverflow.com/questions/42147776/producing-2d-perlin-noise-with-numpy",
    "body": "import numpy as np\nimport matplotlib.pyplot as plt\ndef perlin(x, y, seed=0):\n    # permutation table\n    np.random.seed(seed)\n    p = np.arange(256, dtype=int)\n    np.random.shuffle(p)\n    p = np.stack([p, p]).flatten()\n    # coordinates of the top-left\n    xi, yi = x.astype(int), y.astype(int)\n    # internal coordinates\n    xf, yf = x - xi, y - yi\n    # fade factors\n    u, v = fade(xf), fade(yf)\n    # noise components\n    n00 = gradient(p[p[xi] + yi], xf, yf)\n    n01 = gradient(p[p[xi] + yi + 1], xf, yf - 1)\n    n11 = gradient(p[p[xi + 1] + yi + 1], xf - 1, yf - 1)\n    n10 = gradient(p[p[xi + 1] + yi], xf - 1, yf)\n    # combine noises\n    x1 = lerp(n00, n10, u)\n    x2 = lerp(n01, n11, u)  # FIX1: I was using n10 instead of n01\n    return lerp(x1, x2, v)  # FIX2: I also had to reverse x1 and x2 here\ndef lerp(a, b, x):\n    \"linear interpolation\"\n    return a + x * (b - a)\ndef fade(t):\n    \"6t^5 - 15t^4 + 10t^3\"\n    return 6 * t**5 - 15 * t**4 + 10 * t**3\ndef gradient(h, x, y):\n    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n    vectors = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]])\n    g = vectors[h % 4]\n    return g[:, :, 0] * x + g[:, :, 1] * y\n# EDIT : generating noise at multiple frequencies and adding them up\np = np.zeros((100,100))\nfor i in range(4):\n    freq = 2**i\n    lin = np.linspace(0, freq, 100, endpoint=False)\n    x, y = np.meshgrid(lin, lin)  # FIX3: I thought I had to invert x and y here but it was a mistake\n    p = perlin(x, y, seed=87) / freq + p\nplt.imshow(p, origin='upper')"
  },
  {
    "url": "https://stackoverflow.com/questions/63560005/draw-curved-lines-to-connect-points-in-matplotlib",
    "body": "import matplotlib.pyplot as plt\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nimport numpy as np\nn_teams = 4\nn_weeks = 4\nt = np.array([[1, 2, 4, 3],\n              [4, 3, 3, 2],\n              [3, 4, 1, 4],\n              [2, 1, 2, 1]])\nfig, ax = plt.subplots(figsize=(10, 4), facecolor='#1b1b1b')\nax.set_facecolor('#1b1b1b')\nindent = 0.8\nfor tj in t:\n    ax.scatter(np.arange(len(tj)), tj, marker='o', color='#4F535C', s=100, zorder=3)\n    # create bezier curves\n    verts = [(i + d, tij) for i, tij in enumerate(tj) for d in (-indent, 0, indent)][1:-1]\n    codes = [Path.MOVETO] + [Path.CURVE4] * (len(verts) - 1)\n    path = Path(verts, codes)\n    patch = patches.PathPatch(path, facecolor='none', lw=2, edgecolor='#4F535C')\n    ax.add_patch(patch)\nax.set_xticks([])\nax.set_yticks([])\nax.autoscale() # sets the xlim and ylim for the added patches\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/51556996/do-a-dry-run-of-an-alembic-upgrade",
    "body": "def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection, target_metadata=target_metadata\n        )\n        with context.begin_transaction():\n            context.run_migrations()"
  },
  {
    "url": "https://stackoverflow.com/questions/51556996/do-a-dry-run-of-an-alembic-upgrade",
    "body": "def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n        # Ensure the context will create a transaction\n        # for backends that don't normally use transactional DDL.\n        # Note that ROLLBACK will not roll back DDL structures\n        # on databases such as MySQL, as well as with the SQLite\n        # Python driver's default settings.\n        transactional_ddl=True,\n    )\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection, target_metadata=target_metadata\n        )\n        with context.begin_transaction() as transaction:\n            context.run_migrations()\n            if 'dry-run' in context.get_x_argument():\n                print('Dry-run succeeded; now rolling back transaction...')\n                transaction.rollback()"
  },
  {
    "url": "https://stackoverflow.com/questions/724924/how-to-make-pdb-recognize-that-the-source-has-changed-between-runs",
    "body": "# pdbs.py -\tPDB support\nfrom __future__ import print_function\ndef r():\n    \"\"\"Reload all non-system modules, to reload stuff on pbd restart. \"\"\"\n    import importlib\n    import sys\n    # This is likely to be OS-specific\n    SYS_PREFIX = '/usr/lib'\n    for k, v in list(sys.modules.items()):\n        if (\n            k == \"__main__\" or\n            k.startswith(\"pdb\") or\n            not getattr(v, \"__file__\", None)\n            or v.__file__.startswith(SYS_PREFIX)\n        ):\n            continue\n        print(\"reloading %s [%s]\" % (k, v.__file__), file=sys.stderr)\n        importlib.reload(v)"
  },
  {
    "url": "https://stackoverflow.com/questions/62376164/how-to-change-max-iter-in-optimize-function-used-by-sklearn-gaussian-process-reg",
    "body": "from functools import partial\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nimport scipy.optimize\nclass MyGPR(GaussianProcessRegressor):\n    def __init__(self, *args, max_iter=15000, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._max_iter = max_iter\n    def _constrained_optimization(self, obj_func, initial_theta, bounds):\n        def new_optimizer(obj_func, initial_theta, bounds):\n            return scipy.optimize.minimize(\n                obj_func,\n                initial_theta,\n                method=\"L-BFGS-B\",\n                jac=True,\n                bounds=bounds,\n                max_iter=self._max_iter,\n            )\n        self.optimizer = new_optimizer\n        return super()._constrained_optimization(obj_func, initial_theta, bounds)"
  },
  {
    "url": "https://stackoverflow.com/questions/38000993/how-can-i-get-my-assertions-in-pytest-to-stop-being-abbreviated-with-ellipsis",
    "body": "========================================================== FAILURES ===========================================================\n______________________________________________ test_truncated_exception_message _______________________________________________\n    def test_truncated_exception_message():\n        with raises(Exception) as exception_info:\n            raise ValueError(\"a\"*1024)\n\n        exception_str = str(exception_info.value)\n>       assert problem_function(\"a\"*1024, exception_str)\nE       AssertionError: assert False\nE        +  where False = problem_function(('a' * 1024), 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')\nexception_info = <ExceptionInfo ValueError('aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa') tblen=1>\nexception_str = 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaa'\nbeartype_test/unit/pep/p484/test_p484.py:39: AssertionError"
  },
  {
    "url": "https://stackoverflow.com/questions/72779926/gunicorn-cuda-cannot-re-initialize-cuda-in-forked-subprocess",
    "body": "import time\nimport torch\nimport torch.multiprocessing as mp\ndef f(q):\n    y = q.get()\n    y[0] = 1000\ndef g(q):\n    x = torch.zeros(1).cuda()\n    x.share_memory_()\n    q.put(x)\n    q.put(x)\n    while True:\n        time.sleep(1)  # this process must live as long as x is in use\nif __name__ == '__main__':\n    queue = mp.Queue()\n    pf = mp.Process(target=f, args=(queue,), daemon=True)\n    pf.start()\n    pg = mp.Process(target=g, args=(queue,), daemon=True)\n    pg.start()\n    pf.join()\n    x = queue.get()\n    print(\"x =\", x.item())  # Prints x = 1000.0"
  },
  {
    "url": "https://stackoverflow.com/questions/72779926/gunicorn-cuda-cannot-re-initialize-cuda-in-forked-subprocess",
    "body": "import logging\nimport os\nfrom torch.multiprocessing.reductions import ForkingPickler\nimport zmq\ndef request_model(zmq_url: str):\n    logging.info(\"Connecting\")\n    context = zmq.Context()\n    with context.socket(zmq.REQ) as socket:\n        socket.connect(zmq_url)\n        logging.info(\"Sending request\")\n        socket.send(ForkingPickler.dumps(os.getpid()))\n        logging.info(\"Waiting for a response\")\n        model = ForkingPickler.loads(socket.recv())\n    logging.info(\"Got response from object server\")\n    return model"
  },
  {
    "url": "https://stackoverflow.com/questions/72779926/gunicorn-cuda-cannot-re-initialize-cuda-in-forked-subprocess",
    "body": "from argparse import ArgumentParser\nimport logging\nimport torch\nfrom torch.multiprocessing.reductions import ForkingPickler\nimport zmq\ndef load_model():\n    model = torch.nn.Linear(10000, 50000)\n    model.cuda()\n    model.share_memory()\n    counter = torch.zeros(1).cuda()\n    counter.share_memory_()\n    return model, counter\ndef share_object(obj, url):\n    context = zmq.Context()\n    socket = context.socket(zmq.REP)\n    socket.bind(url)\n    while True:\n        logging.info(\"Waiting for requests on %s\", url)\n        message = socket.recv()\n        logging.info(\"Got a message from %d\", ForkingPickler.loads(message))\n        socket.send(ForkingPickler.dumps(obj))\nif __name__ == '__main__':\n    parser = ArgumentParser(description=\"Serve model\")\n    parser.add_argument(\"--listen-address\", default=\"tcp://127.0.0.1:5555\")\n    args = parser.parse_args()\n    logging.basicConfig(level=logging.INFO)\n    logging.info(\"Loading model\")\n    model = load_model()\n    share_object(model, args.listen_address)"
  },
  {
    "url": "https://stackoverflow.com/questions/72779926/gunicorn-cuda-cannot-re-initialize-cuda-in-forked-subprocess",
    "body": "$ python server.py &\nINFO:root:Waiting for requests on tcp://127.0.0.1:5555\n$ gunicorn -c config.py app:app\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Starting gunicorn 20.1.0\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Listening at: http://127.0.0.1:8080 (24113)\n[2023-02-01 16:45:34 +0800] [24113] [INFO] Using worker: sync\n[2023-02-01 16:45:34 +0800] [24186] [INFO] Booting worker with pid: 24186\nINFO:root:Connecting\nINFO:root:Sending request\nINFO:root:Waiting for a response\nINFO:root:Got response from object server"
  },
  {
    "url": "https://stackoverflow.com/questions/43292197/can-python-implement-dependent-types",
    "body": "class Integer:\n    def __init__(self, value):\n        self.value = int(value)\n        self.set_class()\n\n    def set_class(self):\n        if self.value < 10:\n            self.__class__ = LessThanTen\n        else:\n            self.__class__ = TenOrMore\n\n    def add(self, value):\n        self.value += int(value)\n        self.set_class()\n\nclass TenOrMore(Integer):\n    def __init__(self):\n        pass\n        raise ValueError(\"Use Integer()\")\n\nclass LessThanTen(Integer):\n    def __init__(self):\n        raise ValueError(\"Use Integer()\")"
  },
  {
    "url": "https://stackoverflow.com/questions/52558519/bluez-profile-registration",
    "body": "\"\"\"\nBluetooth HID keyboard emulator DBUS Service\nOriginal idea taken from:\nhttp://yetanotherpointlesstechblog.blogspot.com/2016/04/emulating-bluetooth-keyboard-with.html\nMoved to Python 3 and tested with BlueZ 5.43\nUpdates documented at:\nhttps://gist.github.com/ukBaz/a47e71e7b87fbc851b27cde7d1c0fcf0\n\"\"\"\nimport os\nimport sys\nimport dbus\nimport dbus.service\nimport socket\nfrom gi.repository import GLib\nfrom dbus.mainloop.glib import DBusGMainLoop\nclass HumanInterfaceDeviceProfile(dbus.service.Object):\n    \"\"\"\n    BlueZ D-Bus Profile for HID\n    \"\"\"\n    fd = -1\n    @dbus.service.method('org.bluez.Profile1',\n                         in_signature='', out_signature='')\n    def Release(self):\n            print('Release')\n            mainloop.quit()\n    @dbus.service.method('org.bluez.Profile1',\n                         in_signature='oha{sv}', out_signature='')\n    def NewConnection(self, path, fd, properties):\n            self.fd = fd.take()\n            print('NewConnection({}, {})'.format(path, self.fd))\n            for key in properties.keys():\n                    if key == 'Version' or key == 'Features':\n                            print('  {} = 0x{:04x}'.format(key,\n                                                           properties[key]))\n                    else:\n                            print('  {} = {}'.format(key, properties[key]))\n    @dbus.service.method('org.bluez.Profile1',\n                         in_signature='o', out_signature='')\n    def RequestDisconnection(self, path):\n            print('RequestDisconnection {}'.format(path))\n            if self.fd > 0:\n                    os.close(self.fd)\n                    self.fd = -1\nclass BTKbDevice:\n    \"\"\"\n    create a bluetooth device to emulate a HID keyboard\n    \"\"\"\n    MY_DEV_NAME = 'BT_HID_Keyboard'\n    # Service port - must match port configured in SDP record\n    P_CTRL = 17\n    # Service port - must match port configured in SDP record#Interrrupt port\n    P_INTR = 19\n    # BlueZ dbus\n    PROFILE_DBUS_PATH = '/bluez/yaptb/btkb_profile'\n    ADAPTER_IFACE = 'org.bluez.Adapter1'\n    DEVICE_INTERFACE = 'org.bluez.Device1'\n    DBUS_PROP_IFACE = 'org.freedesktop.DBus.Properties'\n    DBUS_OM_IFACE = 'org.freedesktop.DBus.ObjectManager'\n    # file path of the sdp record to laod\n    install_dir  = os.path.dirname(os.path.realpath(__file__))\n    SDP_RECORD_PATH = os.path.join(install_dir,\n                                   'sdp_record.xml')\n    # UUID for HID service (1124)\n    # https://www.bluetooth.com/specifications/assigned-numbers/service-discovery\n    UUID = '00001124-0000-1000-8000-00805f9b34fb'\n    def __init__(self, hci=0):\n        self.scontrol = None\n        self.ccontrol = None  # Socket object for control\n        self.sinterrupt = None\n        self.cinterrupt = None  # Socket object for interrupt\n        self.dev_path = '/org/bluez/hci{}'.format(hci)\n        print('Setting up BT device')\n        self.bus = dbus.SystemBus()\n        self.adapter_methods = dbus.Interface(\n            self.bus.get_object('org.bluez',\n                                self.dev_path),\n            self.ADAPTER_IFACE)\n        self.adapter_property = dbus.Interface(\n            self.bus.get_object('org.bluez',\n                                self.dev_path),\n            self.DBUS_PROP_IFACE)\n        self.bus.add_signal_receiver(self.interfaces_added,\n                                     dbus_interface=self.DBUS_OM_IFACE,\n                                     signal_name='InterfacesAdded')\n        self.bus.add_signal_receiver(self._properties_changed,\n                                     dbus_interface=self.DBUS_PROP_IFACE,\n                                     signal_name='PropertiesChanged',\n                                     arg0=self.DEVICE_INTERFACE,\n                                     path_keyword='path')\n        print('Configuring for name {}'.format(BTKbDevice.MY_DEV_NAME))\n        self.config_hid_profile()\n        # set the Bluetooth device configuration\n        self.alias = BTKbDevice.MY_DEV_NAME\n        self.discoverabletimeout = 0\n        self.discoverable = True\n    def interfaces_added(self):\n        pass\n    def _properties_changed(self, interface, changed, invalidated, path):\n        if self.on_disconnect is not None:\n            if 'Connected' in changed:\n                if not changed['Connected']:\n                    self.on_disconnect()\n    def on_disconnect(self):\n        print('The client has been disconnect')\n        self.listen()\n    @property\n    def address(self):\n        \"\"\"Return the adapter MAC address.\"\"\"\n        return self.adapter_property.Get(self.ADAPTER_IFACE,\n                                         'Address')\n    @property\n    def powered(self):\n        \"\"\"\n        power state of the Adapter.\n        \"\"\"\n        return self.adapter_property.Get(self.ADAPTER_IFACE, 'Powered')\n    @powered.setter\n    def powered(self, new_state):\n        self.adapter_property.Set(self.ADAPTER_IFACE, 'Powered', new_state)\n    @property\n    def alias(self):\n        return self.adapter_property.Get(self.ADAPTER_IFACE,\n                                         'Alias')\n    @alias.setter\n    def alias(self, new_alias):\n        self.adapter_property.Set(self.ADAPTER_IFACE,\n                                  'Alias',\n                                  new_alias)\n    @property\n    def discoverabletimeout(self):\n        \"\"\"Discoverable timeout of the Adapter.\"\"\"\n        return self.adapter_props.Get(self.ADAPTER_IFACE,\n                                      'DiscoverableTimeout')\n    @discoverabletimeout.setter\n    def discoverabletimeout(self, new_timeout):\n        self.adapter_property.Set(self.ADAPTER_IFACE,\n                                  'DiscoverableTimeout',\n                                  dbus.UInt32(new_timeout))\n    @property\n    def discoverable(self):\n        \"\"\"Discoverable state of the Adapter.\"\"\"\n        return self.adapter_props.Get(\n            self.ADAPTER_INTERFACE, 'Discoverable')\n    @discoverable.setter\n    def discoverable(self, new_state):\n        self.adapter_property.Set(self.ADAPTER_IFACE,\n                                  'Discoverable',\n                                  new_state)\n    def config_hid_profile(self):\n        \"\"\"\n        Setup and register HID Profile\n        \"\"\"\n        print('Configuring Bluez Profile')\n        service_record = self.read_sdp_service_record()\n        opts = {\n            'Role': 'server',\n            'RequireAuthentication': False,\n            'RequireAuthorization': False,\n            'AutoConnect': True,\n            'ServiceRecord': service_record,\n        }\n        manager = dbus.Interface(self.bus.get_object('org.bluez',\n                                                     '/org/bluez'),\n                                 'org.bluez.ProfileManager1')\n        HumanInterfaceDeviceProfile(self.bus,\n                                    BTKbDevice.PROFILE_DBUS_PATH)\n        manager.RegisterProfile(BTKbDevice.PROFILE_DBUS_PATH,\n                                BTKbDevice.UUID,\n                                opts)\n        print('Profile registered ')\n    @staticmethod\n    def read_sdp_service_record():\n        \"\"\"\n        Read and return SDP record from a file\n        :return: (string) SDP record\n        \"\"\"\n        print('Reading service record')\n        try:\n            fh = open(BTKbDevice.SDP_RECORD_PATH, 'r')\n        except OSError:\n            sys.exit('Could not open the sdp record. Exiting...')\n        return fh.read()\n    def listen(self):\n        \"\"\"\n        Listen for connections coming from HID client\n        \"\"\"\n        print('Waiting for connections')\n        self.scontrol = socket.socket(socket.AF_BLUETOOTH,\n                                      socket.SOCK_SEQPACKET,\n                                      socket.BTPROTO_L2CAP)\n        self.scontrol.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.sinterrupt = socket.socket(socket.AF_BLUETOOTH,\n                                        socket.SOCK_SEQPACKET,\n                                        socket.BTPROTO_L2CAP)\n        self.sinterrupt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.scontrol.bind((self.address, self.P_CTRL))\n        self.sinterrupt.bind((self.address, self.P_INTR))\n        # Start listening on the server sockets\n        self.scontrol.listen(1)  # Limit of 1 connection\n        self.sinterrupt.listen(1)\n        self.ccontrol, cinfo = self.scontrol.accept()\n        print('{} connected on the control socket'.format(cinfo[0]))\n        self.cinterrupt, cinfo = self.sinterrupt.accept()\n        print('{} connected on the interrupt channel'.format(cinfo[0]))\n    def send(self, msg):\n        \"\"\"\n        Send HID message\n        :param msg: (bytes) HID packet to send\n        \"\"\"\n        self.cinterrupt.send(bytes(bytearray(msg)))\nclass BTKbService(dbus.service.Object):\n    \"\"\"\n    Setup of a D-Bus service to recieve HID messages from other\n    processes.\n    Send the recieved HID messages to the Bluetooth HID server to send\n    \"\"\"\n    def __init__(self):\n        print('Setting up service')\n        bus_name = dbus.service.BusName('org.yaptb.btkbservice',\n                                        bus=dbus.SystemBus())\n        dbus.service.Object.__init__(self, bus_name, '/org/yaptb/btkbservice')\n        # create and setup our device\n        self.device = BTKbDevice()\n        # start listening for socket connections\n        self.device.listen()\n    @dbus.service.method('org.yaptb.btkbservice',\n                         in_signature='ay')\n    def send_keys(self, cmd):\n        self.device.send(cmd)\nif __name__ == '__main__':\n    # The sockets require root permission\n    if not os.geteuid() == 0:\n        sys.exit('Only root can run this script')\n    DBusGMainLoop(set_as_default=True)\n    myservice = BTKbService()\n    mainloop = GLib.MainLoop()\n    mainloop.run()"
  },
  {
    "url": "https://stackoverflow.com/questions/52558519/bluez-profile-registration",
    "body": "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<record>\n\t<attribute id=\"0x0001\">\n\t\t<sequence>\n\t\t\t<uuid value=\"0x1124\" />\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x0004\">\n\t\t<sequence>\n\t\t\t<sequence>\n\t\t\t\t<uuid value=\"0x0100\" />\n\t\t\t\t<uint16 value=\"0x0011\" />\n\t\t\t</sequence>\n\t\t\t<sequence>\n\t\t\t\t<uuid value=\"0x0011\" />\n\t\t\t</sequence>\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x0005\">\n\t\t<sequence>\n\t\t\t<uuid value=\"0x1002\" />\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x0006\">\n\t\t<sequence>\n\t\t\t<uint16 value=\"0x656e\" />\n\t\t\t<uint16 value=\"0x006a\" />\n\t\t\t<uint16 value=\"0x0100\" />\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x0009\">\n\t\t<sequence>\n\t\t\t<sequence>\n\t\t\t\t<uuid value=\"0x1124\" />\n\t\t\t\t<uint16 value=\"0x0100\" />\n\t\t\t</sequence>\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x000d\">\n\t\t<sequence>\n\t\t\t<sequence>\n\t\t\t\t<sequence>\n\t\t\t\t\t<uuid value=\"0x0100\" />\n\t\t\t\t\t<uint16 value=\"0x0013\" />\n\t\t\t\t</sequence>\n\t\t\t\t<sequence>\n\t\t\t\t\t<uuid value=\"0x0011\" />\n\t\t\t\t</sequence>\n\t\t\t</sequence>\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x0100\">\n\t\t<text value=\"Raspberry Pi Virtual Keyboard\" />\n\t</attribute>\n\t<attribute id=\"0x0101\">\n\t\t<text value=\"USB > BT Keyboard\" />\n\t</attribute>\n\t<attribute id=\"0x0102\">\n\t\t<text value=\"Raspberry Pi\" />\n\t</attribute>\n\t<attribute id=\"0x0200\">\n\t\t<uint16 value=\"0x0100\" />\n\t</attribute>\n\t<attribute id=\"0x0201\">\n\t\t<uint16 value=\"0x0111\" />\n\t</attribute>\n\t<attribute id=\"0x0202\">\n\t\t<uint8 value=\"0x40\" />\n\t</attribute>\n\t<attribute id=\"0x0203\">\n\t\t<uint8 value=\"0x00\" />\n\t</attribute>\n\t<attribute id=\"0x0204\">\n\t\t<boolean value=\"false\" />\n\t</attribute>\n\t<attribute id=\"0x0205\">\n\t\t<boolean value=\"false\" />\n\t</attribute>\n\t<attribute id=\"0x0206\">\n\t\t<sequence>\n\t\t\t<sequence>\n\t\t\t\t<uint8 value=\"0x22\" />\n\t\t\t\t<text encoding=\"hex\" value=\"05010906a101850175019508050719e029e715002501810295017508810395057501050819012905910295017503910395067508150026ff000507190029ff8100c0050c0901a1018503150025017501950b0a23020a21020ab10109b809b609cd09b509e209ea09e9093081029501750d8103c0\" />\n\t\t\t</sequence>\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x0207\">\n\t\t<sequence>\n\t\t\t<sequence>\n\t\t\t\t<uint16 value=\"0x0409\" />\n\t\t\t\t<uint16 value=\"0x0100\" />\n\t\t\t</sequence>\n\t\t</sequence>\n\t</attribute>\n\t<attribute id=\"0x020b\">\n\t\t<uint16 value=\"0x0100\" />\n\t</attribute>\n\t<attribute id=\"0x020c\">\n\t\t<uint16 value=\"0x0c80\" />\n\t</attribute>\n\t<attribute id=\"0x020d\">\n\t\t<boolean value=\"true\" />\n\t</attribute>\n\t<attribute id=\"0x020e\">\n\t\t<boolean value=\"false\" />\n\t</attribute>\n\t<attribute id=\"0x020f\">\n\t\t<uint16 value=\"0x0640\" />\n\t</attribute>\n\t<attribute id=\"0x0210\">\n\t\t<uint16 value=\"0x0320\" />\n\t</attribute>\n</record>"
  },
  {
    "url": "https://stackoverflow.com/questions/62759748/downloading-data-from-a-shared-google-drive-link-in-google-colab",
    "body": "# Install the PyDrive wrapper & import libraries.\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n# Authenticate and create the PyDrive client.\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\nfile_id = '1cUaIEd9-MLJHFGjLz5QziNvfBtYygplX'\ndownloaded = drive.CreateFile({'id':file_id})\ndownloaded.FetchMetadata(fetch_all=True)\ndownloaded.GetContentFile(downloaded.metadata['title'])"
  },
  {
    "url": "https://stackoverflow.com/questions/19227724/check-if-a-function-uses-classmethod",
    "body": ">>> class notclassmethod:\n...     def __init__(self, f):\n...         self.f = f\n...     def __get__(self, _, typ=None):\n...         return self.f.__get__(typ, typ)\n...\n>>> class Base:\n...     @classmethod\n...     def base_cm(cls): pass\n...     @notclassmethod\n...     def base_ncm(cls): pass\n...     def base_m(self): pass\n...\n>>> class Derived(Base):\n...     @classmethod\n...     def derived_cm(cls): pass\n...     @notclassmethod\n...     def derived_ncm(cls): pass\n...     def derived_m(self): pass\n...\n>>> inspect.ismethod(Derived.base_cm) and Derived.base_cm.__self__ is Derived\nTrue\n>>> inspect.ismethod(Derived.base_ncm) and Derived.base_ncm.__self__ is Derived\nTrue\n>>> inspect.ismethod(Derived.base_m) and Derived.base_m.__self__ is Derived\nFalse\n>>> inspect.ismethod(Derived.derived_cm) and Derived.derived_cm.__self__ is Derived\nTrue\n>>> inspect.ismethod(Derived.derived_ncm) and Derived.derived_ncm.__self__ is Derived\nTrue\n>>> inspect.ismethod(Derived.derived_m) and Derived.derived_m.__self__ is Derived\nFalse\n>>> isclassmethod(Derived.base_cm)\nTrue\n>>> isclassmethod(Derived.base_ncm)\nFalse\n>>> isclassmethod(Derived.base_m)\nFalse\n>>> isclassmethod(Derived.derived_cm)\nTrue\n>>> isclassmethod(Derived.derived_ncm)\nFalse\n>>> isclassmethod(Derived.derived_m)\nFalse"
  },
  {
    "url": "https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame",
    "body": "def flatten_json(nested_json, exclude=['']):\n    \"\"\"Flatten json object with nested keys into a single level.\n        Args:\n            nested_json: A nested json object.\n            exclude: Keys to exclude from output.\n        Returns:\n            The flattened json object if successful, None otherwise.\n    \"\"\"\n    out = {}\n    def flatten(x, name='', exclude=exclude):\n        if type(x) is dict:\n            for a in x:\n                if a not in exclude: flatten(x[a], name + a + '_')\n        elif type(x) is list:\n            i = 0\n            for a in x:\n                flatten(a, name + str(i) + '_')\n                i += 1\n        else:\n            out[name[:-1]] = x\n    flatten(nested_json)\n    return out"
  },
  {
    "url": "https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame",
    "body": "this_dict = {'events': [\n  {'id': 142896214,\n   'playerId': 37831,\n   'teamId': 3157,\n   'matchId': 2214569,\n   'matchPeriod': '1H',\n   'eventSec': 0.8935539999999946,\n   'eventId': 8,\n   'eventName': 'Pass',\n   'subEventId': 85,\n   'subEventName': 'Simple pass',\n   'positions': [{'x': 51, 'y': 49}, {'x': 40, 'y': 53}],\n   'tags': [{'id': 1801, 'tag': {'label': 'accurate'}}]},\n {'id': 142896214,\n   'playerId': 37831,\n   'teamId': 3157,\n   'matchId': 2214569,\n   'matchPeriod': '1H',\n   'eventSec': 0.8935539999999946,\n   'eventId': 8,\n   'eventName': 'Pass',\n   'subEventId': 85,\n   'subEventName': 'Simple pass',\n   'positions': [{'x': 51, 'y': 49}, {'x': 40, 'y': 53},{'x': 51, 'y': 49}],\n   'tags': [{'id': 1801, 'tag': {'label': 'accurate'}}]}\n]}"
  },
  {
    "url": "https://stackoverflow.com/questions/52795561/flattening-nested-json-in-pandas-data-frame",
    "body": "pd.DataFrame([flatten_json(x) for x in this_dict['events']])\nOut[1]:\n          id  playerId  teamId  matchId matchPeriod  eventSec  eventId  \\\n0  142896214     37831    3157  2214569          1H  0.893554        8\n1  142896214     37831    3157  2214569          1H  0.893554        8\n  eventName  subEventId subEventName  positions_0_x  positions_0_y  \\\n0      Pass          85  Simple pass             51             49\n1      Pass          85  Simple pass             51             49\n   positions_1_x  positions_1_y  tags_0_id tags_0_tag_label  positions_2_x  \\\n0             40             53       1801         accurate            NaN\n1             40             53       1801         accurate           51.0\n   positions_2_y\n0            NaN\n1           49.0"
  },
  {
    "url": "https://stackoverflow.com/questions/16024677/generate-correlated-data-in-python-3-3",
    "body": "import numpy as np\nimport matplotlib.pyplot as plt\nnum_samples = 400\n# The desired mean values of the sample.\nmu = np.array([5.0, 0.0, 10.0])\n# The desired covariance matrix.\nr = np.array([\n        [  3.40, -2.75, -2.00],\n        [ -2.75,  5.50,  1.50],\n        [ -2.00,  1.50,  1.25]\n    ])\n# Generate the random samples.\nrng = np.random.default_rng()\ny = rng.multivariate_normal(mu, r, size=num_samples)\n# Plot various projections of the samples.\nplt.subplot(2,2,1)\nplt.plot(y[:,0], y[:,1], 'b.', alpha=0.25)\nplt.plot(mu[0], mu[1], 'ro', ms=3.5)\nplt.ylabel('y[1]')\nplt.axis('equal')\nplt.grid(True)\nplt.subplot(2,2,3)\nplt.plot(y[:,0], y[:,2], 'b.', alpha=0.25)\nplt.plot(mu[0], mu[2], 'ro', ms=3.5)\nplt.xlabel('y[0]')\nplt.ylabel('y[2]')\nplt.axis('equal')\nplt.grid(True)\nplt.subplot(2,2,4)\nplt.plot(y[:,1], y[:,2], 'b.', alpha=0.25)\nplt.plot(mu[1], mu[2], 'ro', ms=3.5)\nplt.xlabel('y[1]')\nplt.axis('equal')\nplt.grid(True)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/69183922/playwright-auto-scroll-to-bottom-of-infinite-scroll-page",
    "body": "page.evaluate(\n    \"\"\"\n    var intervalID = setInterval(function () {\n        var scrollingElement = (document.scrollingElement || document.body);\n        scrollingElement.scrollTop = scrollingElement.scrollHeight;\n    }, 200);\n    \"\"\"\n)\nprev_height = None\nwhile True:\n    curr_height = page.evaluate('(window.innerHeight + window.scrollY)')\n    if not prev_height:\n        prev_height = curr_height\n        time.sleep(1)\n    elif prev_height == curr_height:\n        page.evaluate('clearInterval(intervalID)')\n        break\n    else:\n        prev_height = curr_height\n        time.sleep(1)"
  },
  {
    "url": "https://stackoverflow.com/questions/4040605/does-anyone-have-good-examples-of-using-mutagen-to-write-to-files",
    "body": "from pathlib import Path\nfrom mutagen.mp3 import HeaderNotFoundError\nfrom mutagen.id3 import Encoding, ID3NoHeaderError, ID3, TIT2, TALB, TPE1, TPE2, COMM, TCOM, TCON, TDRC, TRCK\n# Read the ID3 tag or create one if not present\naudio_path = Path('/path/to/Music/audio.mp3')\nwith audio_path.open(mode='rb') as fib:\n    try:\n        tags = ID3(fib)\n        print(f'BEFORE:\\n\\n{tags.pprint()}')\n    except (ID3NoHeaderError, HeaderNotFoundError):\n        print(\"Adding ID3 header..\")\n        tags = ID3(audio_path)\n    tags[\"TIT2\"] = TIT2(encoding=Encoding.UTF16BE, text='mutagen Title')\n    tags[\"TALB\"] = TALB(encoding=Encoding.UTF16BE, text='mutagen Album Name 2025')\n    tags[\"TPE2\"] = TPE2(encoding=Encoding.UTF16BE, text='mutagen Band')\n    tags[\"COMM\"] = COMM(encoding=Encoding.UTF16BE, lang='eng', desc='desc', text='mutagen comment')\n    tags[\"TPE1\"] = TPE1(encoding=Encoding.UTF16BE, text='mutagen Artist')\n    tags[\"TCOM\"] = TCOM(encoding=Encoding.UTF16BE, text='mutagen Composer')\n    tags[\"TCON\"] = TCON(encoding=Encoding.UTF16BE, text='mutagen Genre')\n    tags[\"TDRC\"] = TDRC(encoding=Encoding.UTF16BE, text='2010')\n    tags[\"TRCK\"] = TRCK(encoding=Encoding.UTF16BE, text='track_number')\n    tags.save(audio_path)\n    print(f'\\n\\nAFTER:\\n\\n{tags.pprint()}')"
  },
  {
    "url": "https://stackoverflow.com/questions/44374215/how-do-i-specify-url-resolution-in-pythons-requests-library-in-a-similar-fashio",
    "body": "import socket\ndns_cache = {}\n# Capture a dict of hostname and their IPs to override with\ndef override_dns(domain, ip):\n    dns_cache[domain] = ip\nprv_getaddrinfo = socket.getaddrinfo\n# Override default socket.getaddrinfo() and pass ip instead of host\n# if override is detected\ndef new_getaddrinfo(*args):\n    if args[0] in dns_cache:\n        print(\"Forcing FQDN: {} to IP: {}\".format(args[0], dns_cache[args[0]]))\n        return prv_getaddrinfo(dns_cache[args[0]], *args[1:])\n    else:\n        return prv_getaddrinfo(*args)\nsocket.getaddrinfo = new_getaddrinfo"
  },
  {
    "url": "https://stackoverflow.com/questions/48673402/how-can-i-standardize-only-numeric-variables-in-an-sklearn-pipeline",
    "body": "# Author: Pedro Morales <part.morales@gmail.com>\n#\n# License: BSD 3 clause\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nnp.random.seed(0)\n# Load data from https://www.openml.org/d/40945\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)"
  },
  {
    "url": "https://stackoverflow.com/questions/54181163/fasttext-embeddings-sentence-vectors",
    "body": "def l2_norm(x):\n   return np.sqrt(np.sum(x**2))\ndef div_norm(x):\n   norm_value = l2_norm(x)\n   if norm_value > 0:\n       return x * ( 1.0 / norm_value)\n   else:\n       return x\n# Getting word vectors for 'one' and 'two'.\none = model.get_word_vector('yksi')\ntwo = model.get_word_vector('kaksi')\neos = model.get_word_vector('\\n')\n# Getting the sentence vector for the sentence \"one two\" in Finnish.\none_two = model.get_sentence_vector('yksi kaksi')\none_two_avg = (div_norm(one) + div_norm(two) + div_norm(eos)) / 3"
  },
  {
    "url": "https://stackoverflow.com/questions/43009566/skip-forbidden-parameter-combinations-when-using-gridsearchcv",
    "body": "from sklearn import svm, datasets\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import FitFailedWarning, ConvergenceWarning\nfrom sklearn.model_selection import GridSearchCV\nwith ignore_warnings(category=[ConvergenceWarning, FitFailedWarning]):\n    iris = datasets.load_iris()\n    parameters = {'dual':[True, False], 'penalty' : ['l1', 'l2'], \\\n                 'loss': ['hinge', 'squared_hinge']}\n    svc = svm.LinearSVC()\n    clf = GridSearchCV(svc, parameters, error_score=0.0)\n    clf.fit(iris.data, iris.target)"
  },
  {
    "url": "https://stackoverflow.com/questions/33151463/how-to-bin-time-in-a-pandas-dataframe",
    "body": "import pandas as pd\nimport numpy as np  # for test data\nimport random  # for test data\n# setup a sample dataframe; creates 1.5 months of hourly observations\nnp.random.seed(365)\nrandom.seed(365)\ndata = {'date': pd.bdate_range('2020-09-21', freq='h', periods=1100).tolist(),\n        'x': np.random.randint(10, size=(1100))}\ndf = pd.DataFrame(data)\n# the date column of the sample data is already in a datetime format\n# if the date column is not a datetime, then uncomment the following line\n# df.date= pd.to_datetime(df.date)\n# define the bins\nbins = [0, 6, 12, 18, 24]\n# add custom labels if desired\nlabels = ['00:00-05:59', '06:00-11:59', '12:00-17:59', '18:00-23:59']\n# add the bins to the dataframe\ndf['Time Bin'] = pd.cut(df.date.dt.hour, bins, labels=labels, right=False)"
  },
  {
    "url": "https://stackoverflow.com/questions/66602480/fastapi-uvicorn-not-logging-errors",
    "body": "config = {}\n# this is default (site-packages\\uvicorn\\main.py)\nconfig['log_config'] = \"{\n   \"version\":1,\n   \"disable_existing_loggers\":true,\n   \"formatters\":{\n      \"default\":{\n         \"()\":\"uvicorn.logging.DefaultFormatter\",\n         \"fmt\":\"%(levelprefix)s %(message)s\",\n         \"use_colors\":\"None\"\n      },\n      \"access\":{\n         \"()\":\"uvicorn.logging.AccessFormatter\",\n         \"fmt\":\"%(levelprefix)s %(client_addr)s - \\\"%(request_line)s\\\" %(status_code)s\"\n      }\n   },\n   \"handlers\":{\n      \"default\":{\n         \"formatter\":\"default\",\n         \"class\":\"logging.StreamHandler\",\n         \"stream\":\"ext://sys.stderr\"\n      },\n      \"access\":{\n         \"formatter\":\"access\",\n         \"class\":\"logging.StreamHandler\",\n         \"stream\":\"ext://sys.stdout\"\n      }\n   },\n   \"loggers\":{\n      \"uvicorn\":{\n         \"handlers\":[\n            \"default\"\n         ],\n         \"level\":\"INFO\"\n      },\n      \"uvicorn.error\":{\n         \"level\":\"INFO\",\n         \"handlers\":[\n            \"default\"\n         ],\n         \"propagate\":true\n      },\n      \"uvicorn.access\":{\n         \"handlers\":[\n            \"access\"\n         ],\n         \"level\":\"INFO\",\n         \"propagate\":false\n      }\n   }\n}\n# add your handler to it (in my case, I'm working with quart, but you can do this with Flask etc. as well, they're all the same)\nconfig['log_config']['loggers']['quart'] =\n{\n   \"handlers\":[\n      \"default\"\n   ],\n   \"level\":\"INFO\"\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/65362524/in-json-created-from-a-pydantic-basemodel-exclude-optional-if-not-set",
    "body": "from pydantic import BaseModel\nfrom typing import Optional\nfrom pydantic.json import pydantic_encoder\nimport json\nclass Foo(BaseModel):\n    x: int\n    y: int = 42\n    z: Optional[int]\ndef exclude_optional_dict(model: BaseModel):\n    return {**model.dict(exclude_unset=True), **model.dict(exclude_none=True)}\ndef exclude_optional_json(model: BaseModel):\n    return json.dumps(exclude_optional_dict(model), default=pydantic_encoder)\n\nprint(exclude_optional_json(Foo(x=3)))  # {\"x\": 3, \"y\": 42}\nprint(exclude_optional_json(Foo(x=3, z=None)))  # {\"x\": 3, \"z\": null, \"y\": 42}\nprint(exclude_optional_json(Foo(x=3, z=77)))  # {\"x\": 3, \"z\": 77, \"y\": 42}"
  },
  {
    "url": "https://stackoverflow.com/questions/65362524/in-json-created-from-a-pydantic-basemodel-exclude-optional-if-not-set",
    "body": "def union(source, destination):\n    for key, value in source.items():\n        if isinstance(value, dict):\n            node = destination.setdefault(key, {})\n            union(value, node)\n        else:\n            destination[key] = value\n    return destination\ndef exclude_optional_dict(model: BaseModel):\n    return union(model.dict(exclude_unset=True), model.dict(exclude_none=True))\nclass Foo(BaseModel):\n    x: int\n    y: int = 42\n    z: Optional[int]\nclass Bar(BaseModel):\n    a: int\n    b: int = 52\n    c: Optional[int]\n    d: Foo\nprint(exclude_optional_json(Bar(a=4, d=Foo(x=3))))\nprint(exclude_optional_json(Bar(a=4, c=None, d=Foo(x=3, z=None))))\nprint(exclude_optional_json(Bar(a=4, c=78, d=Foo(x=3, z=77))))"
  },
  {
    "url": "https://stackoverflow.com/questions/67733566/how-to-use-pipenv-on-mac",
    "body": "   eval \"$(brew shellenv)\"\n\n   # Set your preferred python version.\n   # If you just want the latest release, you don't need to\n   # specify anything more than the major version number.\n   export PYENV_VERSION=3\n\n   # Tell pyenv where to keep your python installations.\n   export PYENV_ROOT=~/.pyenv\n\n   # Tell pipx where to install executables.\n   # pipx is like brew, but for python.\n   export PIPX_BIN_DIR=~/.local/bin\n\n   # -U eliminates duplicates.\n   export -U PATH path\n   path=(\n     $PIPX_BIN_DIR\n     $PYENV_ROOT/{bin,shims}\n     $path\n   )\n\n   # Installs/updates pipenv and all of its dependencies.\n   pybake() {\n     # If any commands fail, exit the function.\n     setopt LOCAL_OPTIONS ERR_RETURN\n\n     install-or-upgrade() {\n       if command -v $1 &>/dev/null; then\n         print -n \"upgrade $1\"\n       else\n         print -n \"install $1\"\n       fi\n     }\n\n     # Store the command to unfunction in a trap\n     trap \"unfunction install-or-upgrade\" EXIT\n\n     brew $(install-or-upgrade pyenv)\n     pyenv install --skip-existing $PYENV_VERSION\n     pip install --upgrade pip\n\n     # --user installs to ~/.local/bin\n     pip install --upgrade --user pipx\n\n     pipx $(install-or-upgrade pipenv)\n   }"
  },
  {
    "url": "https://stackoverflow.com/questions/20208562/homepage-login-form-django",
    "body": "{% extends \"base.html\" %}\n{% block head %}\n  <title>Login</title>\n{% endblock %}\n{% block body %}\n  {% if form.errors %}\n    <p>Your username and password didn't match. Please try again.</p>\n  {% endif %}\n\n  <form method=\"post\" action=\"{% url 'django.contrib.auth.views.login' %}\">\n    {% csrf_token %}\n    <table>\n      <tr>\n        <td>{{ form.username.label_tag }}</td>\n        <td>{{ form.username }}</td>\n      </tr>\n      <tr>\n        <td>{{ form.password.label_tag }}</td>\n        <td>{{ form.password }}</td>\n      </tr>\n    </table>\n\n    <input type=\"submit\" value=\"login\" />\n    <input type=\"hidden\" name=\"next\" value=\"{{ next }}\" />\n  </form>\n{% endblock %}"
  },
  {
    "url": "https://stackoverflow.com/questions/60883696/k-fold-cross-validation-using-dataloaders-in-pytorch",
    "body": "# define a cross validation function\ndef crossvalid(model=None,criterion=None,optimizer=None,dataset=None,k_fold=5):\n\n    train_score = pd.Series()\n    val_score = pd.Series()\n\n    total_size = len(dataset)\n    fraction = 1/k_fold\n    seg = int(total_size * fraction)\n    # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset\n    # index: [trll,trlr],[vall,valr],[trrl,trrr]\n    for i in range(k_fold):\n        trll = 0\n        trlr = i * seg\n        vall = trlr\n        valr = i * seg + seg\n        trrl = valr\n        trrr = total_size\n        # msg\n#         print(\"train indices: [%d,%d),[%d,%d), test indices: [%d,%d)\"\n#               % (trll,trlr,trrl,trrr,vall,valr))\n\n        train_left_indices = list(range(trll,trlr))\n        train_right_indices = list(range(trrl,trrr))\n\n        train_indices = train_left_indices + train_right_indices\n        val_indices = list(range(vall,valr))\n\n        train_set = torch.utils.data.dataset.Subset(dataset,train_indices)\n        val_set = torch.utils.data.dataset.Subset(dataset,val_indices)\n\n#         print(len(train_set),len(val_set))\n#         print()\n\n        train_loader = torch.utils.data.DataLoader(train_set, batch_size=50,\n                                          shuffle=True, num_workers=4)\n        val_loader = torch.utils.data.DataLoader(val_set, batch_size=50,\n                                          shuffle=True, num_workers=4)\n        train_acc = train(res_model,criterion,optimizer,train_loader,epoch=1)\n        train_score.at[i] = train_acc\n        val_acc = valid(res_model,criterion,optimizer,val_loader)\n        val_score.at[i] = val_acc\n\n    return train_score,val_score\n\ntrain_score,val_score = crossvalid(res_model,criterion,optimizer,dataset=tiny_dataset)"
  },
  {
    "url": "https://stackoverflow.com/questions/58992252/how-to-enforce-dataclass-fields-types",
    "body": "import dataclasses\n@dataclasses.dataclass()\nclass Parent:\n    def __post_init__(self):\n        for (name, field_type) in self.__annotations__.items():\n            if not isinstance(self.__dict__[name], field_type):\n                current_type = type(self.__dict__[name])\n                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n        print(\"Check is passed successfully\")\n@dataclasses.dataclass()\nclass MyClass(Parent):\n    value: str\nobj1 = MyClass(value=\"1\")\nobj2 = MyClass(value=1)"
  },
  {
    "url": "https://stackoverflow.com/questions/26824019/are-sessions-needed-for-python-social-auth",
    "body": "/**\n * This function gets called after successfully getting the access_token from Facebook's API.\n */\nfunction successLoginFbFn(response) {\n    var deferred = $q.defer();\n    $http.post('/api/v1/auth/facebook/', {\n        \"access_token\": response.authResponse.accessToken,\n        \"backend\": \"facebook\"\n    }).success(function(response, status, headers, config) {\n        // Success\n        if (response.token) {\n            // Save the token to localStorage and redirect the user to the front-page.\n            Authentication.setToken(response.token);\n            window.location = '/';\n        }\n        deferred.resolve(response, status, headers, config);\n    }).error(function(response, status, headers, config) {\n        // Error\n        console.error('Authentication error.');\n        deferred.reject(response, status, headers, config);\n    });\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/52104682/rendering-a-pandas-dataframe-as-html-with-same-styling-as-jupyter-notebook",
    "body": "def getTableHTML(df):\n\n    \"\"\"\n    From https://stackoverflow.com/a/49687866/2007153\n\n    Get a Jupyter like html of pandas dataframe\n\n    \"\"\"\n    styles = [\n        #table properties\n        dict(selector=\" \",\n             props=[(\"margin\",\"0\"),\n                    (\"font-family\",'\"Helvetica\", \"Arial\", sans-serif'),\n                    (\"border-collapse\", \"collapse\"),\n                    (\"border\",\"none\"),\n    #                 (\"border\", \"2px solid #ccf\")\n                       ]),\n        #header color - optional\n    #     dict(selector=\"thead\",\n    #          props=[(\"background-color\",\"#cc8484\")\n    #                ]),\n        #background shading\n        dict(selector=\"tbody tr:nth-child(even)\",\n             props=[(\"background-color\", \"#fff\")]),\n        dict(selector=\"tbody tr:nth-child(odd)\",\n             props=[(\"background-color\", \"#eee\")]),\n        #cell spacing\n        dict(selector=\"td\",\n             props=[(\"padding\", \".5em\")]),\n        #header cell properties\n        dict(selector=\"th\",\n             props=[(\"font-size\", \"100%\"),\n                    (\"text-align\", \"center\")]),\n    ]\n    return (df.style.set_table_styles(styles)).render()"
  },
  {
    "url": "https://stackoverflow.com/questions/62413698/how-to-use-refresh-token-with-fastapi",
    "body": "from fastapi import FastAPI, Depends, HTTPException\nfrom fastapi_jwt_auth import AuthJWT\nfrom pydantic import BaseModel\napp = FastAPI()\nclass User(BaseModel):\n    email: str\n    password: str\nclass Settings(BaseModel):\n    authjwt_secret_key: str = \"secret\"\n@AuthJWT.load_config\ndef get_config():\n    return Settings()\n@app.post('/login')\ndef login(user: User, Authorize: AuthJWT = Depends()):\n    if user.email != \"test@test.com\" or user.password != \"test\":\n        raise HTTPException(status_code=401,detail=\"Incorrect email or password\")\n    access_token = Authorize.create_access_token(subject=user.email)\n    refresh_token = Authorize.create_refresh_token(subject=user.email)\n    return {\"access_token\": access_token, \"refresh_token\": refresh_token}"
  },
  {
    "url": "https://stackoverflow.com/questions/45882401/how-to-deal-with-userwarning-converting-sparse-indexedslices-to-a-dense-tensor",
    "body": "# Flatten batch elements to rank-2 tensor where 1st max_length rows belong to first batch element and so forth\nall_timesteps = tf.reshape(raw_output, [-1, n_dim])  # (batch_size*max_length, n_dim)\n# Indices to last element of each sequence.\n# Index to first element is the sequence order number times max sequence length.\n# Index to last element is the index to first element plus sequence length.\nrow_inds = tf.range(0, batch_size) * max_length + (seq_len - 1)\n# Creating a vector of 0s and 1s that will specify what timesteps to choose.\npartitions = tf.reduce_sum(tf.one_hot(row_inds, tf.shape(all_timesteps)[0], dtype='int32'), 0)\n# Selecting the elements we want to choose.\nlast_timesteps = tf.dynamic_partition(all_timesteps, partitions, 2)  # (batch_size, n_dim)\nlast_timesteps = last_timesteps[1]"
  },
  {
    "url": "https://stackoverflow.com/questions/56112506/pylint-protection-against-self-assignment",
    "body": "from pylint.checkers import BaseChecker\nfrom pylint.interfaces import IAstroidChecker\nclass SelfAssignChecker(BaseChecker):\n    __implements__ = IAstroidChecker\n    name = 'self-assign-returns'\n    priority = -1\n    msgs = {\n        'W5555': (\n            'Self assignment (%s).',\n            'self-assign',\n            'useless assignment.'\n        ),\n    }\n    def visit_assign(self, node):\n        names = []\n        for child in node.get_children():\n            if not hasattr(child, 'name'):\n                return\n            if child.name not in names:\n                names.append(child.name)\n            else:\n                self.add_message(\"self-assign\", node=node, args=child.name)\ndef register(linter):\n    linter.register_checker(SelfAssignChecker(linter))"
  },
  {
    "url": "https://stackoverflow.com/questions/63316840/django-3-1-streaminghttpresponse-with-an-async-generator",
    "body": "import asyncio\n# By design asyncio does not allow its event loop to be nested.\n# Trying to do so will give the error \"RuntimeError: This event loop is already running\".\n# This library solves that problem.\nimport nest_asyncio\nfrom django.http.response import StreamingHttpResponse\nclass AsyncStreamingHttpResponse(StreamingHttpResponse):\n    def __init__(self, streaming_content=(), *args, **kwargs):\n        sync_streaming_content = self.get_sync_iterator(streaming_content)\n        super().__init__(streaming_content=sync_streaming_content, *args, **kwargs)\n    @staticmethod\n    async def convert_async_iterable(stream):\n        \"\"\"Accepts async_generator and async_iterator\"\"\"\n        return iter([chunk async for chunk in stream])\n    def get_sync_iterator(self, async_iterable):\n        nest_asyncio.apply()\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        result = loop.run_until_complete(self.convert_async_iterable(async_iterable))\n        return result"
  },
  {
    "url": "https://stackoverflow.com/questions/55801796/how-can-we-create-data-columns-in-dash-table-dynamically-using-callback-with-a-f",
    "body": "html.Div(\n        id = 'tableDiv',\n        className = 'tableDiv'\n    )\n...\n  @app.callback([Output('tableDiv', 'children')]\n          [Input('submit', 'n_clicks')],\n          [State('ID', 'value'),  State('pattern_desc', 'value'),\n        State('file_path', 'value')])\n   def update_table(n_clicks, ID, pattern_desc, file_path):\n         df = someFunc(ID, pattern_desc, file_path)\n    mycolumns = [{'name': i, 'id': i} for i in df.columns]\n        return html.Div([\n                dt.DataTable(\n            id='table',\n            columns=mycolumns,\n            data=df.to_dict(\"rows\")\n         )\n        ])"
  },
  {
    "url": "https://stackoverflow.com/questions/3586046/fastest-way-to-take-a-screenshot-with-python-on-windows",
    "body": "import win32gui\nimport win32ui\nimport win32con\nw = 1920 # set this\nh = 1080 # set this\nbmpfilenamename = \"out.bmp\" #set this\nhwnd = win32gui.FindWindow(None, windowname)\nwDC = win32gui.GetWindowDC(hwnd)\ndcObj=win32ui.CreateDCFromHandle(wDC)\ncDC=dcObj.CreateCompatibleDC()\ndataBitMap = win32ui.CreateBitmap()\ndataBitMap.CreateCompatibleBitmap(dcObj, w, h)\ncDC.SelectObject(dataBitMap)\ncDC.BitBlt((0,0),(w, h) , dcObj, (0,0), win32con.SRCCOPY)\ndataBitMap.SaveBitmapFile(cDC, bmpfilenamename)\n# Free Resources\ndcObj.DeleteDC()\ncDC.DeleteDC()\nwin32gui.ReleaseDC(hwnd, wDC)\nwin32gui.DeleteObject(dataBitMap.GetHandle())"
  },
  {
    "url": "https://stackoverflow.com/questions/58321991/is-it-possible-to-change-pytests-assert-statement-behaviour-in-python",
    "body": "$ pytest test_foo.py -s --pdb --pdbcls=demo.custom_pdb:CustomPdb\n[ ... ]\n    def test_ham():\n>       assert 42 == 17\nE       assert 42 == 17\ntest_foo.py:2: AssertionError\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nSorry, not interested in this failure\nF\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n    def test_spam():\n>       int(\"Vikings\")\nE       ValueError: invalid literal for int() with base 10: 'Vikings'\ntest_foo.py:4: ValueError\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n> /.../test_foo.py(4)test_spam()\n-> int(\"Vikings\")\n(Pdb)"
  },
  {
    "url": "https://stackoverflow.com/questions/58321991/is-it-possible-to-change-pytests-assert-statement-behaviour-in-python",
    "body": "import pytest\n@pytest.hookimpl(trylast=True)\ndef pytest_configure(config):\n    # unregister returns the unregistered plugin\n    pdbinvoke = config.pluginmanager.unregister(name=\"pdbinvoke\")\n    if pdbinvoke is None:\n        # no --pdb switch used, no debugging requested\n        return\n    # get the terminalreporter too, to write to the console\n    tr = config.pluginmanager.getplugin(\"terminalreporter\")\n    # create or own plugin\n    plugin = ExceptionFilter(pdbinvoke, tr)\n    # register our plugin, pytest will then start calling our plugin hooks\n    config.pluginmanager.register(plugin, \"exception_filter\")\nclass ExceptionFilter:\n    def __init__(self, pdbinvoke, terminalreporter):\n        # provide the same functionality as pdbinvoke\n        self.pytest_internalerror = pdbinvoke.pytest_internalerror\n        self.orig_exception_interact = pdbinvoke.pytest_exception_interact\n        self.tr = terminalreporter\n    def pytest_exception_interact(self, node, call, report):\n        if not call.excinfo. errisinstance(ValueError):\n            self.tr.write_line(\"Sorry, not interested!\")\n            return\n        return self.orig_exception_interact(node, call, report)"
  },
  {
    "url": "https://stackoverflow.com/questions/58321991/is-it-possible-to-change-pytests-assert-statement-behaviour-in-python",
    "body": "$ pytest demo/test_foo.py --pdb --pdbcls=IPython.core.debugger:Pdb\n[ ... ]\ndemo/test_foo.py F\nSorry, not interested!\ndemo/test_foo.py F\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n    def test_spam():\n>       int(\"Vikings\")\nE       ValueError: invalid literal for int() with base 10: 'Vikings'\ndemo/test_foo.py:4: ValueError\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n> /.../demo/test_foo.py(4)test_spam()\n      1 def test_ham():\n      2     assert 42 == 17\n      3 def test_spam():\n----> 4     int(\"Vikings\")\nipdb>"
  },
  {
    "url": "https://stackoverflow.com/questions/58321991/is-it-possible-to-change-pytests-assert-statement-behaviour-in-python",
    "body": "$ pytest -r a demo/test_foo.py\n============================= test session starts =============================\nplatform darwin -- Python 3.8.0, pytest-3.10.0, py-1.7.0, pluggy-0.8.0\nrootdir: ..., inifile:\ncollected 2 items\ndemo/test_foo.py sF                                                      [100%]\n=================================== FAILURES ===================================\n__________________________________ test_spam ___________________________________\n    def test_spam():\n>       int(\"Vikings\")\nE       ValueError: invalid literal for int() with base 10: 'Vikings'\ndemo/test_foo.py:4: ValueError\n=========================== short test summary info ============================\nFAIL demo/test_foo.py::test_spam\nSKIP [1] .../demo/conftest.py:12: [NOTRUN] ignoring everything but ValueError\n===================== 1 failed, 1 skipped in 0.07 seconds ======================"
  },
  {
    "url": "https://stackoverflow.com/questions/2318288/how-to-use-custom-png-image-marker-with-plot",
    "body": "import matplotlib.pyplot as plt\nfrom matplotlib import image\n# constant\ndpi = 72\npath = 'smile.png'\n# read in our png file\nim = image.imread(path)\nimage_size = im.shape[1], im.shape[0]\nfig = plt.figure(dpi=dpi)\nax = fig.add_subplot(111)\n# plot our line with transparent markers, and markersize the size of our image\nline, = ax.plot((1,2,3,4),(1,2,3,4),\"bo\",mfc=\"None\",mec=\"None\",markersize=image_size[0] * (dpi/ 96))\n# we need to make the frame transparent so the image can be seen\n# only in trunk can you put the image on top of the plot, see this link:\n# http://www.mail-archive.com/matplotlib-users@lists.sourceforge.net/msg14534.html\nax.patch.set_alpha(0)\nax.set_xlim((0,5))\nax.set_ylim((0,5))\n# translate point positions to pixel positions\n# figimage needs pixels not points\nline._transform_path()\npath, affine = line._transformed_path.get_transformed_points_and_affine()\npath = affine.transform_path(path)\nfor pixelPoint in path.vertices:\n    # place image at point, centering it\n    fig.figimage(im,pixelPoint[0]-image_size[0]/2,pixelPoint[1]-image_size[1]/2,origin=\"upper\")\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/69954587/no-blas-lapack-libraries-found-when-installing-scipy",
    "body": "# Install lapack and openblas from Homebrew\nbrew install openblas lapack\n# Tell Numpy installer where to find lapack\nexport LDFLAGS=\"-L/usr/local/opt/lapack/lib\"\nexport CPPFLAGS=\"-I/usr/local/opt/lapack/include\"\nexport PKG_CONFIG_PATH=\"/usr/local/opt/lapack/lib/pkgconfig\"\n# See https://github.com/scipy/scipy/issues/12935\nexport CFLAGS=-Wno-error=implicit-function-declaration\n# The location may vary - use find command to find this on your local /usr/local/opt\nexport LAPACK=/usr/local/opt/lapack/lib/liblapack.dylib\nexport BLAS=/usr/local/opt/openblas/lib/libopenblasp-r0.3.19.dylib"
  },
  {
    "url": "https://stackoverflow.com/questions/38839402/how-to-use-assert-frame-equal-in-unittest",
    "body": "import unittest\nimport pandas as pd\nimport pandas.testing as pd_testing\nclass TestSplitWeight(unittest.TestCase):\n    def assertDataframeEqual(self, a, b, msg):\n        try:\n            pd_testing.assert_frame_equal(a, b)\n        except AssertionError as e:\n            raise self.failureException(msg) from e\n    def setUp(self):\n        self.addTypeEqualityFunc(pd.DataFrame, self.assertDataframeEqual)\n    def test_allZero(self):\n        self.assertEqual(pd.DataFrame([0,0,0,0]), pd.DataFrame([0,0,0,0]))"
  },
  {
    "url": "https://stackoverflow.com/questions/55525195/do-i-have-to-do-one-hot-encoding-separately-for-train-and-test-dataset",
    "body": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n### Correct\ntrain = pd.DataFrame(['A', 'B', 'A', 'C'])\ntest = pd.DataFrame(['B', 'A', 'D'])\nenc = OneHotEncoder(handle_unknown = 'ignore')\nenc.fit(train)\nenc.transform(train).toarray()\n#array([[1., 0., 0.],\n#       [0., 1., 0.],\n#       [1., 0., 0.],\n#       [0., 0., 1.]])\nenc.transform(test).toarray()\n#array([[0., 1., 0.],\n#       [1., 0., 0.],\n#       [0., 0., 0.]])\n### Incorrect\nfull = pd.concat((train, test))\nenc = OneHotEncoder(handle_unknown = 'ignore')\nenc.fit(full)\nenc.transform(train).toarray()\n#array([[1., 0., 0., 0.],\n#       [0., 1., 0., 0.],\n#       [1., 0., 0., 0.],\n#       [0., 0., 1., 0.]])\nenc.transform(test).toarray()\n#array([[0., 1., 0., 0.],\n#       [1., 0., 0., 0.],\n#       [0., 0., 0., 1.]])"
  },
  {
    "url": "https://stackoverflow.com/questions/20847727/python-inheritance-versus-composition",
    "body": "class Person:\n    def __init__(self, firstname, lastname):\n        self.firstname = firstname\n        self.lastname = lastname\n    def get_name(self):\n        return f\"{self.firstname} {self.lastname}\"\nclass Parent(Person):\n    def __init__(self, firstname, lastname):\n        super().__init__(firstname, lastname)\n        self.kids = []\n    def havechild(self, firstname):\n        print(self.firstname, \"is having a child\")\n        self.kids.append(Child(self, firstname))\nclass Child(Person):\n    def __init__(self, parent, firstname):\n        super().__init__(firstname, parent.lastname)\n        self.parent = parent"
  },
  {
    "url": "https://stackoverflow.com/questions/20847727/python-inheritance-versus-composition",
    "body": "from collections import defaultdict\nclass Person:\n    def __init__(self, firstname, lastname):\n        self.firstname = firstname\n        self.lastname = lastname\n    def get_name(self):\n        return f\"{self.firstname} {self.lastname}\"\nclass FamilyRegistry(object):\n    def __init__(self):\n        self.kids = defaultdict(list)\n    def register_birth(self, parent, child_name):\n        print(parent.firstname, \"is having a child\")\n        child = Person(child_name, parent.lastname)\n        self.kids[parent.lastname].append(child)\n        return child\n    def print_children(self, person):\n        children = self.kids[person.lastname]\n        if len(children) == 0:\n            print(\"{} has no children\" % person.get_name())\n            return\n        for child in children:\n            print(child.get_name())"
  },
  {
    "url": "https://stackoverflow.com/questions/24063788/python3-singledispatch-in-class-how-to-dispatch-self-type",
    "body": "from functools import singledispatch, update_wrapper\n# Python 3.8 singledispatchmethod, backported\nclass singledispatchmethod:\n    \"\"\"Single-dispatch generic method descriptor.\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    \"\"\"\n    def __init__(self, func):\n        if not callable(func) and not hasattr(func, \"__get__\"):\n            raise TypeError(f\"{func!r} is not callable or a descriptor\")\n        self.dispatcher = singledispatch(func)\n        self.func = func\n    def register(self, cls, method=None):\n        \"\"\"generic_method.register(cls, func) -> func\n        Registers a new implementation for the given *cls* on a *generic_method*.\n        \"\"\"\n        return self.dispatcher.register(cls, func=method)\n    def __get__(self, obj, cls):\n        def _method(*args, **kwargs):\n            method = self.dispatcher.dispatch(args[0].__class__)\n            return method.__get__(obj, cls)(*args, **kwargs)\n        _method.__isabstractmethod__ = self.__isabstractmethod__\n        _method.register = self.register\n        update_wrapper(_method, self.func)\n        return _method\n    @property\n    def __isabstractmethod__(self):\n        return getattr(self.func, '__isabstractmethod__', False)"
  },
  {
    "url": "https://stackoverflow.com/questions/68893521/simple-example-of-pandas-extensionarray",
    "body": "from __future__ import annotations\nimport operator\nimport re\nfrom typing import Any, Sequence\nimport numpy as np\nimport pandas as pd\n@pd.api.extensions.register_extension_dtype\nclass AngleDtype(pd.core.dtypes.dtypes.PandasExtensionDtype):\n    \"\"\"\n    An ExtensionDtype for unit-aware angular data.\n    \"\"\"\n    # Required for all parameterized dtypes\n    _metadata = ('unit',)\n    _match = re.compile(r'(A|a)ngle\\[(?P<unit>.+)\\]')\n    def __init__(self, unit=None):\n        if unit is None:\n            unit = 'rad'\n        if unit not in ['rad', 'deg']:\n            msg = f\"'{type(self).__name__}' only supports 'rad' and 'deg' units\"\n            raise ValueError(msg)\n        self._unit = unit\n    def __str__(self) -> str:\n        return f'angle[{self.unit}]'\n    # TestDtypeTests\n    def __hash__(self) -> int:\n        return hash(str(self))\n    # TestDtypeTests\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, str):\n            return self.name == other\n        else:\n            return isinstance(other, type(self)) and self.unit == other.unit\n    # Required for pickle compat (see GH26067)\n    def __setstate__(self, state) -> None:\n        self._unit = state['unit']\n    # Required for all ExtensionDtype subclasses\n    @classmethod\n    def construct_array_type(cls):\n        \"\"\"\n        Return the array type associated with this dtype.\n        \"\"\"\n        return AngleArray\n    # Recommended for parameterized dtypes\n    @classmethod\n    def construct_from_string(cls, string: str) -> AngleDtype:\n        \"\"\"\n        Construct an AngleDtype from a string.\n        Example\n        -------\n        >>> AngleDtype.construct_from_string('angle[deg]')\n        angle['deg']\n        \"\"\"\n        if not isinstance(string, str):\n            msg = f\"'construct_from_string' expects a string, got {type(string)}\"\n            raise TypeError(msg)\n        msg = f\"Cannot construct a '{cls.__name__}' from '{string}'\"\n        match = cls._match.match(string)\n        if match:\n            d = match.groupdict()\n            try:\n                return cls(unit=d['unit'])\n            except (KeyError, TypeError, ValueError) as err:\n                raise TypeError(msg) from err\n        else:\n            raise TypeError(msg)\n    # Required for all ExtensionDtype subclasses\n    @property\n    def type(self):\n        \"\"\"\n        The scalar type for the array (e.g., int).\n        \"\"\"\n        return np.generic\n    # Required for all ExtensionDtype subclasses\n    @property\n    def name(self) -> str:\n        \"\"\"\n        A string representation of the dtype.\n        \"\"\"\n        return str(self)\n    @property\n    def unit(self) -> str:\n        \"\"\"\n        The angle unit.\n        \"\"\"\n        return self._unit"
  },
  {
    "url": "https://stackoverflow.com/questions/68893521/simple-example-of-pandas-extensionarray",
    "body": "class AngleArray(pd.api.extensions.ExtensionArray):\n    \"\"\"\n    An ExtensionArray for unit-aware angular data.\n    \"\"\"\n    # Include `copy` param for TestInterfaceTests\n    def __init__(self, data, unit='rad', copy: bool=False):\n        self._data = np.array(data, copy=copy)\n        self._unit = unit\n    # Required for all ExtensionArray subclasses\n    def __getitem__(self, index: int) -> AngleArray | Any:\n        \"\"\"\n        Select a subset of self.\n        \"\"\"\n        if isinstance(index, int):\n            return self._data[index]\n        else:\n            # Check index for TestGetitemTests\n            index = pd.core.indexers.check_array_indexer(self, index)\n            return type(self)(self._data[index])\n    # TestSetitemTests\n    def __setitem__(self, index: int, value: np.generic) -> None:\n        \"\"\"\n        Set one or more values in-place.\n        \"\"\"\n        # Check index for TestSetitemTests\n        index = pd.core.indexers.check_array_indexer(self, index)\n        # Upcast to value's type (if needed) for TestMethodsTests\n        if self._data.dtype < type(value):\n            self._data = self._data.astype(type(value))\n        # TODO: Validate value for TestSetitemTests\n        # value = self._validate_setitem_value(value)\n        self._data[index] = value\n    # Required for all ExtensionArray subclasses\n    def __len__(self) -> int:\n        \"\"\"\n        Length of this array.\n        \"\"\"\n        return len(self._data)\n    # TestUnaryOpsTests\n    def __invert__(self) -> AngleArray:\n        \"\"\"\n        Element-wise inverse of this array.\n        \"\"\"\n        data = ~self._data\n        return type(self)(data, unit=self.dtype.unit)\n    def _ensure_same_units(self, other) -> AngleArray:\n        \"\"\"\n        Helper method to ensure `self` and `other` have the same units.\n        \"\"\"\n        if isinstance(other, type(self)) and self.dtype.unit != other.dtype.unit:\n            return other.asunit(self.dtype.unit)\n        else:\n            return other\n    def _apply_operator(self, op, other, recast=False) -> np.ndarray | AngleArray:\n        \"\"\"\n        Helper method to apply an operator `op` between `self` and `other`.\n        Some ops require the result to be recast into AngleArray:\n        * Comparison ops: recast=False\n        * Arithmetic ops: recast=True\n        \"\"\"\n        f = operator.attrgetter(op)\n        data, other = np.array(self), np.array(self._ensure_same_units(other))\n        result = f(data)(other)\n        return result if not recast else type(self)(result, unit=self.dtype.unit)\n    def _apply_operator_if_not_series(self, op, other, recast=False) -> np.ndarray | AngleArray:\n        \"\"\"\n        Wraps _apply_operator only if `other` is not Series/DataFrame.\n\n        Some ops should return NotImplemented if `other` is a Series/DataFrame:\n        https://github.com/pandas-dev/pandas/blob/e7e7b40722e421ef7e519c645d851452c70a7b7c/pandas/tests/extension/base/ops.py#L115\n        \"\"\"\n        if isinstance(other, (pd.Series, pd.DataFrame)):\n            return NotImplemented\n        else:\n            return self._apply_operator(op, other, recast=recast)\n    # Required for all ExtensionArray subclasses\n    @pd.core.ops.unpack_zerodim_and_defer('__eq__')\n    def __eq__(self, other):\n        return self._apply_operator('__eq__', other, recast=False)\n    # TestComparisonOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__ne__')\n    def __ne__(self, other):\n        return self._apply_operator('__ne__', other, recast=False)\n    # TestComparisonOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__lt__')\n    def __lt__(self, other):\n        return self._apply_operator('__lt__', other, recast=False)\n    # TestComparisonOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__gt__')\n    def __gt__(self, other):\n        return self._apply_operator('__gt__', other, recast=False)\n    # TestComparisonOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__le__')\n    def __le__(self, other):\n        return self._apply_operator('__le__', other, recast=False)\n    # TestComparisonOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__ge__')\n    def __ge__(self, other):\n        return self._apply_operator('__ge__', other, recast=False)\n\n    # TestArithmeticOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__add__')\n    def __add__(self, other) -> AngleArray:\n        return self._apply_operator_if_not_series('__add__', other, recast=True)\n    # TestArithmeticOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__sub__')\n    def __sub__(self, other) -> AngleArray:\n        return self._apply_operator_if_not_series('__sub__', other, recast=True)\n    # TestArithmeticOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__mul__')\n    def __mul__(self, other) -> AngleArray:\n        return self._apply_operator_if_not_series('__mul__', other, recast=True)\n    # TestArithmeticOpsTests\n    @pd.core.ops.unpack_zerodim_and_defer('__truediv__')\n    def __truediv__(self, other) -> AngleArray:\n        return self._apply_operator_if_not_series('__truediv__', other, recast=True)\n    # Required for all ExtensionArray subclasses\n    @classmethod\n    def _from_sequence(cls, data, dtype=None, copy: bool=False):\n        \"\"\"\n        Construct a new AngleArray from a sequence of scalars.\n        \"\"\"\n        if dtype is None:\n            dtype = AngleDtype()\n        if not isinstance(dtype, AngleDtype):\n            msg = f\"'{cls.__name__}' only supports 'AngleDtype' dtype\"\n            raise ValueError(msg)\n        else:\n            return cls(data, unit=dtype.unit, copy=copy)\n    # TestParsingTests\n    @classmethod\n    def _from_sequence_of_strings(cls, strings, *, dtype=None, copy: bool=False) -> AngleArray:\n        \"\"\"\n        Construct a new AngleArray from a sequence of strings.\n        \"\"\"\n        scalars = pd.to_numeric(strings, errors='raise')\n        return cls._from_sequence(scalars, dtype=dtype, copy=copy)\n    # Required for all ExtensionArray subclasses\n    @classmethod\n    def _from_factorized(cls, uniques: np.ndarray, original: AngleArray):\n        \"\"\"\n        Reconstruct an AngleArray after factorization.\n        \"\"\"\n        return cls(uniques, unit=original.dtype.unit)\n    # Required for all ExtensionArray subclasses\n    @classmethod\n    def _concat_same_type(cls, to_concat: Sequence[AngleArray]) -> AngleArray:\n        \"\"\"\n        Concatenate multiple AngleArrays.\n        \"\"\"\n        # ensure same units\n        counts = pd.value_counts([array.dtype.unit for array in to_concat])\n        unit = counts.index[0]\n        if counts.size > 1:\n            to_concat = [a.asunit(unit) for a in to_concat]\n        return cls(np.concatenate(to_concat), unit=unit)\n    # Required for all ExtensionArray subclasses\n    @property\n    def dtype(self):\n        \"\"\"\n        An instance of AngleDtype.\n        \"\"\"\n        return AngleDtype(self._unit)\n    # Required for all ExtensionArray subclasses\n    @property\n    def nbytes(self) -> int:\n        \"\"\"\n        The number of bytes needed to store this object in memory.\n        \"\"\"\n        return self._data.nbytes\n    @property\n    def unit(self):\n        return self.dtype.unit\n    # Test*ReduceTests\n    def all(self) -> bool:\n        return all(self)\n    def any(self) -> bool:  # Test*ReduceTests\n        return any(self)\n    def sum(self) -> np.generic:  # Test*ReduceTests\n        return self._data.sum()\n    def mean(self) -> np.generic:  # Test*ReduceTests\n        return self._data.mean()\n    def max(self) -> np.generic:  # Test*ReduceTests\n        return self._data.max()\n    def min(self) -> np.generic:  # Test*ReduceTests\n        return self._data.min()\n    def prod(self) -> np.generic:  # Test*ReduceTests\n        return self._data.prod()\n    def std(self) -> np.generic:  # Test*ReduceTests\n        return pd.Series(self._data).std()\n    def var(self) -> np.generic:  # Test*ReduceTests\n        return pd.Series(self._data).var()\n    def median(self) -> np.generic:  # Test*ReduceTests\n        return np.median(self._data)\n    def skew(self) -> np.generic:  # Test*ReduceTests\n        return pd.Series(self._data).skew()\n    def kurt(self) -> np.generic:  # Test*ReduceTests\n        return pd.Series(self._data).kurt()\n    # Test*ReduceTests\n    def _reduce(self, name: str, *, skipna: bool=True, **kwargs):\n        \"\"\"\n        Return a scalar result of performing the reduction operation.\n        \"\"\"\n        f = operator.attrgetter(name)\n        return f(self)()\n    # Required for all ExtensionArray subclasses\n    def isna(self):\n        \"\"\"\n        A 1-D array indicating if each value is missing.\n        \"\"\"\n        return pd.isnull(self._data)\n    # Required for all ExtensionArray subclasses\n    def copy(self):\n        \"\"\"\n        Return a copy of the array.\n        \"\"\"\n        copied = self._data.copy()\n        return type(self)(copied, unit=self.unit)\n    # Required for all ExtensionArray subclasses\n    def take(self, indices, allow_fill=False, fill_value=None):\n        \"\"\"\n        Take elements from an array.\n        \"\"\"\n        if allow_fill and fill_value is None:\n            fill_value = self.dtype.na_value\n        result = pd.core.algorithms.take(self._data, indices, allow_fill=allow_fill,\n                                         fill_value=fill_value)\n        return self._from_sequence(result)\n    # TestMethodsTests\n    def value_counts(self, dropna: bool=True):\n        \"\"\"\n        Return a Series containing descending counts of unique values (excludes NA values by default).\n        \"\"\"\n        return pd.core.algorithms.value_counts(self._data, dropna=dropna)\n    def asunit(self, unit: str) -> AngleArray:\n        \"\"\"\n        Cast to an AngleDtype unit.\n        \"\"\"\n        if unit not in ['rad', 'deg']:\n            msg = f\"'{type(self.dtype).__name__}' only supports 'rad' and 'deg' units\"\n            raise ValueError(msg)\n        elif self.dtype.unit == unit:\n            return self\n        else:\n            rad2deg = self.dtype.unit == 'rad' and unit == 'deg'\n            data = np.rad2deg(self._data) if rad2deg else np.deg2rad(self._data)\n            return type(self)(data, unit)"
  },
  {
    "url": "https://stackoverflow.com/questions/68893521/simple-example-of-pandas-extensionarray",
    "body": "import operator\nimport numpy as np\nfrom pandas import Series\nimport pytest\nfrom pandas.tests.extension.base.casting import BaseCastingTests  # noqa\nfrom pandas.tests.extension.base.constructors import BaseConstructorsTests  # noqa\nfrom pandas.tests.extension.base.dtype import BaseDtypeTests  # noqa\nfrom pandas.tests.extension.base.getitem import BaseGetitemTests  # noqa\nfrom pandas.tests.extension.base.groupby import BaseGroupbyTests  # noqa\nfrom pandas.tests.extension.base.interface import BaseInterfaceTests  # noqa\nfrom pandas.tests.extension.base.io import BaseParsingTests  # noqa\nfrom pandas.tests.extension.base.methods import BaseMethodsTests  # noqa\nfrom pandas.tests.extension.base.missing import BaseMissingTests  # noqa\nfrom pandas.tests.extension.base.ops import (  # noqa\n    BaseArithmeticOpsTests,\n    BaseComparisonOpsTests,\n    BaseOpsUtil,\n    BaseUnaryOpsTests,\n)\nfrom pandas.tests.extension.base.printing import BasePrintingTests  # noqa\nfrom pandas.tests.extension.base.reduce import (  # noqa\n    BaseBooleanReduceTests,\n    BaseNoReduceTests,\n    BaseNumericReduceTests,\n)\nfrom pandas.tests.extension.base.reshaping import BaseReshapingTests  # noqa\nfrom pandas.tests.extension.base.setitem import BaseSetitemTests  # noqa\nfrom extension import AngleDtype, AngleArray\n@pytest.fixture\ndef dtype():\n    \"\"\"\n    A fixture providing the ExtensionDtype to validate.\n    \"\"\"\n    return AngleDtype()\n@pytest.fixture\ndef data():\n    \"\"\"\n    Length-100 array for this type.\n    * data[0] and data[1] should both be non missing\n    * data[0] and data[1] should not be equal\n    \"\"\"\n    return AngleArray(np.arange(100))\n@pytest.fixture\ndef data_for_twos():\n    \"\"\"\n    Length-100 array in which all the elements are two.\n    \"\"\"\n    return AngleArray(np.array([2] * 100))\n@pytest.fixture\ndef data_missing():\n    \"\"\"\n    Length-2 array with [NA, Valid].\n    \"\"\"\n    return AngleArray(np.array([np.nan, 2]))\n@pytest.fixture(params=['data', 'data_missing'])\ndef all_data(request, data, data_missing):\n    \"\"\"\n    Parameterized fixture giving 'data' and 'data_missing'.\n    \"\"\"\n    if request.param == 'data':\n        return data\n    elif request.param == 'data_missing':\n        return data_missing\n@pytest.fixture\ndef data_repeated(data):\n    \"\"\"\n    Generate many datasets.\n    Parameters\n    ----------\n    data : fixture implementing `data`\n    Returns\n    -------\n    Callable[[int], Generator]:\n        A callable that takes a `count` argument and\n        returns a generator yielding `count` datasets.\n    \"\"\"\n    def gen(count):\n        for _ in range(count):\n            yield data\n    return gen\n@pytest.fixture\ndef data_for_sorting():\n    \"\"\"\n    Length-3 array with a known sort order.\n    This should be three items [B, C, A] with A < B < C.\n    \"\"\"\n    return AngleArray(np.array([2, 3, 1]))\n@pytest.fixture\ndef data_missing_for_sorting():\n    \"\"\"\n    Length-3 array with a known sort order.\n    This should be three items [B, NA, A] with A < B and NA missing.\n    \"\"\"\n    return AngleArray(np.array([2, np.nan, 1]))\n@pytest.fixture\ndef na_cmp():\n    \"\"\"\n    Binary operator for comparing NA values.\n    Should return a function of two arguments that returns\n    True if both arguments are (scalar) NA for your type.\n    By default, uses ``operator.is_``.\n    \"\"\"\n    return lambda a, b: np.array_equal(a, b, equal_nan=True)\n@pytest.fixture\ndef na_value():\n    \"\"\"\n    The scalar missing value for this type. Default 'None'.\n    \"\"\"\n    return np.nan\n@pytest.fixture\ndef data_for_grouping():\n    \"\"\"\n    Data for factorization, grouping, and unique tests.\n    Expected to be like [B, B, NA, NA, A, A, B, C] where A < B < C and NA is missing.\n    \"\"\"\n    return AngleArray(np.array([2, 2, np.nan, np.nan, 1, 1, 2, 3]))\n@pytest.fixture(params=[True, False])\ndef box_in_series(request):\n    \"\"\"\n    Whether to box the data in a Series.\n    \"\"\"\n    return request.param\n@pytest.fixture(\n    params=[\n        lambda x: 1,\n        lambda x: [1] * len(x),\n        lambda x: Series([1] * len(x)),\n        lambda x: x,\n    ],\n    ids=['scalar', 'list', 'series', 'object'],\n)\ndef groupby_apply_operator(request):\n    \"\"\"\n    Functions to test groupby.apply().\n    \"\"\"\n    return request.param\n@pytest.fixture(params=[True, False])\ndef as_frame(request):\n    \"\"\"\n    Boolean fixture to support Series and Series.to_frame() comparison testing.\n    \"\"\"\n    return request.param\n@pytest.fixture(params=[True, False])\ndef as_series(request):\n    \"\"\"\n    Boolean fixture to support arr and Series(arr) comparison testing.\n    \"\"\"\n    return request.param\n@pytest.fixture(params=[True, False])\ndef use_numpy(request):\n    \"\"\"\n    Boolean fixture to support comparison testing of ExtensionDtype array    and numpy array.\n    \"\"\"\n    return request.param\n@pytest.fixture(params=['ffill', 'bfill'])\ndef fillna_method(request):\n    \"\"\"\n    Parameterized fixture giving method parameters 'ffill' and 'bfill' for\n    Series.fillna(method=<method>) testing.\n    \"\"\"\n    return request.param\n@pytest.fixture(params=[True, False])\ndef as_array(request):\n    \"\"\"\n    Boolean fixture to support ExtensionDtype _from_sequence method testing.\n    \"\"\"\n    return request.param\n@pytest.fixture(params=[None, lambda x: x])\ndef sort_by_key(request):\n    \"\"\"\n    Simple fixture for testing keys in sorting methods.\n    Tests None (no key) and the identity key.\n    \"\"\"\n    return request.param\n# TODO: Finish implementing all operators\n_all_arithmetic_operators = [\n    '__add__',\n    #  '__radd__',\n    '__sub__',\n    #  '__rsub__',\n    '__mul__',\n    #  '__rmul__',\n    #  '__floordiv__',\n    #  '__rfloordiv__',\n    '__truediv__',\n    #  '__rtruediv__',\n    #  '__pow__',\n    #  '__rpow__',\n    #  '__mod__',\n    #  '__rmod__',\n]\n@pytest.fixture(params=_all_arithmetic_operators)\ndef all_arithmetic_operators(request):\n    \"\"\"\n    Fixture for dunder names for common arithmetic operations.\n    \"\"\"\n    return request.param\n_all_numeric_reductions = [\n    'sum',\n    'max',\n    'min',\n    'mean',\n    'prod',\n    'std',\n    'var',\n    'median',\n    'kurt',\n    'skew',\n]\n@pytest.fixture(params=_all_numeric_reductions)\ndef all_numeric_reductions(request):\n    \"\"\"\n    Fixture for numeric reduction names.\n    \"\"\"\n    return request.param\n_all_boolean_reductions = ['all', 'any']\n@pytest.fixture(params=_all_boolean_reductions)\ndef all_boolean_reductions(request):\n    \"\"\"\n    Fixture for boolean reduction names.\n    \"\"\"\n    return request.param\n_all_reductions = _all_numeric_reductions + _all_boolean_reductions\n@pytest.fixture(params=_all_reductions)\ndef all_reductions(request):\n    \"\"\"\n    Fixture for all (boolean + numeric) reduction names.\n    \"\"\"\n    return request.param\n_all_compare_operators = [\n    '__eq__',\n    '__ne__',\n    '__le__',\n    '__lt__',\n    '__ge__',\n    '__gt__',\n]\n@pytest.fixture(params=_all_compare_operators)\ndef all_compare_operators(request):\n    \"\"\"\n    Fixture for dunder names for common compare operations:\n    * >=\n    * >\n    * ==\n    * !=\n    * <\n    * <=\n    \"\"\"\n    return request.param\nclass TestCastingTests(BaseCastingTests):\n    pass\nclass TestConstructorsTests(BaseConstructorsTests):\n    pass\nclass TestDtypeTests(BaseDtypeTests):\n    pass\nclass TestGetitemTests(BaseGetitemTests):\n    pass\nclass TestGroupbyTests(BaseGroupbyTests):\n    pass\nclass TestInterfaceTests(BaseInterfaceTests):\n    pass\nclass TestParsingTests(BaseParsingTests):\n    pass\nclass TestMethodsTests(BaseMethodsTests):\n    pass\nclass TestMissingTests(BaseMissingTests):\n    pass\nclass TestArithmeticOpsTests(BaseArithmeticOpsTests):\n    series_scalar_exc = None\n    frame_scalar_exc = None\n    series_array_exc = None\n    divmod_exc = TypeError  # TODO: Implement divmod\nclass TestComparisonOpsTests(BaseComparisonOpsTests):\n    # See pint-pandas test suite\n    def _compare_other(self, s, data, op_name, other):\n        op = self.get_op_from_name(op_name)\n        result = op(s, other)\n        expected = op(s.to_numpy(), other)\n        assert (result == expected).all()\nclass TestOpsUtil(BaseOpsUtil):\n    pass\nclass TestUnaryOpsTests(BaseUnaryOpsTests):\n    pass\nclass TestPrintingTests(BasePrintingTests):\n    pass\nclass TestBooleanReduceTests(BaseBooleanReduceTests):\n    pass\nclass TestNumericReduceTests(BaseNumericReduceTests):\n    pass\n# AFAICT NoReduce and Boolean+NumericReduce are mutually exclusive\n# class TestNoReduceTests(BaseNoReduceTests):\n    # pass\nclass TestReshapingTests(BaseReshapingTests):\n    pass\nclass TestSetitemTests(BaseSetitemTests):\n    pass"
  },
  {
    "url": "https://stackoverflow.com/questions/64303607/python-asyncio-how-to-read-stdin-and-write-to-stdout",
    "body": "`\nimport asyncio\nimport sys\nasync def connect_stdin_stdout():\n    loop = asyncio.get_event_loop()\n    reader = asyncio.StreamReader()\n    protocol = asyncio.StreamReaderProtocol(reader)\n    await loop.connect_read_pipe(lambda: protocol, sys.stdin)\n    w_transport, w_protocol = await loop.connect_write_pipe(asyncio.streams.FlowControlMixin, sys.stdout)\n    writer = asyncio.StreamWriter(w_transport, w_protocol, reader, loop)\n    return reader, writer\nasync def main():\n    reader, writer = await connect_stdin_stdout()\n    while True:\n        res = await reader.read(100)\n        if not res:\n            break\n        writer.write(res)\n        await writer.drain()\nif __name__ == \"__main__\":\n    asyncio.run(main())"
  },
  {
    "url": "https://stackoverflow.com/questions/39788591/python-simplehttpserver-to-receive-files",
    "body": "#!/usr/env python3\nimport http.server\nimport socketserver\nimport io\nimport cgi\n# Change this to serve on a different port\nPORT = 44444\nclass CustomHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):\n    def do_POST(self):\n        r, info = self.deal_post_data()\n        print(r, info, \"by: \", self.client_address)\n        f = io.BytesIO()\n        if r:\n            f.write(b\"Success\\n\")\n        else:\n            f.write(b\"Failed\\n\")\n        length = f.tell()\n        f.seek(0)\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/plain\")\n        self.send_header(\"Content-Length\", str(length))\n        self.end_headers()\n        if f:\n            self.copyfile(f, self.wfile)\n            f.close()\n    def deal_post_data(self):\n        ctype, pdict = cgi.parse_header(self.headers['Content-Type'])\n        pdict['boundary'] = bytes(pdict['boundary'], \"utf-8\")\n        pdict['CONTENT-LENGTH'] = int(self.headers['Content-Length'])\n        if ctype == 'multipart/form-data':\n            form = cgi.FieldStorage( fp=self.rfile, headers=self.headers, environ={'REQUEST_METHOD':'POST', 'CONTENT_TYPE':self.headers['Content-Type'], })\n            print (type(form))\n            try:\n                if isinstance(form[\"file\"], list):\n                    for record in form[\"file\"]:\n                        open(\"./%s\"%record.filename, \"wb\").write(record.file.read())\n                else:\n                    open(\"./%s\"%form[\"file\"].filename, \"wb\").write(form[\"file\"].file.read())\n            except IOError:\n                    return (False, \"Can't create file to write, do you have permission to write?\")\n        return (True, \"Files uploaded\")\nHandler = CustomHTTPRequestHandler\nwith socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n    print(\"serving at port\", PORT)\n    httpd.serve_forever()"
  },
  {
    "url": "https://stackoverflow.com/questions/55776571/how-to-split-a-date-column-into-separate-day-month-year-column-in-pandas",
    "body": "df['day'] = df.index.day\ndf['month'] = df.index.month\ndf['year'] = df.index.year\nprint(df)\n                 Dewptm  Fog   Humidity    Pressurem      Tempm     Wspdm  \\\ndatetime_utc\n1996-11-01    11.666667  0.0  52.916667 -2659.666667  22.333333  2.466667\n1996-11-02    10.458333  0.0  48.625000  1009.833333  22.916667  8.028571\n1996-11-03    12.041667  0.0  55.958333  1010.500000  21.791667  4.804545\n1996-11-04    10.222222  0.0  48.055556  1011.333333  22.722222  1.964706\n              Rainfall  day  month  year\ndatetime_utc\n1996-11-01           0    1     11  1996\n1996-11-02           0    2     11  1996\n1996-11-03           0    3     11  1996\n1996-11-04           0    4     11  1996"
  },
  {
    "url": "https://stackoverflow.com/questions/55776571/how-to-split-a-date-column-into-separate-day-month-year-column-in-pandas",
    "body": "# Reset our index so datetime_utc becomes a column\ndf.reset_index(inplace=True)\n# Create new columns\ndf['day'] = df['datetime_utc'].dt.day\ndf['month'] = df['datetime_utc'].dt.month\ndf['year'] = df['datetime_utc'].dt.year\nprint(df)\n  datetime_utc     Dewptm  Fog   Humidity    Pressurem      Tempm     Wspdm  \\\n0   1996-11-01  11.666667  0.0  52.916667 -2659.666667  22.333333  2.466667\n1   1996-11-02  10.458333  0.0  48.625000  1009.833333  22.916667  8.028571\n2   1996-11-03  12.041667  0.0  55.958333  1010.500000  21.791667  4.804545\n3   1996-11-04  10.222222  0.0  48.055556  1011.333333  22.722222  1.964706\n   Rainfall  day  month  year\n0         0    1     11  1996\n1         0    2     11  1996\n2         0    3     11  1996\n3         0    4     11  1996"
  },
  {
    "url": "https://stackoverflow.com/questions/59681461/read-a-big-mbox-file-with-python",
    "body": "#!/usr/bin/env python3\nimport email\nfrom email.policy import default\nclass MboxReader:\n    def __init__(self, filename):\n        self.handle = open(filename, 'rb')\n        assert self.handle.readline().startswith(b'From ')\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.handle.close()\n    def __iter__(self):\n        return iter(self.__next__())\n    def __next__(self):\n        lines = []\n        while True:\n            line = self.handle.readline()\n            if line == b'' or line.startswith(b'From '):\n                yield email.message_from_bytes(b''.join(lines), policy=default)\n                if line == b'':\n                    break\n                lines = []\n                continue\n            lines.append(line)"
  },
  {
    "url": "https://stackoverflow.com/questions/55466089/image-processing-algorithm-improvement-for-real-time-fedex-logo-detector",
    "body": "import numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nlogo = cv2.imread('logo.jpg', 0) # query Image\nimg = cv2.imread('main2.jpg',0)  # target Image\n# Create the sift object\nsift = cv2.xfeatures2d.SIFT_create(700)\n# Find keypoints and descriptors directly\nkp1, des1 = sift.detectAndCompute(img, None)\nkp2, des2 = sift.detectAndCompute(logo,None)\n# FLANN parameters\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks=50)   # or pass empty dictionary\nflann = cv2.FlannBasedMatcher(index_params,search_params)\nmatches = flann.knnMatch(des1,des2,k=2)\n# Need to draw only good matches, so create a mask\nmatchesMask = [[0,0] for i in range(len(matches))]\n# ratio test as per Lowe's paper\nfor i,(m,n) in enumerate(matches):\n    if m.distance < 0.7*n.distance:\n        matchesMask[i]=[1,0]\n# Draw lines\ndraw_params = dict(matchColor = (0,255,0),\n                   singlePointColor = (255,0,0),\n                   matchesMask = matchesMask,\n                   flags = 0)\n# Display the matches\nimg3 = cv2.drawMatchesKnn(img,kp1,logo,kp2,matches,None,**draw_params)\nplt.imshow(img3, )\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/56131308/create-an-abstract-enum-class",
    "body": "from abc import abstractmethod, ABC, ABCMeta\nfrom enum import auto, Flag, EnumMeta\nclass ABCEnumMeta(ABCMeta, EnumMeta):\n    def __new__(mcls, *args, **kw):\n        abstract_enum_cls = super().__new__(mcls, *args, **kw)\n        # Only check abstractions if members were defined.\n        if abstract_enum_cls._member_map_:\n            try:  # Handle existence of undefined abstract methods.\n                absmethods = list(abstract_enum_cls.__abstractmethods__)\n                if absmethods:\n                    missing = ', '.join(f'{method!r}' for method in absmethods)\n                    plural = 's' if len(absmethods) > 1 else ''\n                    raise TypeError(\n                       f\"cannot instantiate abstract class {abstract_enum_cls.__name__!r}\"\n                       f\" with abstract method{plural} {missing}\")\n            except AttributeError:\n                pass\n        return abstract_enum_cls\nclass TranslateableFlag(Flag, metaclass=ABCEnumMeta):\n    @classmethod\n    @abstractmethod\n    def base(cls):\n        pass\n    def translate(self):\n        base = self.base()\n        if self in base:\n            return base[self]\n        else:\n            ret = []\n            for basic in base:\n                if basic in self:\n                    ret.append(base[basic])\n            return \" | \".join(ret)\nclass Students1(TranslateableFlag):\n    ALICE = auto()\n    BOB = auto()\n    CHARLIE = auto()\n    ALL = ALICE | BOB | CHARLIE\n    @classmethod\n    def base(cls):\n        return {Students1.ALICE: \"Alice\", Students1.BOB: \"Bob\",\n                Students1.CHARLIE: \"Charlie\"}\n# Abstract method not defined - should raise TypeError.\nclass Students2(TranslateableFlag):\n    ALICE = auto()\n    BOB = auto()\n    CHARLIE = auto()\n    ALL = ALICE | BOB | CHARLIE\n#    @classmethod\n#    def base(cls):\n#        ..."
  },
  {
    "url": "https://stackoverflow.com/questions/76771761/why-does-llama-index-still-require-an-openai-key-when-using-hugging-face-local-e",
    "body": "from pathlib import Path\nimport gradio as gr\nimport sys\nimport logging\nimport os\nfrom llama_index.llms import HuggingFaceLLM\nfrom llama_index.prompts.prompts import SimpleInputPrompt\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext\nstorage_path = \"storage\"\ndocs_path=\"docs\"\nprint(storage_path)\nmax_input_size = 4096\nnum_outputs = 512\n#max_chunk_overlap = 20\nchunk_overlap_ratio = 0.1\nchunk_size_limit = 600\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n# This will wrap the default prompts that are internal to llama-index\nquery_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\nllm = HuggingFaceLLM(\n    context_window=4096,\n    max_new_tokens=256,\n    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n    system_prompt=system_prompt,\n    query_wrapper_prompt=query_wrapper_prompt,\n    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    device_map=\"auto\",\n    stopping_ids=[50278, 50279, 50277, 1, 0],\n    tokenizer_kwargs={\"max_length\": 4096},\n    # uncomment this if using CUDA to reduce memory usage\n    # model_kwargs={\"torch_dtype\": torch.float16}\n)\nservice_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=\"local\")\ndocuments = SimpleDirectoryReader(docs_path).load_data()\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\ndef chatbot(input_text):\n    query_engine = index.as_query_engine()\n    response = query_engine.query(input_text)\n    print(response.source_nodes)\n    relevant_files=[]\n    for node_with_score in response.source_nodes:\n        print(node_with_score)\n        print(node_with_score.node)\n        print(node_with_score.node.metadata)\n        print(node_with_score.node.metadata['file_name'])\n        file = node_with_score.node.metadata['file_name']\n        print( file )\n        # Resolve the full file path for the downloading\n        full_file_path = Path( docs_path, file ).resolve()\n        # See if it's already in the array\n        if full_file_path not in relevant_files:\n            relevant_files.append( full_file_path ) # Add it\n    print( relevant_files )\n    return response.response, relevant_files\niface = gr.Interface(fn=chatbot,\n                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n                     outputs=[\n                        gr.components.Textbox(label=\"Response\"),\n                        gr.components.File(label=\"Relevant Files\")\n                        ],\n                     title=\"Custom-trained AI Chatbot\",\n                     allow_flagging=\"never\")\niface.launch(share=False)"
  },
  {
    "url": "https://stackoverflow.com/questions/59955328/how-to-profile-flask-endpoint",
    "body": "* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nbegin\nend\n--------------------------------------------------------------------------------\nPATH: '/'\n         298 function calls in 2.992 seconds\n   Ordered by: internal time, call count\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    2.969    2.969    2.969    2.969 {built-in method time.sleep}\n        1    0.002    0.002    0.011    0.011 /usr/local/lib/python3.7/site-packages/flask/app.py:1955(finalize_request)\n        1    0.002    0.002    0.008    0.008 /usr/local/lib/python3.7/site-packages/werkzeug/wrappers/base_response.py:173(__init__)\n       35    0.002    0.000    0.002    0.000 {built-in method builtins.isinstance}\n        4    0.001    0.000    0.001    0.000 /usr/local/lib/python3.7/site-packages/werkzeug/datastructures.py:910(_unicodify_header_value)\n        2    0.001    0.000    0.003    0.002 /usr/local/lib/python3.7/site-packages/werkzeug/datastructures.py:1298(__setitem__)\n        1    0.001    0.001    0.001    0.001 /usr/local/lib/python3.7/site-packages/werkzeug/datastructures.py:960(__getitem__)\n        6    0.001    0.000    0.001    0.000 /usr/local/lib/python3.7/site-packages/werkzeug/_compat.py:210(to_unicode)\n        2    0.000    0.000    0.002    0.001 /usr/local/lib/python3.7/site-packages/werkzeug/datastructures.py:1212(set)\n        4    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}\n        1    0.000    0.000    0.002    0.002 /usr/local/lib/python3.7/site-packages/werkzeug/wrappers/base_response.py:341(set_data)\n       10    0.000    0.000    0.001    0.000 /usr/local/lib/python3.7/site-packages/werkzeug/local.py:70(__getattr__)\n        8    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n        1    0.000    0.000    0.008    0.008 /usr/local/lib/python3.7/site-packages/flask/app.py:2029(make_response)\n        1    0.000    0.000    0.004    0.004 /usr/local/lib/python3.7/site-packages/werkzeug/routing.py:1551(bind_to_environ)\n        1    0.000    0.000    0.000    0.000 /usr/local/lib/python3.7/site-packages/werkzeug/_internal.py:67(_get_environ)\n        1    0.000    0.000    0.001    0.001 /usr/local/lib/python3.7/site-packages/werkzeug/routing.py:1674(__init__)\n[snipped for berevity]"
  },
  {
    "url": "https://stackoverflow.com/questions/59955328/how-to-profile-flask-endpoint",
    "body": "* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nbegin\nend\n--------------------------------------------------------------------------------\nPATH: '/'\n         300 function calls in 3.016 seconds\n   Ordered by: internal time, call count\n   List reduced from 131 to 2 due to restriction <'/code/app.py'>\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n        1    0.000    0.000    3.007    3.007 /code/app.py:12(index)\n        1    0.000    0.000    2.002    2.002 /code/app.py:9(slower)\n--------------------------------------------------------------------------------"
  },
  {
    "url": "https://stackoverflow.com/questions/55565760/anaconda-python-site-packages-subfolders-with-tilde-in-name-what-are-they",
    "body": "class AdjacentTempDirectory(TempDirectory):\n    \"\"\"Helper class that creates a temporary directory adjacent to a real one.\n    Attributes:\n        original\n            The original directory to create a temp directory for.\n        path\n            After calling create() or entering, contains the full\n            path to the temporary directory.\n        delete\n            Whether the directory should be deleted when exiting\n            (when used as a contextmanager)\n    \"\"\"\n    # The characters that may be used to name the temp directory\n    # We always prepend a ~ and then rotate through these until\n    # a usable name is found.\n    # pkg_resources raises a different error for .dist-info folder\n    # with leading '-' and invalid metadata\n    LEADING_CHARS = \"-~.=%0123456789\"\n    ..."
  },
  {
    "url": "https://stackoverflow.com/questions/57874226/valueerror-view-limit-minimum-35738-3640567-is-less-than-1-and-is-an-invalid-m",
    "body": "import pandas as pd\nimport matplotlib.pyplot as plt\n# given the following data\ndata = {'datetime': ['2018-05-15', '2018-05-16', '2018-05-17', '2018-05-18', '2018-05-21', '2018-05-22', '2018-05-23', '2018-05-24', '2018-05-25', '2018-05-29'],\n        'price': [1079.22998, 1081.77002, 1078.589966, 1066.359985, 1079.579956, 1069.72998, 1079.689941, 1079.23999, 1075.660034, 1060.319946]}\ndf_google = pd.DataFrame(data)\n# convert the datetime column to a datetime type and assign it back to the column\ndf_google.datetime = pd.to_datetime(df_google.datetime)\n# display(df_google.head())\n     datetime        price\n0  2018-05-15  1079.229980\n1  2018-05-16  1081.770020\n2  2018-05-17  1078.589966\n3  2018-05-18  1066.359985\n4  2018-05-21  1079.579956\n5  2018-05-22  1069.729980\n6  2018-05-23  1079.689941\n7  2018-05-24  1079.239990\n8  2018-05-25  1075.660034\n9  2018-05-29  1060.319946"
  },
  {
    "url": "https://stackoverflow.com/questions/63427037/package-python3-7-is-not-available",
    "body": "apt-get update\napt-get install -y build-essential openssl openssl-dev* wget curl\nwget https://www.python.org/ftp/python/3.7.8/Python-3.7.8.tgz\ntar -xvf Python-3.7.8.tgz\ncd Python-3.7.8\n./configure --enable-shared\nmake\nmake test\nmake install\n# Steps from here are to enable other libraries in linux to\n# access the shared python libraries.\ncd /usr/local/lib/\ncp libpython3.so /usr/lib64/\ncp libpython3.so /usr/lib\ncp libpython3.7m.so.1.0 /usr/lib64/\ncp libpython3.7m.so.1.0 /usr/lib/\ncd /usr/lib64\nln -s libpython3.7m.so.1.0 libpython3.7m.so\ncd /usr/lib\nln -s libpython3.7m.so.1.0 libpython3.7m.so"
  },
  {
    "url": "https://stackoverflow.com/questions/63427037/package-python3-7-is-not-available",
    "body": "(testvirtual) root@fe794c7ff15e:~# pip install flask\nCollecting flask\n  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n     |████████████████████████████████| 94 kB 404 kB/s\nCollecting Jinja2>=2.10.1\n  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n     |████████████████████████████████| 125 kB 10.4 MB/s\nCollecting click>=5.1\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n     |████████████████████████████████| 82 kB 165 kB/s\nCollecting Werkzeug>=0.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n     |████████████████████████████████| 298 kB 11.9 MB/s\nCollecting itsdangerous>=0.24\n  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\nCollecting MarkupSafe>=0.23\n  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)\nInstalling collected packages: MarkupSafe, Jinja2, click, Werkzeug, itsdangerous, flask\nSuccessfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0"
  },
  {
    "url": "https://stackoverflow.com/questions/18679264/how-to-use-malloc-and-free-with-python-ctypes",
    "body": "import ctypes as ct\nclass Example(ct.Structure):\n    _fields_ = (('data', ct.POINTER(ct.c_char)),\n                ('len', ct.c_int),\n                ('doubles', ct.POINTER(ct.c_double)),\n                ('count', ct.c_int))\n    def __init__(self, length, count):\n        self.data = ct.create_string_buffer(length)\n        self.len = length\n        self.doubles = (ct.c_double * count)()\n        self.count = count\n    def __repr__(self):\n        '''Return string describing how to print an Example object.\n        '''\n        # Note that slicing a pointer to a specific\n        # length returns a list of if its objects.\n        return (f'Example({ct.string_at(self.data)}, {self.doubles[:self.count]}')\nclass Dll:\n    def __init__(self):\n        self.dll = ct.CDLL('./test')\n        self.dll.func.argtypes = ct.POINTER(Example),\n        self.dll.func.restype = None\n    def func(self, ex):\n        self.dll.func(ct.byref(ex))\nd = Dll()\ne = Example(20, 5)\nprint('before:', e)\nd.func(e)\nprint('after:', e)"
  },
  {
    "url": "https://stackoverflow.com/questions/48078051/duplicate-log-entries-with-google-cloud-stackdriver-logging-of-python-code-on-ku",
    "body": "import google.cloud.logging\nimport logging\ndef is_cloud_handler(handler: logging.Handler) -> bool:\n    \"\"\"\n    is_cloud_handler\n    Returns True or False depending on whether the input is a\n    google-cloud-logging handler class\n    \"\"\"\n    accepted_handlers = (\n        google.cloud.logging.handlers.StructuredLogHandler,\n        google.cloud.logging.handlers.CloudLoggingHandler,\n        google.cloud.logging.handlers.ContainerEngineHandler,\n        google.cloud.logging.handlers.AppEngineHandler,\n    )\n    return isinstance(handler, accepted_handlers)\ndef set_up_logging():\n    # here we assume you'll be using the basic logging methods\n    # logging.info, logging.warn etc. which invoke the root logger\n    client = google.cloud.logging.Client()\n    client.setup_logging()\n    root_logger = logging.getLogger()\n    root_logger.handlers = [h for h in root_logger.handlers if is_cloud_handler(h)]"
  },
  {
    "url": "https://stackoverflow.com/questions/64083104/making-python-generator-via-c20-coroutines",
    "body": "#include <coroutine>\n#include <exception>\n#include <string>\n#include <iostream>\nstruct generator_input {};\ntemplate <typename OutputType, typename InputType>\nstruct generator {\n    struct promise_type;\n    using coro_handle = std::coroutine_handle<promise_type>;\n    struct passthru_value\n    {\n        InputType &ret_;\n        bool await_ready() {return true;}\n        void await_suspend(coro_handle) {}\n        InputType &await_resume() { return ret_; }\n    };\n    struct promise_type {\n        OutputType current_value;\n        InputType input_value;\n        auto get_return_object() { return generator{coro_handle::from_promise(*this)}; }\n        auto initial_suspend() { return std::suspend_always{}; }\n        auto final_suspend() { return std::suspend_always{}; }\n        void unhandled_exception() { std::terminate(); }\n        auto yield_value(OutputType value) {\n            current_value = value;\n            return std::suspend_always{};\n        }\n        void return_void() {}\n        auto await_transform(generator_input)\n        {\n            return passthru_value{input_value};\n        }\n    };\n    bool next() { return coro ? (coro.resume(), !coro.done()) : false; }\n    OutputType value() { return coro.promise().current_value; }\n    void send(const InputType &input)\n    {\n        coro.promise().input_value = input;\n    }\n    void send(InputType &&input)\n    {\n        coro.promise().input_value = std::move(input);\n    }\n    generator(generator const & rhs) = delete;\n    generator(generator &&rhs)\n        :coro(rhs.coro)\n    {\n        rhs.coro = nullptr;\n    }\n    ~generator() {\n        if (coro)\n            coro.destroy();\n    }\nprivate:\n    generator(coro_handle h) : coro(h) {}\n    coro_handle coro;\n};\ngenerator<char, std::string> hello(){\n    auto word = co_await generator_input{};\n    for(auto &ch: word){\n        co_yield ch;\n    }\n}\nint main(int, char**)\n{\n    auto test = hello();\n    test.send(\"hello world\");\n    while(test.next())\n    {\n        std::cout << test.value() << ' ';\n    }\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/58585019/understanding-gradient-policy-deriving",
    "body": "import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nNUM_EPISODES = 5000\nLEARNING_RATE = 0.0001\nGAMMA = 0.99\n# noinspection PyMethodMayBeStatic\nclass Agent:\n    def __init__(self):\n        self.poly = PolynomialFeatures(1)\n        self.w = np.random.randn(5, 1) * 0.01\n    # Our policy that maps state to action parameterized by w\n    # noinspection PyShadowingNames\n    def policy(self, state):\n        z = np.sum(state.dot(self.w))\n        return self.sigmoid(z)\n    def sigmoid(self, x):\n        s = 1 / (1 + np.exp(-x))\n        return s\n    def sigmoid_grad(self, sig_x):\n        return sig_x * (1 - sig_x)\n    def grad(self, probs, action, state):\n        grad = state.T.dot(probs - action)\n        return -grad\n    def update_with(self, grads, rewards):\n        if len(grads) < 50:\n            return\n        for i in range(len(grads)):\n            # Loop through everything that happened in the episode\n            # and update towards the log policy gradient times **FUTURE** reward\n            total_grad_effect = 0\n            for t, r in enumerate(rewards[i:]):\n                total_grad_effect += r * (GAMMA ** r)\n            self.w += LEARNING_RATE * grads[i] * total_grad_effect\ndef main(argv):\n    env = gym.make('CartPole-v0')\n    np.random.seed(1)\n    agent = Agent()\n    complete_scores = []\n    for e in range(NUM_EPISODES):\n        state = env.reset()[None, :]\n        state = agent.poly.fit_transform(state)\n        rewards = []\n        grads = []\n        score = 0\n        while True:\n            probs = agent.policy(state)\n            action_space = env.action_space.n\n            action = np.random.choice(action_space, p=[1 - probs, probs])\n            next_state, reward, done, _ = env.step(action)\n            next_state = next_state[None, :]\n            next_state = agent.poly.fit_transform(next_state.reshape(1, 4))\n            grad = agent.grad(probs, action, state)\n            grads.append(grad)\n            rewards.append(reward)\n            score += reward\n            state = next_state\n            if done:\n                break\n        agent.update_with(grads, rewards)\n        complete_scores.append(score)\n    env.close()\n    plt.plot(np.arange(NUM_EPISODES),\n             complete_scores)\n    plt.savefig('image1.png')\nif __name__ == '__main__':\n    main(None)"
  },
  {
    "url": "https://stackoverflow.com/questions/56754451/how-to-connect-the-ends-of-edges-in-order-to-close-the-holes-between-them",
    "body": "circle_mask = np.zeros(original.shape, dtype=np.uint8)\ncircles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1.5, 200)\n# Convert the (x, y) coordinates and radius of the circles to integers\ncircles = np.round(circles[0, :]).astype(\"int\")\ncircle_ratio = 0.85\n# Loop over the (x, y) coordinates and radius of the circles\nfor (x, y, r) in circles:\n    # Draw the circle, create mask, and obtain soil ROI\n    cv2.circle(image, (x, y), int(r * circle_ratio), (0, 255, 0), 2)\n    cv2.circle(circle_mask, (x, y), int(r * circle_ratio), (255, 255, 255), -1)\n    soil_ROI = cv2.bitwise_and(original, circle_mask)"
  },
  {
    "url": "https://stackoverflow.com/questions/56754451/how-to-connect-the-ends-of-edges-in-order-to-close-the-holes-between-them",
    "body": "import cv2\nimport numpy as np\nimage = cv2.imread('5.png')\noriginal = image.copy()\nblur = cv2.GaussianBlur(image, (3,3), 0)\ngray = cv2.cvtColor(blur, cv2.COLOR_BGR2GRAY)\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\ncircle_mask = np.zeros(original.shape, dtype=np.uint8)\ncircles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1.5, 200)\n# Convert the (x, y) coordinates and radius of the circles to integers\ncircles = np.round(circles[0, :]).astype(\"int\")\ncircle_ratio = 0.85\n# Loop over the (x, y) coordinates and radius of the circles\nfor (x, y, r) in circles:\n    # Draw the circle, create mask, and obtain soil ROI\n    cv2.circle(image, (x, y), int(r * circle_ratio), (0, 255, 0), 2)\n    cv2.circle(circle_mask, (x, y), int(r * circle_ratio), (255, 255, 255), -1)\n    soil_ROI = cv2.bitwise_and(original, circle_mask)\ngray_soil_ROI = cv2.cvtColor(soil_ROI, cv2.COLOR_BGR2GRAY)\nclose = cv2.morphologyEx(gray_soil_ROI, cv2.MORPH_CLOSE, kernel)\ncnts = cv2.findContours(close, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\ncrack_area = 0\nminumum_area = 25\nfor c in cnts:\n    area = cv2.contourArea(c)\n    if area > minumum_area:\n        cv2.drawContours(original,[c], 0, (36,255,12), 2)\n        crack_area += area\nprint(crack_area)\ncv2.imshow('close', close)\ncv2.imshow('circle_mask', circle_mask)\ncv2.imshow('soil_ROI', soil_ROI)\ncv2.imshow('original', original)\ncv2.waitKey(0)"
  },
  {
    "url": "https://stackoverflow.com/questions/54328681/enable-pk-based-filtering-in-django-graphene-relay-while-retaining-global-ids",
    "body": "import django_filters\nimport graphene\nfrom graphene import relay\nfrom graphene_django import DjangoObjectType\nfrom multy_herr.objections.models import Objection\nclass ObjectionFilter(django_filters.FilterSet):\n    pk = django_filters.NumberFilter(field_name='pk')\n    class Meta:\n        model = Objection\n        fields = [\n            'pk',\n        ]\nclass ObjectionNode(DjangoObjectType):\n    pk = graphene.Field(type=graphene.Int, source='id')\n    class Meta:\n        model = Objection\n        fields = [\n            'id',\n            'pk',\n            'detail',\n            'hidden',\n            'report',\n        ]\n        filter_fields = {\n            'pk': ['exact'],\n            'detail': ['icontains', 'istartswith'],\n            'created_by__name': ['icontains', ],\n            'hidden': ['exact'],\n            'report': ['exact'],\n        }\n        interfaces = (relay.Node,)"
  },
  {
    "url": "https://stackoverflow.com/questions/54328681/enable-pk-based-filtering-in-django-graphene-relay-while-retaining-global-ids",
    "body": "import graphene\nfrom graphene import relay\nfrom graphene_django.filter import DjangoFilterConnectionField\nfrom multy_herr.objections.grapheql.nodes import ObjectionNode, ObjectionFilter\nfrom multy_herr.objections.models import Objection\nclass ObjectionQuery(graphene.ObjectType):\n    objection = relay.Node.Field(ObjectionNode)\n    all_objections = DjangoFilterConnectionField(ObjectionNode,\n                                                 filterset_class=ObjectionFilter)\n    def resolve_all_objections(self, info, **kwargs):\n        if info.context.user.is_authenticated is False:\n            return Objection.objects.none()\n        return Objection.objects.filter(created_by=info.context.user)"
  },
  {
    "url": "https://stackoverflow.com/questions/43723214/pil-image-vs-skimage-io-when-to-use-each-and-which-if-one-is-prefered-over-t",
    "body": "21/10/2022  06:20         3,363,278 imageio-2.22.2-py3-none-any.whl\n21/10/2022  06:20         2,023,640 networkx-2.8.7-py3-none-any.whl\n21/10/2022  06:20        14,643,698 numpy-1.23.4-cp310-cp310-win_amd64.whl\n21/10/2022  06:20            40,750 packaging-21.3-py3-none-any.whl\n21/10/2022  06:20         3,276,402 Pillow-9.2.0-cp310-cp310-win_amd64.whl\n21/10/2022  06:20            98,338 pyparsing-3.0.9-py3-none-any.whl\n21/10/2022  06:20         4,162,789 PyWavelets-1.4.1-cp310-cp310-win_amd64.whl\n21/10/2022  06:20        12,044,719 scikit_image-0.19.3-cp310-cp310-win_amd64.whl\n21/10/2022  06:20        40,141,232 scipy-1.9.3-cp310-cp310-win_amd64.whl\n21/10/2022  06:20           210,312 tifffile-2022.10.10-py3-none-any.whl\n              10 File(s)     80,005,158 bytes"
  },
  {
    "url": "https://stackoverflow.com/questions/64096953/how-to-convert-yolo-format-bounding-box-coordinates-into-opencv-format",
    "body": "import cv2\nimport matplotlib.pyplot as plt\nimg = cv2.imread(<image_path>)\ndh, dw, _ = img.shape\nfl = open(<label_path>, 'r')\ndata = fl.readlines()\nfl.close()\nfor dt in data:\n    # Split string to float\n    _, x, y, w, h = map(float, dt.split(' '))\n    # Taken from https://github.com/pjreddie/darknet/blob/810d7f797bdb2f021dbe65d2524c2ff6b8ab5c8b/src/image.c#L283-L291\n    # via https://stackoverflow.com/questions/44544471/how-to-get-the-coordinates-of-the-bounding-box-in-yolo-object-detection#comment102178409_44592380\n    l = int((x - w / 2) * dw)\n    r = int((x + w / 2) * dw)\n    t = int((y - h / 2) * dh)\n    b = int((y + h / 2) * dh)\n\n    if l < 0:\n        l = 0\n    if r > dw - 1:\n        r = dw - 1\n    if t < 0:\n        t = 0\n    if b > dh - 1:\n        b = dh - 1\n    cv2.rectangle(img, (l, t), (r, b), (0, 0, 255), 1)\nplt.imshow(img)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/52631291/vectorizing-or-speeding-up-fuzzywuzzy-string-matching-on-pandas-column",
    "body": "import pandas as pd, numpy as np\nfrom rapidfuzz import process, utils\norg_list = df['org_name']\nprocessed_orgs = [utils.default_process(org) for org in org_list]\nfor (i, processed_query) in enumerate(processed_orgs):\n    # None is skipped by extractOne, so we set the current element to None an\n    # revert this change after the comparision\n    processed_orgs[i] = None\n    match = process.extractOne(processed_query, processed_orgs, processor=None, score_cutoff=93)\n    processed_orgs[i] = processed_query\n    if match:\n        df.loc[i, 'fuzzy_match'] = org_list[match[2]]\n        df.loc[i, 'fuzzy_match_score'] = match[1]"
  },
  {
    "url": "https://stackoverflow.com/questions/46598371/overlay-a-line-function-on-a-scatter-plot",
    "body": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# create a dataframe with sample x and y\nnp.random.seed(365)\nx = 5*np.random.random(200)\ndf = pd.DataFrame({'x': x, 'y': 10*x+10*np.random.random(200)})\n# add custom line to the dataframe\nbase_beta = [10, 5]\ndf['y_line'] = base_beta[0] + base_beta[1]*df.x\ndisplay(df.head())\n          x          y     y_line\n0  4.707279  50.634968  33.536394\n1  3.208014  33.890507  26.040068\n2  3.423052  37.853276  27.115262\n3  2.942810  29.899257  24.714052\n4  2.719436  36.932170  23.597180"
  },
  {
    "url": "https://stackoverflow.com/questions/63459424/how-to-add-multiple-graphs-to-dash-app-on-a-single-browser-page",
    "body": "import dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\nimport pandas as pd\nimport plotly.express as px\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n# assume you have a \"long-form\" data frame\n# see https://plotly.com/python/px-arguments/ for more options\ndf_bar = pd.DataFrame({\n    \"Fruit\": [\"Apples\", \"Oranges\", \"Bananas\", \"Apples\", \"Oranges\", \"Bananas\"],\n    \"Amount\": [4, 1, 2, 2, 4, 5],\n    \"City\": [\"SF\", \"SF\", \"SF\", \"Montreal\", \"Montreal\", \"Montreal\"]\n})\nfig = px.bar(df_bar, x=\"Fruit\", y=\"Amount\", color=\"City\", barmode=\"group\")\napp.layout = html.Div(children=[\n    # All elements from the top of the page\n    html.Div([\n        html.H1(children='Hello Dash'),\n        html.Div(children='''\n            Dash: A web application framework for Python.\n        '''),\n        dcc.Graph(\n            id='graph1',\n            figure=fig\n        ),\n    ]),\n    # New Div for all elements in the new 'row' of the page\n    html.Div([\n        html.H1(children='Hello Dash'),\n        html.Div(children='''\n            Dash: A web application framework for Python.\n        '''),\n        dcc.Graph(\n            id='graph2',\n            figure=fig\n        ),\n    ]),\n])\nif __name__ == '__main__':\n    app.run_server(debug=True)"
  },
  {
    "url": "https://stackoverflow.com/questions/63459424/how-to-add-multiple-graphs-to-dash-app-on-a-single-browser-page",
    "body": "import dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\nimport pandas as pd\nimport plotly.express as px\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n# assume you have a \"long-form\" data frame\n# see https://plotly.com/python/px-arguments/ for more options\ndf_bar = pd.DataFrame({\n    \"Fruit\": [\"Apples\", \"Oranges\", \"Bananas\", \"Apples\", \"Oranges\", \"Bananas\"],\n    \"Amount\": [4, 1, 2, 2, 4, 5],\n    \"City\": [\"SF\", \"SF\", \"SF\", \"Montreal\", \"Montreal\", \"Montreal\"]\n})\nfig = px.bar(df_bar, x=\"Fruit\", y=\"Amount\", color=\"City\", barmode=\"group\")\n# Data for the tip-graph\ndf_tip = px.data.tips()\napp.layout = html.Div(children=[\n    # All elements from the top of the page\n    html.Div([\n        html.H1(children='Hello Dash'),\n        html.Div(children='''\n            Dash: A web application framework for Python.\n        '''),\n        dcc.Graph(\n            id='example-graph',\n            figure=fig\n        ),\n    ]),\n    # New Div for all elements in the new 'row' of the page\n    html.Div([\n        dcc.Graph(id='tip-graph'),\n        html.Label([\n            \"colorscale\",\n            dcc.Dropdown(\n                id='colorscale-dropdown', clearable=False,\n                value='bluyl', options=[\n                    {'label': c, 'value': c}\n                    for c in px.colors.named_colorscales()\n                ])\n        ]),\n    ])\n])\n# Callback function that automatically updates the tip-graph based on chosen colorscale\n@app.callback(\n    Output('tip-graph', 'figure'),\n    [Input(\"colorscale-dropdown\", \"value\")]\n)\ndef update_tip_figure(colorscale):\n    return px.scatter(\n        df_tip, x=\"total_bill\", y=\"tip\", color=\"size\",\n        color_continuous_scale=colorscale,\n        render_mode=\"webgl\", title=\"Tips\"\n    )\nif __name__ == '__main__':\n    app.run_server(debug=True)"
  },
  {
    "url": "https://stackoverflow.com/questions/63459424/how-to-add-multiple-graphs-to-dash-app-on-a-single-browser-page",
    "body": "import dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\nimport pandas as pd\nimport plotly.express as px\nfrom jupyter_dash import JupyterDash\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n# assume you have a \"long-form\" data frame\n# see https://plotly.com/python/px-arguments/ for more options\ndf_bar = pd.DataFrame({\n    \"Fruit\": [\"Apples\", \"Oranges\", \"Bananas\", \"Apples\", \"Oranges\", \"Bananas\"],\n    \"Amount\": [4, 1, 2, 2, 4, 5],\n    \"City\": [\"SF\", \"SF\", \"SF\", \"Montreal\", \"Montreal\", \"Montreal\"]\n})\nfig = px.bar(df_bar, x=\"Fruit\", y=\"Amount\", color=\"City\", barmode=\"group\")\napp.layout = html.Div(children=[\n    # All elements from the top of the page\n    html.Div([\n        html.Div([\n            html.H1(children='Hello Dash'),\n            html.Div(children='''\n                Dash: A web application framework for Python.\n            '''),\n            dcc.Graph(\n                id='graph1',\n                figure=fig\n            ),\n        ], className='six columns'),\n        html.Div([\n            html.H1(children='Hello Dash'),\n            html.Div(children='''\n                Dash: A web application framework for Python.\n            '''),\n            dcc.Graph(\n                id='graph2',\n                figure=fig\n            ),\n        ], className='six columns'),\n    ], className='row'),\n    # New Div for all elements in the new 'row' of the page\n    html.Div([\n        html.H1(children='Hello Dash'),\n        html.Div(children='''\n            Dash: A web application framework for Python.\n        '''),\n        dcc.Graph(\n            id='graph3',\n            figure=fig\n        ),\n    ], className='row'),\n])\nif __name__ == '__main__':\n    app.run_server(debug=True)"
  },
  {
    "url": "https://stackoverflow.com/questions/15827196/how-can-i-add-a-tag-to-a-key-in-boto-amazon-s3",
    "body": "import boto3\ns3_client = boto3.client(\n    's3',\n    region_name='region-name',\n    aws_access_key_id='aws-access-key-id',\n    aws_secret_access_key='aws-secret-access-key',\n)\nget_tags_response = s3_client.get_object_tagging(\n    Bucket='your-bucket-name',\n    Key='folder-if-any/file-name.extension',\n)\nput_tags_response = s3_client.put_object_tagging(\n    Bucket='your-bucket-name',\n    Key='folder-if-any/file-name.extension',\n    Tagging={\n        'TagSet': [\n            {\n                'Key': 'tag-key',\n                'Value': 'tag-value'\n            },\n        ]\n    }\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/48024720/python-how-to-check-if-socket-is-still-connected",
    "body": "import logging\nimport socket\nlogger = logging.getLogger(__name__)\ndef is_socket_closed(sock: socket.socket) -> bool:\n    try:\n        # this will try to read bytes without blocking and also without removing them from buffer (peek only)\n        data = sock.recv(16, socket.MSG_DONTWAIT | socket.MSG_PEEK)\n        if len(data) == 0:\n            return True\n    except BlockingIOError:\n        return False  # socket is open and reading from it would block\n    except ConnectionResetError:\n        return True  # socket was closed for some other reason\n    except Exception as e:\n        logger.exception(\"unexpected exception when checking if a socket is closed\")\n        return False\n    return False"
  },
  {
    "url": "https://stackoverflow.com/questions/58592291/how-to-capture-multiple-camera-streams-with-opencv",
    "body": "from PyQt4 import QtCore, QtGui\nimport qdarkstyle\nfrom threading import Thread\nfrom collections import deque\nfrom datetime import datetime\nimport time\nimport sys\nimport cv2\nimport imutils\nclass CameraWidget(QtGui.QWidget):\n    \"\"\"Independent camera feed\n    Uses threading to grab IP camera frames in the background\n    @param width - Width of the video frame\n    @param height - Height of the video frame\n    @param stream_link - IP/RTSP/Webcam link\n    @param aspect_ratio - Whether to maintain frame aspect ratio or force into fraame\n    \"\"\"\n    def __init__(self, width, height, stream_link=0, aspect_ratio=False, parent=None, deque_size=1):\n        super(CameraWidget, self).__init__(parent)\n\n        # Initialize deque used to store frames read from the stream\n        self.deque = deque(maxlen=deque_size)\n        # Slight offset is needed since PyQt layouts have a built in padding\n        # So add offset to counter the padding\n        self.offset = 16\n        self.screen_width = width - self.offset\n        self.screen_height = height - self.offset\n        self.maintain_aspect_ratio = aspect_ratio\n        self.camera_stream_link = stream_link\n        # Flag to check if camera is valid/working\n        self.online = False\n        self.capture = None\n        self.video_frame = QtGui.QLabel()\n        self.load_network_stream()\n\n        # Start background frame grabbing\n        self.get_frame_thread = Thread(target=self.get_frame, args=())\n        self.get_frame_thread.daemon = True\n        self.get_frame_thread.start()\n        # Periodically set video frame to display\n        self.timer = QtCore.QTimer()\n        self.timer.timeout.connect(self.set_frame)\n        self.timer.start(.5)\n        print('Started camera: {}'.format(self.camera_stream_link))\n    def load_network_stream(self):\n        \"\"\"Verifies stream link and open new stream if valid\"\"\"\n        def load_network_stream_thread():\n            if self.verify_network_stream(self.camera_stream_link):\n                self.capture = cv2.VideoCapture(self.camera_stream_link)\n                self.online = True\n        self.load_stream_thread = Thread(target=load_network_stream_thread, args=())\n        self.load_stream_thread.daemon = True\n        self.load_stream_thread.start()\n    def verify_network_stream(self, link):\n        \"\"\"Attempts to receive a frame from given link\"\"\"\n        cap = cv2.VideoCapture(link)\n        if not cap.isOpened():\n            return False\n        cap.release()\n        return True\n    def get_frame(self):\n        \"\"\"Reads frame, resizes, and converts image to pixmap\"\"\"\n        while True:\n            try:\n                if self.capture.isOpened() and self.online:\n                    # Read next frame from stream and insert into deque\n                    status, frame = self.capture.read()\n                    if status:\n                        self.deque.append(frame)\n                    else:\n                        self.capture.release()\n                        self.online = False\n                else:\n                    # Attempt to reconnect\n                    print('attempting to reconnect', self.camera_stream_link)\n                    self.load_network_stream()\n                    self.spin(2)\n                self.spin(.001)\n            except AttributeError:\n                pass\n    def spin(self, seconds):\n        \"\"\"Pause for set amount of seconds, replaces time.sleep so program doesnt stall\"\"\"\n        time_end = time.time() + seconds\n        while time.time() < time_end:\n            QtGui.QApplication.processEvents()\n    def set_frame(self):\n        \"\"\"Sets pixmap image to video frame\"\"\"\n        if not self.online:\n            self.spin(1)\n            return\n        if self.deque and self.online:\n            # Grab latest frame\n            frame = self.deque[-1]\n            # Keep frame aspect ratio\n            if self.maintain_aspect_ratio:\n                self.frame = imutils.resize(frame, width=self.screen_width)\n            # Force resize\n            else:\n                self.frame = cv2.resize(frame, (self.screen_width, self.screen_height))\n            # Add timestamp to cameras\n            cv2.rectangle(self.frame, (self.screen_width-190,0), (self.screen_width,50), color=(0,0,0), thickness=-1)\n            cv2.putText(self.frame, datetime.now().strftime('%H:%M:%S'), (self.screen_width-185,37), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), lineType=cv2.LINE_AA)\n            # Convert to pixmap and set to video frame\n            self.img = QtGui.QImage(self.frame, self.frame.shape[1], self.frame.shape[0], QtGui.QImage.Format_RGB888).rgbSwapped()\n            self.pix = QtGui.QPixmap.fromImage(self.img)\n            self.video_frame.setPixmap(self.pix)\n    def get_video_frame(self):\n        return self.video_frame\n\ndef exit_application():\n    \"\"\"Exit program event handler\"\"\"\n    sys.exit(1)\nif __name__ == '__main__':\n    # Create main application window\n    app = QtGui.QApplication([])\n    app.setStyleSheet(qdarkstyle.load_stylesheet_pyqt())\n    app.setStyle(QtGui.QStyleFactory.create(\"Cleanlooks\"))\n    mw = QtGui.QMainWindow()\n    mw.setWindowTitle('Camera GUI')\n    mw.setWindowFlags(QtCore.Qt.FramelessWindowHint)\n    cw = QtGui.QWidget()\n    ml = QtGui.QGridLayout()\n    cw.setLayout(ml)\n    mw.setCentralWidget(cw)\n    mw.showMaximized()\n\n    # Dynamically determine screen width/height\n    screen_width = QtGui.QApplication.desktop().screenGeometry().width()\n    screen_height = QtGui.QApplication.desktop().screenGeometry().height()\n\n    # Create Camera Widgets\n    username = 'Your camera username!'\n    password = 'Your camera password!'\n\n    # Stream links\n    camera0 = 'rtsp://{}:{}@192.168.1.43:554/cam/realmonitor?channel=1&subtype=0'.format(username, password)\n    camera1 = 'rtsp://{}:{}@192.168.1.45/axis-media/media.amp'.format(username, password)\n    camera2 = 'rtsp://{}:{}@192.168.1.47:554/cam/realmonitor?channel=1&subtype=0'.format(username, password)\n    camera3 = 'rtsp://{}:{}@192.168.1.40:554/cam/realmonitor?channel=1&subtype=0'.format(username, password)\n    camera4 = 'rtsp://{}:{}@192.168.1.44:554/cam/realmonitor?channel=1&subtype=0'.format(username, password)\n    camera5 = 'rtsp://{}:{}@192.168.1.42:554/cam/realmonitor?channel=1&subtype=0'.format(username, password)\n    camera6 = 'rtsp://{}:{}@192.168.1.46:554/cam/realmonitor?channel=1&subtype=0'.format(username, password)\n    camera7 = 'rtsp://{}:{}@192.168.1.41:554/cam/realmonitor?channel=1&subtype=0'.format(username, password)\n\n    # Create camera widgets\n    print('Creating Camera Widgets...')\n    zero = CameraWidget(screen_width//3, screen_height//3, camera0)\n    one = CameraWidget(screen_width//3, screen_height//3, camera1)\n    two = CameraWidget(screen_width//3, screen_height//3, camera2)\n    three = CameraWidget(screen_width//3, screen_height//3, camera3)\n    four = CameraWidget(screen_width//3, screen_height//3, camera4)\n    five = CameraWidget(screen_width//3, screen_height//3, camera5)\n    six = CameraWidget(screen_width//3, screen_height//3, camera6)\n    seven = CameraWidget(screen_width//3, screen_height//3, camera7)\n\n    # Add widgets to layout\n    print('Adding widgets to layout...')\n    ml.addWidget(zero.get_video_frame(),0,0,1,1)\n    ml.addWidget(one.get_video_frame(),0,1,1,1)\n    ml.addWidget(two.get_video_frame(),0,2,1,1)\n    ml.addWidget(three.get_video_frame(),1,0,1,1)\n    ml.addWidget(four.get_video_frame(),1,1,1,1)\n    ml.addWidget(five.get_video_frame(),1,2,1,1)\n    ml.addWidget(six.get_video_frame(),2,0,1,1)\n    ml.addWidget(seven.get_video_frame(),2,1,1,1)\n    print('Verifying camera credentials...')\n    mw.show()\n    QtGui.QShortcut(QtGui.QKeySequence('Ctrl+Q'), mw, exit_application)\n    if(sys.flags.interactive != 1) or not hasattr(QtCore, 'PYQT_VERSION'):\n        QtGui.QApplication.instance().exec_()"
  },
  {
    "url": "https://stackoverflow.com/questions/67085963/generate-colors-of-noise-in-python",
    "body": "def noise_psd(N, psd = lambda f: 1):\n        X_white = np.fft.rfft(np.random.randn(N));\n        S = psd(np.fft.rfftfreq(N))\n        # Normalize S\n        S = S / np.sqrt(np.mean(S**2))\n        X_shaped = X_white * S;\n        return np.fft.irfft(X_shaped);\ndef PSDGenerator(f):\n    return lambda N: noise_psd(N, f)\n@PSDGenerator\ndef white_noise(f):\n    return 1;\n@PSDGenerator\ndef blue_noise(f):\n    return np.sqrt(f);\n@PSDGenerator\ndef violet_noise(f):\n    return f;\n@PSDGenerator\ndef brownian_noise(f):\n    return 1/np.where(f == 0, float('inf'), f)\n@PSDGenerator\ndef pink_noise(f):\n    return 1/np.where(f == 0, float('inf'), np.sqrt(f))"
  },
  {
    "url": "https://stackoverflow.com/questions/45828616/streaming-large-training-and-test-files-into-tensorflows-dnnclassifier",
    "body": "class MyCsvDatasetBuilder(tfds.core.GeneratorBasedBuilder):\n  VERSION = tfds.core.Version(\"0.0.1\")\n  def _info(self):\n    return tfds.core.DatasetInfo(\n        builder=self,\n        description=(\n            \"My dataset\"),\n        features=tfds.features.FeaturesDict({\n            \"features\": tfds.features.Tensor(\n              shape=(FEATURE_SIZE,), dtype=tf.float32),\n            \"label\": tfds.features.ClassLabel(\n                names=CLASS_NAMES),\n            \"index\": tfds.features.Tensor(shape=(), dtype=tf.float32)\n        }),\n        supervised_keys=(\"features\", \"label\"),\n    )\n  def _split_generators(self, dl_manager):\n    paths = dict(\n      train='/path/to/train.csv',\n      test='/path/to/test.csv',\n    )\n    # better yet, if the csv files were originally downloaded, use\n    # urls = dict(train=train_url, test=test_url)\n    # paths = dl_manager.download(urls)\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=10,\n            gen_kwargs=dict(path=paths['train'])),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=2,\n            gen_kwargs=dict(cvs_path=paths['test']))\n    ]\n  def _generate_examples(self, csv_path):\n    with open(csv_path, 'r') as f:\n        for i, line in enumerate(f.readlines()):\n            record = line.rstrip().split(',')\n            features = [float(n) for n in record[:-1]]\n            label = int(record[-1])\n            yield dict(features=features, label=label, index=i)"
  },
  {
    "url": "https://stackoverflow.com/questions/39845982/customizing-django-admin-changeform-template-adding-custom-content",
    "body": "class MyObjectAdmin(admin.ModelAdmin):\n    # A template for a very customized change view:\n    change_form_template = 'admin/my_change_form.html'\n    def get_dynamic_info(self):\n        # ...\n        pass\n    def change_view(self, request, object_id, form_url='', extra_context=None):\n        extra_context = extra_context or {}\n        extra_context['osm_data'] = self.get_dynamic_info()\n        return super(MyObjectAdmin, self).change_view(\n            request, object_id, form_url, extra_context=extra_context,\n        )"
  },
  {
    "url": "https://stackoverflow.com/questions/22306341/python-sklearn-how-to-calculate-p-values",
    "body": "                           Logit Regression Results\n==============================================================================\nDep. Variable:                      y   No. Observations:               406723\nModel:                          Logit   Df Residuals:                   406710\nMethod:                           MLE   Df Model:                           12\nDate:                Fri, 12 Apr 2019   Pseudo R-squ.:                0.001661\nTime:                        16:48:45   Log-Likelihood:            -2.8145e+05\nconverged:                      False   LL-Null:                   -2.8192e+05\n                                        LLR p-value:                8.758e-193\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1            -0.0037      0.003     -1.078      0.281      -0.010       0.003"
  },
  {
    "url": "https://stackoverflow.com/questions/53587315/pandas-find-specific-value-in-entire-dataframe",
    "body": "small_df = pd.DataFrame({\"A\":list(range(500)), \"B\":list(range(500, 1000))})\nlarge_df = pd.DataFrame({\"A\":list(range(100000)), \"B\":list(range(100000, 200000))})\nlargest_df = pd.DataFrame({\"A\":list(range(1000000)), \"B\":list(range(1000000, 2000000))})\ndef filter_df_by_value_eq(df, value):\n    return df[df.eq(value).any(axis=1)]\ndef filter_df_by_value_ravel(df, value):\n    return df[(df.values.ravel() == value).reshape(df.shape).any(1)]\nIn [8]: %timeit filter_df_by_value_eq(small_df, 612)\n175 µs ± 1.01 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nIn [9]: %timeit filter_df_by_value_ravel(small_df, 612)\n78.9 µs ± 215 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\nIn [10]: %timeit filter_df_by_value_eq(large_df, 1502964)\n307 µs ± 2.21 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nIn [11]: %timeit filter_df_by_value_ravel(large_df, 1502964)\n1.56 ms ± 13.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nIn [12]: %timeit filter_df_by_value_eq(largest_df, 10502964)\n3.04 ms ± 66.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nIn [13]: %timeit filter_df_by_value_ravel(largest_df, 10502964)\n15.2 ms ± 43.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "url": "https://stackoverflow.com/questions/57249273/how-to-detect-paragraphs-in-a-text-document-image-for-a-non-consistent-text-stru",
    "body": "import cv2\nimport numpy as np\n# Load image, grayscale, Gaussian blur, Otsu's threshold\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(gray, (7,7), 0)\nthresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n# Create rectangular structuring element and dilate\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\ndilate = cv2.dilate(thresh, kernel, iterations=4)\n# Find contours and draw rectangle\ncnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    x,y,w,h = cv2.boundingRect(c)\n    cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\ncv2.imshow('thresh', thresh)\ncv2.imshow('dilate', dilate)\ncv2.imshow('image', image)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/55751368/python-how-to-pass-to-a-function-argument-type-of-a-class-object-typing",
    "body": "from typing import Type, TypeVar\nclass Vehicle:\n    def __init__(self):\n        print(\"Creating a %s\" % self.__class__.__name__)\n    def move(self):\n        print(\"This %s is moving…\" % self.__class__.__name__)\nTVehicle = TypeVar(\"TVehicle\", bound=Vehicle)\nclass Car(Vehicle):\n    def honk(self) -> None:\n        print(\"tuuuuut\")\nclass Bike(Vehicle):\n    def ring(self) -> None:\n        print(\"ring\")\nclass Dog:\n    def bark(self) -> None:\n        print(\"woof!\")\ndef move(v: Vehicle) -> None:\n    v.move()\ndef instantiate(class_to_instantiate: Type[TVehicle]) -> TVehicle:\n    return class_to_instantiate()  # create an instance\nmove(Bike())\nmove(Car())\ninstantiate(Bike).ring()\ninstantiate(Car).honk()\n#instantiate(Dog)"
  },
  {
    "url": "https://stackoverflow.com/questions/44850701/multiple-aiohttp-applications-running-in-the-same-process",
    "body": "import asyncio\nfrom aiohttp import web\nrunners = []\nasync def start_site(app, address='localhost', port=8080):\n    runner = web.AppRunner(app)\n    runners.append(runner)\n    await runner.setup()\n    site = web.TCPSite(runner, address, port)\n    await site.start()\nloop = asyncio.get_event_loop()\nloop.create_task(start_site(web.Application()))\nloop.create_task(start_site(web.Application(), port=8081))\nloop.create_task(start_site(web.Application(), port=8082))\ntry:\n    loop.run_forever()\nexcept:\n    pass\nfinally:\n    for runner in runners:\n        loop.run_until_complete(runner.cleanup())"
  },
  {
    "url": "https://stackoverflow.com/questions/71539448/using-different-pydantic-models-depending-on-the-value-of-fields",
    "body": "#1 Successful Response   #2 Validation error                   #3 Validation error\n\n# Request body           # Request body                        # Request body\n{                        {                                     {\n  \"model_type\": \"m1\",      \"model_type\": \"m1\",                   \"model_type\": \"m2\",\n  \"A\": \"string\",           \"A\": \"string\",                        \"A\": \"string\",\n  \"B\": 0,                  \"C\": \"string\",                        \"C\": \"string\",\n  \"C\": \"string\",           \"D\": \"string\"                         \"D\": \"string\"\n  \"D\": \"string\"          }                                     }\n}\n\n# Server response    \t # Server response                     # Server response\n200                      {                                     {\n                           \"detail\": [                           \"detail\": [\n                             {                                     {\n                               \"loc\": [                              \"loc\": [\n                                 \"body\",                               \"body\",\n                                 \"Model1\",                             \"Model2\",\n                                 \"B\"                                   \"E\"\n                               ],                                    ],\n                               \"msg\": \"field required\",              \"msg\": \"field required\",\n                               \"type\": \"value_error.missing\"         \"type\": \"value_error.missing\"\n                             }                                     },\n                           ]                                       {\n                         }                                           \"loc\": [\n                                                                       \"body\",\n                                                                       \"Model2\",\n                                                                       \"F\"\n                                                                     ],\n                                                                     \"msg\": \"field required\",\n                                                                     \"type\": \"value_error.missing\"\n                                                                   }\n                                                                 ]\n                                                               }"
  },
  {
    "url": "https://stackoverflow.com/questions/53394935/what-is-the-right-way-to-close-a-dask-localcluster",
    "body": "def load_and_predict(input_data_chunk):\n    model_path = '...' # On your disk, so accessible by all processes.\n    model = some_library.load_model(model_path)\n    labels, scores = model.predict(input_data_chunk, ...)\n    return np.array([labels, scores])\n# (not shown) Load `input_data`, a list of your 1M examples.\nimport dask.array as DaskArray\nda_input_data = DaskArray.from_array(input_data, chunks=(10_000,))\nprediction_results = None\nwith LocalCluster(n_workers=int(0.9 * mp.cpu_count()),\n    processes=True,\n    threads_per_worker=1,\n    memory_limit='2GB',\n    ip='tcp://localhost:9895',\n) as cluster, Client(cluster) as client:\n    prediction_results = da_input_data.map_blocks(load_and_predict).compute()\n# Combine prediction_results, which will be a list of Numpy arrays,\n# each with labels, scores for 10,000 examples."
  },
  {
    "url": "https://stackoverflow.com/questions/71814658/python-typing-does-typeddict-allow-additional-extra-keys",
    "body": "> m: Movie = dict(\n>       name='Alien',\n>       year=1979,\n>       director='Ridley Scott')  # error: Unexpected key 'director'\n[emphasis by me]\nThe typecheckers `mypy`, `pyre`,  and `pyright` implement this according to the specification.\nHowever, it is possible that a value with extra keys is accepted. This is because subtyping of TypedDicts is allowed, and the subtype might implement the extra key. PEP-589 only forbids extra keys in object construction, i.e. in literal assignment. As any value that complies with a subtype is always deemed to comply with the parent type and can be upcasted from the subtype to the parent type, an extra key can be introduced through a subtype:"
  },
  {
    "url": "https://stackoverflow.com/questions/71814658/python-typing-does-typeddict-allow-additional-extra-keys",
    "body": "In the example above, we see that the same value can sometimes be considered complying with `Movie` by the typing system, and sometimes not.\nAs a consequence of subtyping, typing a parameter as a certain TypedDict is not a safeguard against extra keys, because they could have been introduced through a subtype.\nIf your code is sensitive with regard to the presence of extra keys (for instance, if it makes use of `param.keys()`, `param.values()` or `len(param)` on the `TypedDict` parameter `param`), this could lead to problems when extra keys are present. A solution to this problem is to either handle the exceptional case that extra keys are actually present on the parameter or to make your code insensitive against extra keys.\nIf you want to test that your code is robust against extra keys, you cannot simply add a key in the test value:"
  },
  {
    "url": "https://stackoverflow.com/questions/62824000/can-i-have-an-optional-parameter-in-dataclasses-that-is-omitted-when-transformed",
    "body": "from dataclasses import asdict, dataclass\nfrom typing import List, Optional\nfrom validated_dc import ValidatedDC\n@dataclass\nclass SubOrder(ValidatedDC):\n    name: str\n@dataclass\nclass Order(ValidatedDC):\n    name: str\n    sub_orders: Optional[List[SubOrder]] = None\n    def as_dict(self):\n        data = asdict(self)\n        return {key: value for key, value in data.items() if value is not None}\ndata = {'name': 'pizza'}\norder = Order(**data)\nassert order.get_errors() is None\nassert asdict(order) == {'name': 'pizza', 'sub_orders': None}\nassert order.as_dict() == {'name': 'pizza'}\ndata = {'name': 'pizza', 'sub_orders': [{'name': 'pasta'}]}\norder = Order(**data)\nassert order.get_errors() is None\nassert asdict(order) == {'name': 'pizza', 'sub_orders': [{'name': 'pasta'}]}\nassert isinstance(order.sub_orders[0], SubOrder)"
  },
  {
    "url": "https://stackoverflow.com/questions/71371909/how-to-calculate-when-ones-10000-day-after-his-or-her-birthday-will-be",
    "body": "def is_it_a_leap_year(year) -> bool:\n    \"\"\"\n    Determine if a year is a leap year\n    Args:\n        year: int\n    Extended Summary:\n        According to:\n            https://airandspace.si.edu/stories/editorial/science-leap-year\n        The rule is that if the year is divisible by 100 and not divisible by\n        400, leap year is skipped. The year 2000 was a leap year, for example,\n        but the years 1700, 1800, and 1900 were not.  The next time a leap year\n        will be skipped is the year 2100.\n    \"\"\"\n    if year % 4 != 0:\n        return False\n    if year % 100 == 0 and year % 400 != 0:\n        return False\n    return True"
  },
  {
    "url": "https://stackoverflow.com/questions/71371909/how-to-calculate-when-ones-10000-day-after-his-or-her-birthday-will-be",
    "body": "def age_after_n_days(start_year: int,\n                     start_month: int,\n                     start_day: int,\n                     n_days: int) -> tuple:\n    \"\"\"\n    Calculate an approximate age of a person after a given number of days,\n    attempting to take into account leap years appropriately.\n    Return the number of days left until their next birthday\n    Args:\n        start_year (int): year of the start date\n        start_month (int): month of the start date\n        start_day (int): day of the start date\n        n_days (int): number of days to elapse\n    \"\"\"\n    # Check if the start date happens on a leap year and occurs before the\n    # 29 February (additional leap year day)\n    start_pre_leap = (is_it_a_leap_year(start_year) and start_month < 3)\n    # Account for the edge case where you start exactly on the 29 February\n    if start_month == 2 and start_day == 29:\n        start_pre_leap = False\n    # Keep a running counter of age\n    age = 0\n    # Store the \"current year\" whilst iterating through the days\n    current_year = start_year\n    # Count the number of days left\n    days_left = n_days\n    # While there is at least one year left to elapse...\n    while days_left > 364:\n        # Is it a leap year?\n        if is_it_a_leap_year(current_year):\n            # If not the first year\n            if age > 0:\n                days_left -= 366\n            # If the first year is a leap year but starting after the 29 Feb...\n            elif age == 0 and not start_pre_leap:\n                days_left -= 365\n            else:\n                days_left -= 366\n        # If not a leap year...\n        else:\n            days_left -= 365\n        # If the number of days left hasn't dropped below zero\n        if days_left >= 0:\n            # Increment age\n            age += 1\n            # Increment year\n            current_year += 1\n    return age, days_left"
  },
  {
    "url": "https://stackoverflow.com/questions/63679315/how-to-use-cython-with-poetry",
    "body": "import os\n# See if Cython is installed\ntry:\n    from Cython.Build import cythonize\n# Do nothing if Cython is not available\nexcept ImportError:\n    # Got to provide this function. Otherwise, poetry will fail\n    def build(setup_kwargs):\n        pass\n# Cython is installed. Compile\nelse:\n    from setuptools import Extension\n    from setuptools.dist import Distribution\n    from distutils.command.build_ext import build_ext\n    # This function will be executed in setup.py:\n    def build(setup_kwargs):\n        # The file you want to compile\n        extensions = [\n            \"mylibrary/myfile.py\"\n        ]\n        # gcc arguments hack: enable optimizations\n        os.environ['CFLAGS'] = '-O3'\n        # Build\n        setup_kwargs.update({\n            'ext_modules': cythonize(\n                extensions,\n                language_level=3,\n                compiler_directives={'linetrace': True},\n            ),\n            'cmdclass': {'build_ext': build_ext}\n        })"
  },
  {
    "url": "https://stackoverflow.com/questions/62066474/python-flask-automatically-generated-swagger-openapi-3-0",
    "body": "from apispec import APISpec\nfrom apispec.ext.marshmallow import MarshmallowPlugin\nfrom apispec_webframeworks.flask import FlaskPlugin\nfrom marshmallow import Schema, fields\nfrom flask import Flask, abort, request, make_response, jsonify\nfrom pprint import pprint\nimport json\nclass DemoParameter(Schema):\n    gist_id = fields.Int()\nclass DemoSchema(Schema):\n    id = fields.Int()\n    content = fields.Str()\nspec = APISpec(\n    title=\"Demo API\",\n    version=\"1.0.0\",\n    openapi_version=\"3.0.2\",\n    info=dict(\n        description=\"Demo API\",\n        version=\"1.0.0-oas3\",\n        contact=dict(\n            email=\"admin@donofden.com\"\n            ),\n        license=dict(\n            name=\"Apache 2.0\",\n            url='http://www.apache.org/licenses/LICENSE-2.0.html'\n            )\n        ),\n    servers=[\n        dict(\n            description=\"Test server\",\n            url=\"https://resources.donofden.com\"\n            )\n        ],\n    tags=[\n        dict(\n            name=\"Demo\",\n            description=\"Endpoints related to Demo\"\n            )\n        ],\n    plugins=[FlaskPlugin(), MarshmallowPlugin()],\n)\nspec.components.schema(\"Demo\", schema=DemoSchema)\n# spec.components.schema(\n#     \"Gist\",\n#     {\n#         \"properties\": {\n#             \"id\": {\"type\": \"integer\", \"format\": \"int64\"},\n#             \"name\": {\"type\": \"string\"},\n#         }\n#     },\n# )\n#\n# spec.path(\n#     path=\"/gist/{gist_id}\",\n#     operations=dict(\n#         get=dict(\n#             responses={\"200\": {\"content\": {\"application/json\": {\"schema\": \"Gist\"}}}}\n#         )\n#     ),\n# )\n# Extensions initialization\n# =========================\napp = Flask(__name__)\n@app.route(\"/demo/<gist_id>\", methods=[\"GET\"])\ndef my_route(gist_id):\n    \"\"\"Gist detail view.\n    ---\n    get:\n      parameters:\n      - in: path\n        schema: DemoParameter\n      responses:\n        200:\n          content:\n            application/json:\n              schema: DemoSchema\n        201:\n          content:\n            application/json:\n              schema: DemoSchema\n    \"\"\"\n    # (...)\n    return jsonify('foo')\n# Since path inspects the view and its route,\n# we need to be in a Flask request context\nwith app.test_request_context():\n    spec.path(view=my_route)\n# We're good to go! Save this to a file for now.\nwith open('swagger.json', 'w') as f:\n    json.dump(spec.to_dict(), f)\npprint(spec.to_dict())\nprint(spec.to_yaml())"
  },
  {
    "url": "https://stackoverflow.com/questions/48524196/how-to-get-python-to-run-python-3-in-wsl-bash",
    "body": "cameron@Nook:/mnt/c/Users/camer$ sudo apt install python-is-python3\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nThe following NEW packages will be installed:\n  python-is-python3\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 2364 B of archives.\nAfter this operation, 10.2 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/main amd64 python-is-python3 all 3.8.2-4 [2364 B]\nFetched 2364 B in 0s (7208 B/s)\nSelecting previously unselected package python-is-python3.\n(Reading database ... 33571 files and directories currently installed.)\nPreparing to unpack .../python-is-python3_3.8.2-4_all.deb ...\nUnpacking python-is-python3 (3.8.2-4) ...\nSetting up python-is-python3 (3.8.2-4) ...\ncameron@Nook:/mnt/c/Users/camer$ python --version\nPython 3.8.10"
  },
  {
    "url": "https://stackoverflow.com/questions/73563804/what-is-the-recommended-way-to-instantiate-and-pass-around-a-redis-client-with-f",
    "body": "from fastapi import Depends, FastAPI\nimport redis\nfrom config.db import pool\napp = FastAPI()\ndef get_redis():\n  # Here, we re-use our connection pool\n  # not creating a new one\n  return redis.Redis(connection_pool=pool)\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, cache = Depends(get_redis)):\n  status = cache.get(item_id)\n  return {\"item_name\": status}\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int, cache = Depends(get_redis)):\n  cache.set(item_id, \"available\")\n  return {\"status\": \"available\", \"item_id\": item_id}"
  },
  {
    "url": "https://stackoverflow.com/questions/64118680/reload-flag-with-uvicorn-can-we-exclude-certain-code",
    "body": "--reload-include TEXT           Set glob patterns to include while watching\n                                  for files. Includes '*.py' by default, which\n                                  can be overridden in reload-excludes.\n  --reload-exclude TEXT           Set glob patterns to exclude while watching\n                                  for files. Includes '.*, .py[cod], .sw.*,\n                                  ~*' by default, which can be overridden in\n                                  reload-excludes."
  },
  {
    "url": "https://stackoverflow.com/questions/35785962/python-how-do-i-find-which-pip-package-a-library-belongs-to",
    "body": ">>> from importlib.metadata import packages_distributions\n>>> packages_distributions()\n 'asttokens': ['asttokens'],\n 'backcall': ['backcall'],\n 'bitarray': ['bitarray'],\n 'colorama': ['colorama'],\n 'decorator': ['decorator'],\n 'executing': ['executing'],\n 'importlib_metadata': ['importlib-metadata'],\n 'impala': ['impyla'],\n 'IPython': ['ipython'],\n 'jedi': ['jedi'],\n 'matplotlib_inline': ['matplotlib-inline'],\n 'parso': ['parso'],\n 'pickleshare': ['pickleshare'],\n 'pip': ['pip'],\n 'prompt_toolkit': ['prompt-toolkit'],\n 'pure_eval': ['pure-eval'],\n 'puresasl': ['pure-sasl'],\n 'pygments': ['Pygments'],\n '_distutils_hack': ['setuptools'],\n 'pkg_resources': ['setuptools'],\n 'setuptools': ['setuptools'],\n 'six': ['six'],\n 'stack_data': ['stack-data'],\n 'thrift': ['thrift'],\n 'thrift_sasl': ['thrift-sasl'],\n 'traitlets': ['traitlets'],\n 'wcwidth': ['wcwidth'],\n 'zipp': ['zipp']}"
  },
  {
    "url": "https://stackoverflow.com/questions/76459034/how-to-load-a-fine-tuned-peft-lora-model-based-on-llama-with-huggingface-transfo",
    "body": "import torch\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\nmodel_name = \"decapoda-research/llama-7b-hf\"\nadapters_name = \"lucas0/empath-llama-7b\"\nprint(f\"Starting to load the model {model_name} into memory\")\nm = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    #load_in_4bit=True,\n    torch_dtype=torch.bfloat16,\n    device_map={\"\": 0}\n)\nm = PeftModel.from_pretrained(m, adapters_name)\nm = m.merge_and_unload()\ntok = LlamaTokenizer.from_pretrained(model_name)\ntok.bos_token_id = 1\nstop_token_ids = [0]\nprint(f\"Successfully loaded the model {model_name} into memory\")"
  },
  {
    "url": "https://stackoverflow.com/questions/59725560/finding-all-the-combinations-of-free-polyominoes-within-a-specific-area-with-a-s",
    "body": "from ortools.sat.python import cp_model\n(W, H) = (3, 3) # Width and height of our grid.\n(X, Y) = (0, 1) # Convenience constants.\ndef main():\n  model = cp_model.CpModel()\n  # Create an Int var for each block of each shape constrained to be within width and height of grid.\n  shapes = [\n    [\n      [ model.NewIntVar(0, W, 's1b1_x'), model.NewIntVar(0, H, 's1b1_y') ],\n      [ model.NewIntVar(0, W, 's1b2_x'), model.NewIntVar(0, H, 's1b2_y') ],\n      [ model.NewIntVar(0, W, 's1b3_x'), model.NewIntVar(0, H, 's1b3_y') ],\n    ],\n    [\n      [ model.NewIntVar(0, W, 's2b1_x'), model.NewIntVar(0, H, 's2b1_y') ],\n      [ model.NewIntVar(0, W, 's2b2_x'), model.NewIntVar(0, H, 's2b2_y') ],\n    ]\n  ]\n  # Define the shapes by constraining the blocks relative to each other.\n  # 3x1 rectangle:\n  s0 = shapes[0]\n  model.Add(s0[0][Y] == s0[1][Y])\n  model.Add(s0[0][Y] == s0[2][Y])\n  model.Add(s0[0][X] == s0[1][X] - 1)\n  model.Add(s0[0][X] == s0[2][X] - 2)\n  # 1x2 rectangle:\n  s1 = shapes[1]\n  model.Add(s1[0][X] == s1[1][X])\n  model.Add(s1[0][Y] == s1[1][Y] - 1)\n  # No blocks can overlap:\n  block_addresses = []\n  for i, block in enumerate(blocks(shapes)):\n    block_address = model.NewIntVar(0, (W+1)*(H+1), 'b%d' % (i,))\n    model.Add(block[X] + (H+1)*block[Y] == block_address)\n    block_addresses.append(block_address)\n  model.AddAllDifferent(block_addresses)\n  # Solve and print solutions as we find them\n  solver = cp_model.CpSolver()\n  solution_printer = SolutionPrinter(shapes)\n  status = solver.SearchForAllSolutions(model, solution_printer)\n  print('Status = %s' % solver.StatusName(status))\n  print('Number of solutions found: %i' % solution_printer.count)\ndef blocks(shapes):\n  ''' Helper to enumerate all blocks. '''\n  for shape in shapes:\n    for block in shape:\n      yield block\nclass SolutionPrinter(cp_model.CpSolverSolutionCallback):\n    ''' Print a solution. '''\n    def __init__(self, variables):\n        cp_model.CpSolverSolutionCallback.__init__(self)\n        self.variables = variables\n        self.count = 0\n    def on_solution_callback(self):\n      self.count += 1\n      solution = [(self.Value(block[X]), self.Value(block[Y])) for shape in self.variables for block in shape]\n      print((W+3)*'-')\n      for y in range(0, H+1):\n        print('|' + ''.join(['#' if (x,y) in solution else ' ' for x in range(0, W+1)]) + '|')\n      print((W+3)*'-')\nif __name__ == '__main__':\n  main()"
  },
  {
    "url": "https://stackoverflow.com/questions/59725560/finding-all-the-combinations-of-free-polyominoes-within-a-specific-area-with-a-s",
    "body": "import numpy as np\nfrom copy import copy\nfrom tabulate import tabulate\nD = 4 # Dimension of square grid.\nKCC = [5,4,2,2] # List of the sizes of the required k connected components (KCCs).\nassert(sum(KCC) <= D*D)\nVALID_CELLS = range(2,D*D)\ndef search():\n  solutions = set() # Stash of unique solutions.\n  for start in VALID_CELLS: # Try starting search from each possible starting point and expand out.\n    marked = np.zeros(D*D).tolist()\n    _search(start, marked, set(), solutions, 0, 0)\n  for solution in solutions:  # Print results.\n    print(tabulate(np.array(solution).reshape(D, D)))\n  print('Number of solutions found:', len(solutions))\ndef _search(i, marked, fringe, solutions, curr_count, curr_part):\n  ''' Recursively find each possible KCC in the remaining available cells the find the next, until none left '''\n  marked[i] = curr_part+1\n  curr_count += 1\n  if curr_count == KCC[curr_part]: # If marked K cells for the current CC move onto the next one.\n    curr_part += 1\n    if curr_part == len(KCC): # If marked K cells and there's no more CCs left we have a solution - not necessarily unique.\n      solutions.add(tuple(marked))\n    else:\n      for start in VALID_CELLS:\n        if marked[start] == 0:\n          _search(start, copy(marked), set(), solutions, 0, curr_part)\n  else:\n    fringe.update(neighbours(i, D))\n    while(len(fringe)):\n      j = fringe.pop()\n      if marked[j] == 0:\n        _search(j, copy(marked), copy(fringe), solutions, curr_count, curr_part)\ndef neighbours(i, D):\n  ''' Find the address of all cells neighbouring the i-th cell in a DxD grid. '''\n  row = int(i/D)\n  n = []\n  n += [i-1] if int((i-1)/D) == row and (i-1) >= 0 else []\n  n += [i+1] if int((i+1)/D) == row and (i+1) < D**2 else []\n  n += [i-D] if (i-D) >=0 else []\n  n += [i+D] if (i+D) < D**2 else []\n  return filter(lambda x: x in VALID_CELLS, n)\nif __name__ == '__main__':\n  search()"
  },
  {
    "url": "https://stackoverflow.com/questions/70383316/pydantic-constr-vs-field-args",
    "body": "    strip_whitespace: bool = False: removes leading and trailing whitespace\n    to_lower: bool = False: turns all characters to lowercase\n    to_upper: bool = False: turns all characters to uppercase\n    strict: bool = False: controls type coercion\n    min_length: int = None: minimum length of the string\n    max_length: int = None: maximum length of the string\n    curtail_length: int = None: shrinks the string length to the set value when it is longer than the set value\n    regex: str = None: regex to validate the string against"
  },
  {
    "url": "https://stackoverflow.com/questions/67750367/fastapi-python-code-execution-speed-impacted-by-deployment-with-uvicorn-vs-gunic",
    "body": "import asyncio, time\nfrom fastapi import FastAPI, Path\nfrom datetime import datetime\nimport statistics\napp = FastAPI()\n@app.get(\"/delay/{delay1}/{delay2}\")\nasync def get_delay(\n    delay1: float = Path(..., title=\"Nonblocking time taken to respond\"),\n    delay2: float = Path(..., title=\"Blocking time taken to respond\"),\n):\n    total_start_time = datetime.now()\n    times = []\n    for i in range(100):\n        start_time = datetime.now()\n        await asyncio.sleep(delay1)\n        time.sleep(delay2)\n        time_delta= (datetime.now()-start_time).microseconds\n        times.append(time_delta)\n    times_average = statistics.mean(times)\n    return {\"delays\":[delay1,delay2],\"total_time_taken\":(datetime.now()-total_start_time).microseconds,\"times_avarage\":times_average,\"times\":times}"
  },
  {
    "url": "https://stackoverflow.com/questions/67750367/fastapi-python-code-execution-speed-impacted-by-deployment-with-uvicorn-vs-gunic",
    "body": "# `uvicorn performance_test:app --port 8083`\n{\"delays\":[0.0,0.0],\"total_time_taken\":553,\"times_avarage\":4.4,\"times\":[15,7,5,4,4,4,4,5,5,4,4,5,4,4,5,4,4,5,4,4,5,4,4,5,4,4,4,5,4,4,5,4,4,5,4,4,4,4,4,5,4,5,5,4,4,4,4,4,4,5,4,4,4,5,4,4,4,4,4,4,5,4,4,5,4,4,4,4,5,4,4,5,4,4,4,4,4,5,4,4,5,4,4,5,4,4,5,4,4,4,4,4,4,4,5,4,4,4,5,4]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":575,\"times_avarage\":4.61,\"times\":[15,6,5,5,5,5,5,5,5,5,5,4,5,5,5,5,4,4,4,4,4,5,5,5,4,5,4,4,4,5,5,5,4,5,5,4,4,4,4,5,5,5,5,4,4,4,4,5,5,4,4,4,4,4,4,4,4,5,5,4,4,4,4,5,5,5,5,5,5,5,4,4,4,4,5,5,4,5,5,4,4,4,4,4,4,5,5,5,4,4,4,4,5,5,5,5,4,4,4,4]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":548,\"times_avarage\":4.31,\"times\":[14,6,5,4,4,4,4,4,4,4,5,4,4,4,4,4,4,5,4,4,5,4,4,4,4,4,4,4,5,4,4,4,5,4,4,4,4,4,4,4,4,5,4,4,4,4,4,4,5,4,4,4,4,4,5,5,4,4,4,4,4,4,4,5,4,4,4,4,4,5,4,4,5,4,4,5,4,4,5,4,4,4,4,4,4,4,5,4,4,5,4,4,5,4,4,5,4,4,4,4]}\n# `gunicorn performance_test:app -b localhost:8084 -k uvicorn.workers.UvicornWorker --workers 1`\n{\"delays\":[0.0,0.0],\"total_time_taken\":551,\"times_avarage\":4.34,\"times\":[13,6,5,5,5,5,5,4,4,4,5,4,4,4,4,4,5,4,4,5,4,4,5,4,4,4,4,4,5,4,4,4,4,4,5,4,4,4,4,4,4,4,5,4,4,5,4,4,4,4,4,4,4,4,5,4,4,4,4,4,4,4,5,4,4,4,4,4,4,4,4,4,5,4,4,5,4,5,4,4,5,4,4,4,4,5,4,4,5,4,4,4,4,4,4,4,5,4,4,5]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":558,\"times_avarage\":4.48,\"times\":[14,7,5,5,5,5,5,5,4,4,4,4,4,4,5,5,4,4,4,4,5,4,4,4,5,5,4,4,4,5,5,4,4,4,5,4,4,4,5,5,4,4,4,4,5,5,4,4,5,5,4,4,5,5,4,4,4,5,4,4,5,4,4,5,5,4,4,4,5,4,4,4,5,4,4,4,5,4,5,4,4,4,5,4,4,4,5,4,4,4,5,4,4,4,5,4,4,4,5,4]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":550,\"times_avarage\":4.34,\"times\":[15,6,5,4,4,4,4,4,4,5,4,4,4,4,4,5,4,4,5,4,4,5,4,4,4,4,4,5,4,4,4,4,5,5,4,4,4,4,5,4,4,4,4,4,5,4,4,5,4,4,5,4,4,5,4,4,5,4,4,5,4,4,4,4,4,4,5,4,4,5,4,4,4,4,4,4,4,4,4,5,4,4,5,4,4,4,4,4,4,4,4,5,4,4,5,4,4,4,4,4]}"
  },
  {
    "url": "https://stackoverflow.com/questions/67750367/fastapi-python-code-execution-speed-impacted-by-deployment-with-uvicorn-vs-gunic",
    "body": "# `uvicorn performance_test:app --port 8083`\n{\"delays\":[0.0,0.0],\"total_time_taken\":159,\"times_avarage\":0.6,\"times\":[3,1,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,0]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":162,\"times_avarage\":0.49,\"times\":[3,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":156,\"times_avarage\":0.61,\"times\":[3,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1]}\n# `gunicorn performance_test:app -b localhost:8084 -k uvicorn.workers.UvicornWorker --workers 1`\n{\"delays\":[0.0,0.0],\"total_time_taken\":159,\"times_avarage\":0.59,\"times\":[2,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":165,\"times_avarage\":0.62,\"times\":[3,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1]}\n{\"delays\":[0.0,0.0],\"total_time_taken\":164,\"times_avarage\":0.54,\"times\":[2,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1]}"
  },
  {
    "url": "https://stackoverflow.com/questions/67750367/fastapi-python-code-execution-speed-impacted-by-deployment-with-uvicorn-vs-gunic",
    "body": "import statistics\nimport requests\nfrom time import sleep\nnumber_of_tests=1000\nsites_to_test=[\n    {\n        'name':'only uvicorn    ',\n        'url':'http://127.0.0.1:8083/delay/0.0/0.0'\n    },\n    {\n        'name':'gunicorn+uvicorn',\n        'url':'http://127.0.0.1:8084/delay/0.0/0.0'\n    }]\nfor test in sites_to_test:\n    total_time_taken_list=[]\n    times_avarage_list=[]\n    requests.get(test['url']) # first request may be slower, so better to not measure it\n    for a in range(number_of_tests):\n        r = requests.get(test['url'])\n        json= r.json()\n        total_time_taken_list.append(json['total_time_taken'])\n        times_avarage_list.append(json['times_avarage'])\n        # sleep(1) # results are slightly different with sleep between requests\n    total_time_taken_avarage=statistics.mean(total_time_taken_list)\n    times_avarage_avarage=statistics.mean(times_avarage_list)\n    print({'name':test['name'], 'number_of_tests':number_of_tests, 'total_time_taken_avarage':total_time_taken_avarage, 'times_avarage_avarage':times_avarage_avarage})"
  },
  {
    "url": "https://stackoverflow.com/questions/67750367/fastapi-python-code-execution-speed-impacted-by-deployment-with-uvicorn-vs-gunic",
    "body": "import statistics\nimport requests\nfrom time import sleep\nnumber_of_tests=1000\nsites_to_test=[\n    {\n        'name':'only uvicorn    ',\n        'url':'http://127.0.0.1:8083/delay/0.0/0.0',\n        'total_time_taken_list':[],\n        'times_avarage_list':[]\n    },\n    {\n        'name':'gunicorn+uvicorn',\n        'url':'http://127.0.0.1:8084/delay/0.0/0.0',\n        'total_time_taken_list':[],\n        'times_avarage_list':[]\n    }]\nfor test in sites_to_test:\n    requests.get(test['url']) # first request may be slower, so better to not measure it\nfor a in range(number_of_tests):\n    for test in sites_to_test:\n        r = requests.get(test['url'])\n        json= r.json()\n        test['total_time_taken_list'].append(json['total_time_taken'])\n        test['times_avarage_list'].append(json['times_avarage'])\n        # sleep(1) # results are slightly different with sleep between requests\nfor test in sites_to_test:\n    total_time_taken_avarage=statistics.mean(test['total_time_taken_list'])\n    times_avarage_avarage=statistics.mean(test['times_avarage_list'])\n    print({'name':test['name'], 'number_of_tests':number_of_tests, 'total_time_taken_avarage':total_time_taken_avarage, 'times_avarage_avarage':times_avarage_avarage})"
  },
  {
    "url": "https://stackoverflow.com/questions/71253495/how-to-annotate-the-type-of-arguments-forwarded-to-another-function",
    "body": "from typing import TypeVar, ParamSpec, Callable, Optional\nT = TypeVar('T')\nP = ParamSpec('P')\ndef take_annotation_from(this: Callable[P, Optional[T]]) -> Callable[[Callable], Callable[P, Optional[T]]]:\n    def decorator(real_function: Callable) -> Callable[P, Optional[T]]:\n        def new_function(*args: P.args, **kwargs: P.kwargs) -> Optional[T]:\n            return real_function(*args, **kwargs)\n        return new_function\n    return decorator\n@take_annotation_from(open)\ndef open_for_writing(*args, **kwargs):\n    kwargs['mode'] = 'w'\n    return open(*args, **kwargs)\nopen_for_writing(some_fake_arg=123)\nopen_for_writing(file='')"
  },
  {
    "url": "https://stackoverflow.com/questions/74968585/using-environment-variables-in-pyproject-toml-for-versioning",
    "body": "# pyproject.toml\n[project]\nname = \"hello-world\"\nreadme = \"README.md\"\nauthors = [...]\ndependencies = [...]\ndynamic = [\"version\"]\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta:__legacy__\"\n# The difference between `__legacy__` and the regular `build_meta`\n# is that `__legacy__` does the equivalent of\n# `sys.path.insert(0, os.path.dirname(__file__))`.\n# This allows you to `import` your modules from the `CWD`.\n# If you don't like using `__legacy__` you can\n# manually add `CWD` to `sys.path` inside your `setup.py`."
  },
  {
    "url": "https://stackoverflow.com/questions/54060274/dynamic-communication-between-main-and-subprocess-in-python",
    "body": "import os\ndef child():\n    \"\"\"This function is executed in a child process.\"\"\"\n    infile = os.fdopen(r1)\n    outfile = os.fdopen(w2, 'w', buffering=1)\n    for line in infile:\n        if line.rstrip() == 'quit':\n            break\n        print(line.upper(), end='', file=outfile)\ndef parent():\n    \"\"\"This function is executed in a parent process.\"\"\"\n    outfile = os.fdopen(w1, 'w', buffering=1)\n    infile = os.fdopen(r2)\n    print('Foo', file=outfile)\n    print(infile.readline(), end='')\n    print('bar', file=outfile)\n    print(infile.readline(), end='')\n    print('quit', file=outfile)\n(r1, w1) = os.pipe()  # for parent -> child writes\n(r2, w2) = os.pipe()  # for child -> parent writes\npid = os.fork()\nif pid == 0:\n    child()  # child code runs here\nelif pid > 0:\n    parent()  # parent code runs here\n    os.waitpid(pid, 0)  # wait for child\nelse:\n    raise RuntimeError(\"This should not have happened.\")"
  },
  {
    "url": "https://stackoverflow.com/questions/49350821/runtimewarning-coroutine-was-never-awaited-in-tests",
    "body": "========================================================================================================================================================================= test session starts ==========================================================================================================================================================================\nplatform darwin -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1\nrootdir: /Users/ldu020/workspace/github.com/mrdulin/python-codelab\nplugins: asyncio-0.10.0\ncollected 1 item\nsrc/stackoverflow/49350821/test_post_helpers.py F                                                                                                                                                                                                                                                                                                                [100%]\n=============================================================================================================================================================================== FAILURES ===============================================================================================================================================================================\n_________________________________________________________________________________________________________________________________________________________________________ test_get_post_exists _________________________________________________________________________________________________________________________________________________________________________\n    @pytest.mark.asyncio\n    async def test_get_post_exists():\n        returned_post = await posts.get_post('0')\n        assert returned_post.id == 0\n        assert returned_post.text == 'Text for the post body.'\n>       assert True == False\nE       assert True == False\nsrc/stackoverflow/49350821/test_post_helpers.py:11: AssertionError\n========================================================================================================================================================================== 1 failed in 0.15s ==========================================================================================================================================================================="
  },
  {
    "url": "https://stackoverflow.com/questions/78574898/how-to-find-base-line-of-curved-text",
    "body": "import cv2\nimport math\nimport uuid\nimport numpy as np\nfrom scipy import stats\ndef resizeImageByPercentage(img,scalePercent = 60):\n    width = int(img.shape[1] * scalePercent / 100)\n    height = int(img.shape[0] * scalePercent / 100)\n    dim = (width, height)\n    # resize image\n    return cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\ndef calcMedianContourWithAndHeigh(contourList):\n    hs = list()\n    ws = list()\n    for cnt in contourList:\n        (x, y, w, h) = cv2.boundingRect(cnt)\n        ws.append(w)\n        hs.append(h)\n    return np.median(ws),np.median(hs)\ndef calcCentroid(contour):\n    houghMoments = cv2.moments(contour)\n    # calculate x,y coordinate of centroid\n    if houghMoments[\"m00\"] != 0: #case no contour could be calculated\n        cX = int(houghMoments[\"m10\"] / houghMoments[\"m00\"])\n        cY = int(houghMoments[\"m01\"] / houghMoments[\"m00\"])\n    else:\n    # set values as what you need in the situation\n        cX, cY = -1, -1\n    return cX,cY\ndef applyDilateImgFilter(img,kernelSize= 3,iterations=1):\n    img_bin = 255 - img #invert\n    kernel = np.ones((kernelSize,kernelSize),np.uint8)\n    img_dilated = cv2.dilate(img_bin, kernel, iterations = iterations)\n    return (255- img_dilated) #invert back\ndef randomColor():\n    return tuple(np.random.randint(0, 255, 3).tolist())\ndef drawGaussianValuesInsideRange(start, end, center, stdDev, amountValues):\n    values = []\n    if center < 0:\n        return values\n    if start > end:\n        return values\n    while len(values) < amountValues:\n        valueListPotencial = np.random.normal(center, stdDev, amountValues)\n        valueListFiltered = [value for value in valueListPotencial if start <= value <= end]\n        values.extend(valueListFiltered)\n    return values[:amountValues]\ndef drawRandomPointsInPolygon(amountPoints, cntFactObj):\n    pointList = list()\n    if not isinstance(cntFactObj, ContourFacts):\n        return pointList\n    #we calc basic parameter from random point selection\n    horizontalStart = cntFactObj.x\n    horizontalEnd = cntFactObj.x + cntFactObj.w\n    verticalStart = cntFactObj.y\n    verticalEnd = cntFactObj.y + cntFactObj.h\n    #calc std deviation connected to length and ratio\n    horitonalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (horizontalEnd-horizontalStart)\n    verticalStdDeviation = 1 / cntFactObj.ratioHeightoWidth * (verticalEnd-verticalStart)\n    while len(pointList)<amountPoints:\n        if cntFactObj.centoird[0] < 0 or cntFactObj.centoird[1] < 0:\n            return pointList\n        drawXValues = drawGaussianValuesInsideRange(horizontalStart, horizontalEnd, cntFactObj.centoird[0],\n                                          horitonalStdDeviation, amountPoints)\n        drawYValues = drawGaussianValuesInsideRange(verticalStart, verticalEnd, cntFactObj.centoird[1],\n                                         verticalStdDeviation, amountPoints)\n        #we create the points and check if they are inside the polygon\n        for i in range(0,len(drawXValues)):\n            #create points\n            point = (drawXValues[i],drawYValues[i])\n            # check if the point is inside the polygon\n            if cv2.pointPolygonTest(cntFactObj.contour, point, False) > 0:\n                pointList.append(point)\n    return pointList[:amountPoints]\ndef drawCountourOn(img,contours,color=None):\n    imgContour = img.copy()\n    for i in range(len(contours)):\n        if color is None:\n            color = randomColor()\n        cv2.drawContours(imgContour, contours, i, color, 2)\n    return imgContour\nDEBUGMODE = True\nfileIn = \"bZzzEeCU.jpg\"#\"269aSnEM.jpg\"\nimg = cv2.imread(fileIn)\n## A) apply filters to merge letters to words\n# prepare img load\nimgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n#gaussian filter\nimgGaussianBlur = cv2.GaussianBlur(imgGrey,(3,3),1)\n#make binary img, black and white via filter\n_, imgBinThres = cv2.threshold(imgGaussianBlur, 140, 230, cv2.THRESH_BINARY)\nif DEBUGMODE:\n    cv2.imwrite(\"img01bw.jpg\",resizeImageByPercentage(imgBinThres,30))\n## 3 steps merged by helper class ContourFacts\n## B) select contours of words (filter by: ratio heights vs widths , area size)\n## C) get random points from wordcontours with gaussian distribution and center point centroid of contour\n## D) use linear regression to find middle line of wordcontours\n#apply dilate filter to merge letter to words\nimgDilated = applyDilateImgFilter(imgBinThres,5,3)\nif DEBUGMODE:\n    cv2.imwrite(\"img02dilated.jpg\",resizeImageByPercentage(imgDilated,30))\n# detect contours\ncontourList, _ = cv2.findContours(imgDilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\nif DEBUGMODE:\n    imgContour = drawCountourOn(img,contourList)\n    cv2.imwrite(\"img03contourAll.jpg\",resizeImageByPercentage(imgContour,30))\n\n#do a selection of contours by rule\n#A) ratio h vs w\n#B) area size\nmediaWordWidth, medianWordHigh = calcMedianContourWithAndHeigh(contourList)\nprint(\"median word width: \", mediaWordWidth)\nprint(\"median word high: \", medianWordHigh)\ncontourSelectedByRatio=list()\n#we calc for every contour ratio h vs w\nratioThresholdHeightToWidth = 1.1 #thresold ratio should be a least be 1 to 1\n# e.g word to -->  10 pixel / 13 pixel\n#helper class for contour atrributess\nclass ContourFacts:\n    def __init__(self,contour):\n        if contour is None:\n            return\n        self.uid = uuid.uuid4()\n        (self.x, self.y, self.w, self.h) = cv2.boundingRect(contour)\n        self.minRect = cv2.minAreaRect(contour)\n        self.angle = self.minRect[-1]\n        _, (rectWidth, rectHeight), _ = self.minRect\n        self.minRectArea = rectWidth * rectHeight\n        self.ratioHeightoWidth = self.h / self.w\n        self.contour = contour\n        self.centoird = calcCentroid(contour)\n        self.randomPoinsInCnt = self.DrawRandomPoints()\n        if len(self.randomPoinsInCnt) > 0:\n            (self.bottomSlope, self.bottomIntercept) = self.EstimateCenterLineViaLinearReg()\n            self.bottomMinX = min([x for x,y in self.randomPoinsInCnt])\n            self.bottomMaxX = max([x for x,y in self.randomPoinsInCnt])\n    def EstimateCenterLineViaLinearReg(self):\n        if self.contour is None:\n            return (0,0)\n        slope = 0\n        intercept = 0\n        #model = slope (x) + intercept\n        xValues = [x for x,y in self.randomPoinsInCnt]\n        yValues = [y for x,y in self.randomPoinsInCnt]\n        if len(xValues) < 2:\n            return (0,0)\n        elif len(xValues) ==2:\n            #we calc a line with 2 points\n            # y = m*x + b\n            deltaX = xValues[1]-xValues[0]\n            if deltaX == 0:\n                return (0,0)\n            slope = (yValues[1]-yValues[0])/(deltaX)\n            intercept = yValues[0] - (slope*xValues[0])\n        else:\n            #normal linear regression above 2 points\n            slope, intercept, r, p, std_err = stats.linregress(xValues, yValues)\n        #TODO check std_err\n        return slope, intercept\n\n    def DrawRandomPoints(self,pointFactor=2):\n        pointList = list()\n        #calc area to amount point relation  -> bigger area more points\n        amountPointsNeeded = int(self.minRectArea/pointFactor)\n        pointList = drawRandomPointsInPolygon(amountPointsNeeded,self)\n        return pointList\n\n    def GetCenterLineLeftCorner(self):\n        if self.contour is None or len(self.randomPoinsInCnt) == 0:\n            return (0,0)\n        # calc via  y = m*x + b with min\n        return (int(self.bottomMinX), int(self.bottomSlope*self.bottomMinX + self.bottomIntercept))\n    def GetCenterLineRightCorner(self):\n        if self.contour is None or len(self.randomPoinsInCnt) == 0:\n            return (0,0)\n        # calc via via y = m*x + b with max\n        return (int(self.bottomMaxX), int(self.bottomSlope*self.bottomMaxX + self.bottomIntercept))\n    def __eq__(self, other):\n        if isinstance(other, ContourFacts):\n            return self.uid == other.uid\n        return False\n    def __hash__(self):\n        return hash(self.uid)\n#calc mean area size from area size\nvectorOfAreaSize = np.array([cv2.contourArea(cnt) for cnt in contourList])\nmeanAreaSize = np.mean(vectorOfAreaSize)\nprint(\"mean area size: \", meanAreaSize)\nstdDevAreaSize = np.std(vectorOfAreaSize)\nprint(\"std dev area size: \", stdDevAreaSize)\nthresoldDiffAreaSize = stdDevAreaSize/4\n#we iterate all contours and select by ratio and size\nfor cnt in contourList:\n    #construct helper class instance\n    contourFactObj = ContourFacts(cnt)\n    #calc abs diff to mean area size\n    diffArea = abs(cv2.contourArea(cnt) - meanAreaSize)\n    if contourFactObj.ratioHeightoWidth < ratioThresholdHeightToWidth and diffArea < (thresoldDiffAreaSize):\n        contourSelectedByRatio.append(contourFactObj)\n#debug print\nif DEBUGMODE:\n    #we print words\n    imgContourSelection = img.copy()\n    for cnt in contourSelectedByRatio:\n        contourColor = randomColor()\n        imgContourSelection = drawCountourOn(imgContourSelection,[cnt.contour],contourColor)\n        #we print centroid\n        cv2.circle(imgContourSelection, cnt.centoird, 5, (0, 0, 255), -1)\n        p1 = cnt.GetCenterLineLeftCorner()\n        p2 = cnt.GetCenterLineRightCorner()\n        if p1 != (0,0) or p2 != (0,0):\n            cv2.circle(imgContourSelection, p1, 5, (0, 0, 255), -1)\n            cv2.circle(imgContourSelection, p2, 5, (0, 0, 255), -1)\n            cv2.line(imgContourSelection, p1, p2, (0, 255, 0), 2)\n    cv2.imwrite(\"img04contourSelection.jpg\",resizeImageByPercentage(imgContourSelection,30))\n## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together)\n#define distance function, differences in height is negativ weighted\ndef euclidianDistanceWithNegativHeightWeight(cnt1,cnt2,negativeHeightWeight=2.0):\n    if cnt1 is None or cnt2 is None:\n        return 1000000\n    if not isinstance(cnt1, ContourFacts) or not isinstance(cnt2, ContourFacts):\n        return 1000000\n    p1 = cnt1.GetCenterLineRightCorner()\n    p2 = cnt2.GetCenterLineLeftCorner()\n    return math.sqrt((p2[0] - p1[0])**2 + (negativeHeightWeight*(p2[1] - p1[1]))**2)\n# helper class to group contours\nclass ContourGroup:\n    def __init__(self):\n        self.uuid = uuid.uuid4()\n        self.contourList = list()\n    def GetLastElement(self):\n        if len(self.contourList) == 0:\n            return None\n        return self.contourList[-1]\n    def Add(self,cnt):\n        self.contourList.append(cnt)\n    def __eq__(self, other):\n        if isinstance(other, ContourGroup):\n            return self.uuid == other.uuid\n        return False\n\ngroupMap = dict()\nlineGroupList = list()\n## we grouping the contours to lines\nmaxDistanceThresholNextWord= medianWordHigh *0.9 #TODO get better estimate\n#recursive function to get nearest neighbors\ndef getNearestNeighbors(cnt1,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord):\n    maxDepth = 10 #var for max recursion depth\n    nearestCnt = None\n    nearestDist = maxDistanceThresholNextWord\n    for j in range(0,len(contourSelectedByRatio)):\n        cnt2 = contourSelectedByRatio[j]\n        if cnt1 == cnt2:#skip same\n            continue\n        dist = euclidianDistanceWithNegativHeightWeight(cnt1,cnt2)\n        if dist < nearestDist:\n            nearestDist = dist\n            nearestCnt = cnt2\n    if nearestCnt is not None:#call recursive\n        nearaestListWeHave = [nearestCnt] #new list\n        depthCounter += 1\n        if depthCounter < maxDepth:# all to call\n            nearListWeGet =getNearestNeighbors(nearestCnt,depthCounter,contourSelectedByRatio,maxDistanceThresholNextWord)\n            if nearListWeGet is None:\n                return nearaestListWeHave\n            else:\n                nearListWeGet.extend(nearaestListWeHave)\n                return nearListWeGet\n        else:#limit reached of recursion skip\n            return nearaestListWeHave\n    else:\n        return None\n## E) merge all wordcontours which are neighbours to linecontours (outer middle line points are close together)\n#we group all contours\nfor i in range(0,len(contourSelectedByRatio)):\n    cnt1 = contourSelectedByRatio[i]\n    if cnt1 in groupMap:\n        continue\n    lineGroup = ContourGroup()\n    lineGroup.Add(cnt1)\n    groupMap[cnt1] = lineGroup\n    depthCounter = 0\n    nearaestList = getNearestNeighbors(cnt1,depthCounter,\n                                       contourSelectedByRatio,maxDistanceThresholNextWord)\n    if nearaestList is None:\n        lineGroupList.append(lineGroup) #no neighbor found\n        continue\n    for cnt in nearaestList:\n        groupMap[cnt] = lineGroup\n        lineGroup.Add(cnt)\n    lineGroupList.append(lineGroup)\nif DEBUGMODE:\n    imgContourGroup = img.copy()\n    for group in lineGroupList:\n        #print(f\"group({group.uuid} size: {len(group.contourList)}\")\n        #we print all corner points\n        for cnt in group.contourList:\n            leftCorner = cnt.GetCenterLineLeftCorner()\n            rigthCorner = cnt.GetCenterLineRightCorner()\n            cv2.circle(imgContourGroup, leftCorner, 5, (0, 0, 255), -1)\n            cv2.circle(imgContourGroup, rigthCorner, 5, (140, 0, 0), -1)\n        #we print estimated underlines\n        for cnt in group.contourList:\n            leftCorner = cnt.GetCenterLineLeftCorner()\n            rigthCorner = cnt.GetCenterLineRightCorner()\n            cv2.line(imgContourGroup, leftCorner, rigthCorner, (0, 255, 0), 2)\n        # we print all contours\n        groupColor = randomColor()\n        cntList = [cnt.contour for cnt in group.contourList]\n        imgContourGroup = drawCountourOn(imgContourGroup,cntList,groupColor)\n    cv2.imwrite(\"img05contourGroup.jpg\",resizeImageByPercentage(imgContourGroup,30))\n## F) do polynomial regression 2nd order to estimate middle line of linecontours\n# calc line from stable group points\nminAmountRegressionElements = 12\nmovingWindowSize = 3\nletterCenterOffset = medianWordHigh * 0.5\nlineListCollection = list()\nfor group in lineGroupList:\n    stablePoints = list()\n    for cnt in group.contourList:\n        stablePoints.extend(cnt.randomPoinsInCnt)\n    if len(stablePoints) >= minAmountRegressionElements :\n        xValues = [x for x,y in stablePoints]\n        yValues = [y for x,y in stablePoints]\n        # perform polynomial regression of degree 2\n        coefffientValues = np.polyfit(np.array(xValues), np.array(yValues), 2)\n        # create a polynomial function with the coefficients\n        polynomial = np.poly1d(coefffientValues)\n        #we filter to build something like a line\n        xValuesNewLineFilter = list()\n        xMin =int( min(xValues))\n        xMax = int(max(xValues))\n        for xNew in range(xMin,xMax,movingWindowSize):\n                xValuesNewLineFilter.append(xNew)\n        #we predict new points with all old x values\n        yValuesNew = polynomial(xValuesNewLineFilter)\n        yValuesNewHighCorrect =np.array(yValuesNew) + letterCenterOffset\n        lineList = list()\n        #we create a list of points\n        for i in range(0,len(xValuesNewLineFilter)):\n            pointInt = (int(xValuesNewLineFilter[i]),int(yValuesNewHighCorrect[i]))\n            lineList.append(pointInt)\n        lineListCollection.append(lineList)\n## G) write the lines\nimgLines = img.copy()\nfor lineList in lineListCollection:\n    p1 = lineList[0]\n    for j in range(1,len(lineList)):\n        p2 = lineList[j]\n        #cv2.circle(imgLines, p2Int, 5, (0, 0, 255), -1)\n        cv2.line(imgLines, p1, p2, (0, 255, 0), 2)\n        p1 = p2\ncv2.imwrite(\"img06Lines.jpg\",resizeImageByPercentage(imgLines,30))\nif DEBUGMODE:\n    cv2.waitKey(0)"
  },
  {
    "url": "https://stackoverflow.com/questions/48171611/difference-between-pandas-read-sql-query-and-read-sql-table",
    "body": "import time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nsqlite_engine = create_engine('sqlite:///coffee.db', echo=False)\nmariadb_engine = create_engine('mariadb+mariadbconnector://root:admin@127.0.0.1:3306/coffee')\npostgres_engine = create_engine('postgresql://postgres:admin@127.0.0.1:5432/coffee')\nfor engine in [sqlite_engine, mariadb_engine, postgres_engine]:\n    print(engine)\n    print('\\tpd.read_sql_query:')\n    startTime = time.time()\n    for i in range(100):\n        pd.read_sql_query('SELECT * FROM arabica;', engine)\n    print(f\"\\t[-- TIME --] {time.time()-startTime:.2f} sec\\n\")\n    print('\\tpd.read_sql_table:')\n    startTime = time.time()\n    for i in range(100):\n        pd.read_sql_table('arabica', engine)\n    print(f\"\\t[-- TIME --] {time.time()-startTime:.2f} sec\\n\")"
  },
  {
    "url": "https://stackoverflow.com/questions/54426123/how-to-inform-class-weights-when-using-tensorflow-python-keras-estimator-model",
    "body": "import tensorflow as tf\nfrom tensorflow.python.keras import backend as K\ndef weighted_loss_fn(class_weights):\n\n    def _loss_fn(y_true, y_pred):\n        class_weights_tensor = K.variable(class_weights)\n        y_true_labels = K.argmax(y_true,axis=1)\n        weights = K.gather(class_weights_tensor,y_true_labels)\n        return tf.losses.softmax_cross_entropy(onehot_labels=y_true, logits=y_pred, weights=weights)\n    return _loss_fn\ndef keras_estimator_model(n_classes=None, model_dir='./tmp-model/', config=None, class_weights=None):\n    with tf.device('/gpu:0'):\n        # Inputs\n        inp_raw = Input(shape=(max_len,), name='word_raw')\n        # raw text LSTM network\n        word_raw_emb = Embedding(\n            input_dim=nunique_chars_raw,\n            output_dim=EMBED_SIZE,\n            input_length=MAX_WORD_LENGTH,\n            trainable=True,\n            name='word_raw_emb')(inp_raw)\n        word_raw_emb = Dropout(rate=dropout_rate)(word_raw_emb)\n        word_raw_emb_lstm = Bidirectional(\n            LSTM(48, return_sequences=True))(word_raw_emb)\n        word_raw_emb_gru = Bidirectional(\n            GRU(48, return_sequences=False))(word_raw_emb_lstm)\n        word_raw_net = Dense(16, activation='relu')(word_raw_emb_gru)\n        output_raw_net = Dense(n_classes, activation='softmax')(word_raw_net)\n        model = Model(inputs=inp_raw, outputs=output_raw_net)\n        optz = keras.optimizers.RMSprop(\n            lr=0.002, rho=0.9, epsilon=None, decay=0.0)\n        loss_fn = weighted_loss_fn(class_weights)\n        model.compile(loss=loss_fn,\n                      optimizer=optz, metrics=['categorical_accuracy'])\n        model_estimator = model_to_estimator(keras_model=model, model_dir=model_dir, config=config)\n        return model_estimator\nestimator_model = keras_estimator_model(5)\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn,max_steps=10)\neval_spec = tf.estimator.EvalSpec(\n        input_fn=eval_input_fn,\n        steps=None,\n        start_delay_secs=10,\n        throttle_secs=10)\ntf.estimator.train_and_evaluate(estimator_model, train_spec, eval_spec)"
  },
  {
    "url": "https://stackoverflow.com/questions/52576365/resampling-time-series-or-dataframes-with-javascript-node-js",
    "body": "var a = [{\n    \"time\": \"28-09-2018 21:29:04\",\n    \"value1\": 1280,\n    \"value2\": 800\n},\n{\n    \"time\": \"28-09-2018 21:38:56\",\n    \"value1\": 600,\n    \"value2\": 700\n},\n{\n    \"time\": \"29-09-2018 10:40:00\",\n    \"value1\": 1100,\n    \"value2\": 300\n},\n{\n    \"time\": \"29-09-2018 23:50:48\",\n    \"value1\": 140\n}];\nvar b = Object.values(a.reduce((container, current) => {\n  var date = current['time'].substring(0, 10);\n  if (!container[date])\n    container[date] = {time: date + ' 00:00:00', value1: current['value1'] || 0, value2: current['value2'] || 0};\n  else {\n    container[date]['value1'] += current['value1'] || 0;\n    container[date]['value2'] += current['value2'] || 0;\n  }\n  return container;\n}, {}));"
  },
  {
    "url": "https://stackoverflow.com/questions/52576365/resampling-time-series-or-dataframes-with-javascript-node-js",
    "body": "var resample = 12;\nvar b = Object.values(a.reduce((container, current) => {\n  var date = new Date(current['time'].replace(/(\\d+)-(\\d+)-(\\d+) (\\d+):(\\d+):(\\d+)/, '$3-$2-$1T$4:$5:$6'));\n  date.setHours(Math.floor(date.getHours() / resample) * resample);\n  date.setMinutes(0);\n  date.setSeconds(0);\n  if (!container[date.toString()])\n    container[date.toString()] = {time: date, value1: current['value1'] || 0, value2: current['value2'] || 0};\n  else {\n    container[date.toString()]['value1'] += current['value1'] || 0;\n    container[date.toString()]['value2'] += current['value2'] || 0;\n  }\n  return container;\n}, {}));"
  },
  {
    "url": "https://stackoverflow.com/questions/46689334/how-to-customize-docstring-color-for-python-in-vscodes-default-theme",
    "body": "    \"editor.tokenColorCustomizations\": {\n        \"textMateRules\": [\n            {\n                \"scope\": [\n                    \"string.quoted.docstring.multi.python\",\n                    \"string.quoted.docstring.multi.python punctuation.definition.string.begin.python\",\n                    \"string.quoted.docstring.multi.python punctuation.definition.string.end.python\",\n                    \"string.quoted.docstring.multi.python constant.character.escape.python\"\n                ],\n                \"settings\": {\n                    \"foreground\": \"#aab5da\" //change to your preference\n                }\n            }\n        ]\n    },"
  },
  {
    "url": "https://stackoverflow.com/questions/42076424/how-to-batch-get-item-many-items-at-once-given-a-list-of-primary-partition-key-v",
    "body": "import logging\nimport boto3\ndynamodb = boto3.resource('dynamodb')\nlogger = logging.getLogger(__name__)\nmovie_table = dynamodb.Table('Movies')\nactor_table = dyanmodb.Table('Actors')\nbatch_keys = {\n    movie_table.name: {\n        'Keys': [{'year': movie[0], 'title': movie[1]} for movie in movie_list]\n    },\n    actor_table.name: {\n        'Keys': [{'name': actor} for actor in actor_list]\n    }\n}\nresponse = dynamodb.batch_get_item(RequestItems=batch_keys)\nfor response_table, response_items in response.items():\n    logger.info(\"Got %s items from %s.\", len(response_items), response_table)"
  },
  {
    "url": "https://stackoverflow.com/questions/73150560/get-the-name-of-all-fields-in-a-dataclass",
    "body": "import dataclasses\nimport inspect\n@dataclasses.dataclass\nclass Test:\n    a: str = \"a value\"\n    b: str = \"b value\"\ndef print_data_class(dataclass_instance):\n    # option 1: fields\n    fields = dataclasses.fields(dataclass_instance)\n    # option 2: inspect\n    members = inspect.getmembers(type(dataclass_instance))\n    fields = list(dict(members)['__dataclass_fields__'].values())\n    for v in fields:\n        print(f'{v.name}: ({v.type.__name__}) = {getattr(dataclass_instance, v.name)}')\nprint_data_class(Test())\n# a: (str) = a value\n# b: (str) = b value\nprint_data_class(Test(a=\"1\", b=\"2\"))\n# a: (str) = 1\n# b: (str) = 2"
  },
  {
    "url": "https://stackoverflow.com/questions/58895486/how-to-send-server-side-events-from-python-fastapi-upon-calls-to-a-function-th",
    "body": "import asyncio\nimport uvicorn\nfrom fastapi import FastAPI, Request\nfrom sse_starlette.sse import EventSourceResponse\nMESSAGE_STREAM_DELAY = 1  # second\nMESSAGE_STREAM_RETRY_TIMEOUT = 15000  # milisecond\napp = FastAPI()\n@app.get('/stream')\nasync def message_stream(request: Request):\n    def new_messages(): ...\n    async def event_generator():\n        while True:\n            # If client was closed the connection\n            if await request.is_disconnected():\n                break\n            # Checks for new messages and return them to client if any\n            if new_messages():\n                yield {\n                        \"event\": \"new_message\",\n                        \"id\": \"message_id\",\n                        \"retry\": MESSAGE_STREAM_RETRY_TIMEOUT,\n                        \"data\": \"message_content\"\n                }\n            await asyncio.sleep(MESSAGE_STREAM_DELAY)\n    return EventSourceResponse(event_generator())\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
  },
  {
    "url": "https://stackoverflow.com/questions/16065694/is-it-possible-to-create-encoded-base64-url-from-image-object",
    "body": "data:image/jpg;base64,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"
  },
  {
    "url": "https://stackoverflow.com/questions/15229896/combining-scatter-plot-with-surface-plot",
    "body": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom random import random, seed\nfrom matplotlib import cm\nimport matplotlib as mpl\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(projection='3d')              # to work in 3d\nx_surf=np.arange(0, 1, 0.01)                # generate a mesh\ny_surf=np.arange(0, 1, 0.01)\nx_surf, y_surf = np.meshgrid(x_surf, y_surf)\nz_surf = np.sqrt(x_surf+y_surf)             # ex. function, which depends on x and y\nax.plot_surface(x_surf, y_surf, z_surf, cmap=cm.hot, ec='k')  # plot a 3d surface plot\nn = 100\nseed(0)                                     # seed let us to have a reproducible set of random numbers\nx=[random() for i in range(n)]              # generate n random points\ny=[random() for i in range(n)]\nz=[random() for i in range(n)]\nax.scatter(x, y, z);                        # plot a 3d scatter plot\nax.set_xlabel('x label')\nax.set_ylabel('y label')\nax.set_zlabel('z label')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/68705698/how-to-write-tests-for-pydantic-models-in-fastapi",
    "body": "from fastapi.testclient import TestClient\nclient = TestClient(app)\ndef test_add_phonenumber_ok():\n    \"\"\"Valid PhoneNumber, should be 200/OK\"\"\"\n    # This would be what the JSON body of the request would look like\n    body = {\n        \"id\": 1,\n        \"country\": \"Japan\",\n        \"country_code\": \"JA\",\n        \"number\": \"123\",\n        \"extension\": \"81\",\n    }\n    response = client.post(\"/phonenumber\", json=body)\n    assert response.status_code == 200\ndef test_add_phonenumber_error():\n    \"\"\"Invalid PhoneNumber, should be a validation error\"\"\"\n    # This would be what the JSON body of the request would look like\n    body = {\n        \"id\": 1,\n        \"country\": \"Japan\",\n                             # `country_code` is missing\n        \"number\": 99999,     # `number` is int, not str\n        \"extension\": \"81\",\n    }\n    response = client.post(\"/phonenumber\", json=body)\n    assert response.status_code == 422\n    assert response.json() == {\n        'detail': [{\n            'loc': ['body', 'country_code'],\n            'msg': 'field required',\n            'type': 'value_error.missing'\n        }]\n    }"
  },
  {
    "url": "https://stackoverflow.com/questions/57024802/numpy-in-place-operation-performance",
    "body": "from simple_benchmark import BenchmarkBuilder, MultiArgument\nimport numpy as np\nb = BenchmarkBuilder()\n@b.add_function()\ndef func1(a1, a2):\n    a1 = a1 + a2\n@b.add_function()\ndef func2(a1, a2):\n    a1 += a2\n\n@b.add_arguments('array size')\ndef argument_provider():\n    for exp in range(3, 28):\n        dim_size = int(1.4**exp)\n        a1 = np.random.random([dim_size, dim_size])\n        a2 = np.random.random([dim_size, dim_size])\n        yield dim_size ** 2, MultiArgument([a1, a2])\n\nr = b.run()\nr.plot()"
  },
  {
    "url": "https://stackoverflow.com/questions/51206500/how-to-convert-a-string-datetime-with-unknown-timezone-to-timestamp-in-python",
    "body": "#!/usr/bin/env python\n# encoding:utf-8\nfrom dateutil import parser\nwhois_timezone_info = {\n        \"A\": 1 * 3600,\n        \"ACDT\": 10.5 * 3600,\n        \"ACST\": 9.5 * 3600,\n        \"ACT\": -5 * 3600,\n        \"ACWST\": 8.75 * 3600,\n        \"ADT\": 4 * 3600,\n        \"AEDT\": 11 * 3600,\n        \"AEST\": 10 * 3600,\n        \"AET\": 10 * 3600,\n        \"AFT\": 4.5 * 3600,\n        \"AKDT\": -8 * 3600,\n        \"AKST\": -9 * 3600,\n        \"ALMT\": 6 * 3600,\n        \"AMST\": -3 * 3600,\n        \"AMT\": -4 * 3600,\n        \"ANAST\": 12 * 3600,\n        \"ANAT\": 12 * 3600,\n        \"AQTT\": 5 * 3600,\n        \"ART\": -3 * 3600,\n        \"AST\": 3 * 3600,\n        \"AT\": -4 * 3600,\n        \"AWDT\": 9 * 3600,\n        \"AWST\": 8 * 3600,\n        \"AZOST\": 0 * 3600,\n        \"AZOT\": -1 * 3600,\n        \"AZST\": 5 * 3600,\n        \"AZT\": 4 * 3600,\n        \"AoE\": -12 * 3600,\n        \"B\": 2 * 3600,\n        \"BNT\": 8 * 3600,\n        \"BOT\": -4 * 3600,\n        \"BRST\": -2 * 3600,\n        \"BRT\": -3 * 3600,\n        \"BST\": 6 * 3600,\n        \"BTT\": 6 * 3600,\n        \"C\": 3 * 3600,\n        \"CAST\": 8 * 3600,\n        \"CAT\": 2 * 3600,\n        \"CCT\": 6.5 * 3600,\n        \"CDT\": -5 * 3600,\n        \"CEST\": 2 * 3600,\n        \"CET\": 1 * 3600,\n        \"CHADT\": 13.75 * 3600,\n        \"CHAST\": 12.75 * 3600,\n        \"CHOST\": 9 * 3600,\n        \"CHOT\": 8 * 3600,\n        \"CHUT\": 10 * 3600,\n        \"CIDST\": -4 * 3600,\n        \"CIST\": -5 * 3600,\n        \"CKT\": -10 * 3600,\n        \"CLST\": -3 * 3600,\n        \"CLT\": -4 * 3600,\n        \"COT\": -5 * 3600,\n        \"CST\": -6 * 3600,\n        \"CT\": -6 * 3600,\n        \"CVT\": -1 * 3600,\n        \"CXT\": 7 * 3600,\n        \"ChST\": 10 * 3600,\n        \"D\": 4 * 3600,\n        \"DAVT\": 7 * 3600,\n        \"DDUT\": 10 * 3600,\n        \"E\": 5 * 3600,\n        \"EASST\": -5 * 3600,\n        \"EAST\": -6 * 3600,\n        \"EAT\": 3 * 3600,\n        \"ECT\": -5 * 3600,\n        \"EDT\": -4 * 3600,\n        \"EEST\": 3 * 3600,\n        \"EET\": 2 * 3600,\n        \"EGST\": 0 * 3600,\n        \"EGT\": -1 * 3600,\n        \"EST\": -5 * 3600,\n        \"ET\": -5 * 3600,\n        \"F\": 6 * 3600,\n        \"FET\": 3 * 3600,\n        \"FJST\": 13 * 3600,\n        \"FJT\": 12 * 3600,\n        \"FKST\": -3 * 3600,\n        \"FKT\": -4 * 3600,\n        \"FNT\": -2 * 3600,\n        \"G\": 7 * 3600,\n        \"GALT\": -6 * 3600,\n        \"GAMT\": -9 * 3600,\n        \"GET\": 4 * 3600,\n        \"GFT\": -3 * 3600,\n        \"GILT\": 12 * 3600,\n        \"GMT\": 0 * 3600,\n        \"GST\": 4 * 3600,\n        \"GYT\": -4 * 3600,\n        \"H\": 8 * 3600,\n        \"HDT\": -9 * 3600,\n        \"HKT\": 8 * 3600,\n        \"HOVST\": 8 * 3600,\n        \"HOVT\": 7 * 3600,\n        \"HST\": -10 * 3600,\n        \"I\": 9 * 3600,\n        \"ICT\": 7 * 3600,\n        \"IDT\": 3 * 3600,\n        \"IOT\": 6 * 3600,\n        \"IRDT\": 4.5 * 3600,\n        \"IRKST\": 9 * 3600,\n        \"IRKT\": 8 * 3600,\n        \"IRST\": 3.5 * 3600,\n        \"IST\": 5.5 * 3600,\n        \"JST\": 9 * 3600,\n        \"K\": 10 * 3600,\n        \"KGT\": 6 * 3600,\n        \"KOST\": 11 * 3600,\n        \"KRAST\": 8 * 3600,\n        \"KRAT\": 7 * 3600,\n        \"KST\": 9 * 3600,\n        \"KUYT\": 4 * 3600,\n        \"L\": 11 * 3600,\n        \"LHDT\": 11 * 3600,\n        \"LHST\": 10.5 * 3600,\n        \"LINT\": 14 * 3600,\n        \"M\": 12 * 3600,\n        \"MAGST\": 12 * 3600,\n        \"MAGT\": 11 * 3600,\n        \"MART\": 9.5 * 3600,\n        \"MAWT\": 5 * 3600,\n        \"MDT\": -6 * 3600,\n        \"MHT\": 12 * 3600,\n        \"MMT\": 6.5 * 3600,\n        \"MSD\": 4 * 3600,\n        \"MSK\": 3 * 3600,\n        \"MST\": -7 * 3600,\n        \"MT\": -7 * 3600,\n        \"MUT\": 4 * 3600,\n        \"MVT\": 5 * 3600,\n        \"MYT\": 8 * 3600,\n        \"N\": -1 * 3600,\n        \"NCT\": 11 * 3600,\n        \"NDT\": 2.5 * 3600,\n        \"NFT\": 11 * 3600,\n        \"NOVST\": 7 * 3600,\n        \"NOVT\": 7 * 3600,\n        \"NPT\": 5.5 * 3600,\n        \"NRT\": 12 * 3600,\n        \"NST\": 3.5 * 3600,\n        \"NUT\": -11 * 3600,\n        \"NZDT\": 13 * 3600,\n        \"NZST\": 12 * 3600,\n        \"O\": -2 * 3600,\n        \"OMSST\": 7 * 3600,\n        \"OMST\": 6 * 3600,\n        \"ORAT\": 5 * 3600,\n        \"P\": -3 * 3600,\n        \"PDT\": -7 * 3600,\n        \"PET\": -5 * 3600,\n        \"PETST\": 12 * 3600,\n        \"PETT\": 12 * 3600,\n        \"PGT\": 10 * 3600,\n        \"PHOT\": 13 * 3600,\n        \"PHT\": 8 * 3600,\n        \"PKT\": 5 * 3600,\n        \"PMDT\": -2 * 3600,\n        \"PMST\": -3 * 3600,\n        \"PONT\": 11 * 3600,\n        \"PST\": -8 * 3600,\n        \"PT\": -8 * 3600,\n        \"PWT\": 9 * 3600,\n        \"PYST\": -3 * 3600,\n        \"PYT\": -4 * 3600,\n        \"Q\": -4 * 3600,\n        \"QYZT\": 6 * 3600,\n        \"R\": -5 * 3600,\n        \"RET\": 4 * 3600,\n        \"ROTT\": -3 * 3600,\n        \"S\": -6 * 3600,\n        \"SAKT\": 11 * 3600,\n        \"SAMT\": 4 * 3600,\n        \"SAST\": 2 * 3600,\n        \"SBT\": 11 * 3600,\n        \"SCT\": 4 * 3600,\n        \"SGT\": 8 * 3600,\n        \"SRET\": 11 * 3600,\n        \"SRT\": -3 * 3600,\n        \"SST\": -11 * 3600,\n        \"SYOT\": 3 * 3600,\n        \"T\": -7 * 3600,\n        \"TAHT\": -10 * 3600,\n        \"TFT\": 5 * 3600,\n        \"TJT\": 5 * 3600,\n        \"TKT\": 13 * 3600,\n        \"TLT\": 9 * 3600,\n        \"TMT\": 5 * 3600,\n        \"TOST\": 14 * 3600,\n        \"TOT\": 13 * 3600,\n        \"TRT\": 3 * 3600,\n        \"TVT\": 12 * 3600,\n        \"U\": -8 * 3600,\n        \"ULAST\": 9 * 3600,\n        \"ULAT\": 8 * 3600,\n        \"UTC\": 0 * 3600,\n        \"UYST\": -2 * 3600,\n        \"UYT\": -3 * 3600,\n        \"UZT\": 5 * 3600,\n        \"V\": -9 * 3600,\n        \"VET\": -4 * 3600,\n        \"VLAST\": 11 * 3600,\n        \"VLAT\": 10 * 3600,\n        \"VOST\": 6 * 3600,\n        \"VUT\": 11 * 3600,\n        \"W\": -10 * 3600,\n        \"WAKT\": 12 * 3600,\n        \"WARST\": -3 * 3600,\n        \"WAST\": 2 * 3600,\n        \"WAT\": 1 * 3600,\n        \"WEST\": 1 * 3600,\n        \"WET\": 0 * 3600,\n        \"WFT\": 12 * 3600,\n        \"WGST\": -2 * 3600,\n        \"WGT\": -3 * 3600,\n        \"WIB\": 7 * 3600,\n        \"WIT\": 9 * 3600,\n        \"WITA\": 8 * 3600,\n        \"WST\": 14 * 3600,\n        \"WT\": 0 * 3600,\n        \"X\": -11 * 3600,\n        \"Y\": -12 * 3600,\n        \"YAKST\": 10 * 3600,\n        \"YAKT\": 9 * 3600,\n        \"YAPT\": 10 * 3600,\n        \"YEKST\": 6 * 3600,\n        \"YEKT\": 5 * 3600,\n        \"Z\": 0 * 3600,\n}\ntimestamp = parser.parse(\"Thu Jun 02 11:56:53 CDT 2011\", tzinfos={\"CDT\": -5*3600})"
  },
  {
    "url": "https://stackoverflow.com/questions/74965764/how-can-i-properly-hash-dictionaries-with-a-common-set-of-keys-for-deduplicatio",
    "body": ">>> def show_hash(d):\n...     return bin(hash(frozenset(d.values())))\n...\n>>> show_hash({'id': '1', 'error': None, 'value': 'apple'})\n'0b101010010100001000111001000001000111101111110100010000010101110'\n>>> # Changing a value changes the hash...\n>>> show_hash({'id': '1', 'error': None, 'value': 'orange'})\n'0b11111111001000011101011001001011100010100100010010110000100100'\n>>> # but rearranging them does not:\n>>> show_hash({'id': '1', 'error': 'orange', 'value': None})\n'0b11111111001000011101011001001011100010100100010010110000100100'"
  },
  {
    "url": "https://stackoverflow.com/questions/61041707/plotly-log-scale-in-subplot-python",
    "body": "import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Default Scale\", \"Logarithmic Scale\"))\n# subplot in default scale\nfig.add_trace(go.Scatter(x=[0.1, 0.2, 0.3, 0.4, 0.5], y=[1.105, 1.221, 1.35, 1.492, 1.649]), row=1, col=1)\nfig.update_xaxes(title_text=\"x-axis in default scale\", row=1, col=1)\nfig.update_yaxes(title_text=\"y-axis in default scale\", row=1, col=1)\n# subplot in logarithmic scale\nfig.add_trace(go.Scatter(x=[0.1, 0.2, 0.3, 0.4, 0.5], y=[1.105, 1.221, 1.35, 1.492, 1.649]), row=1, col=2)\nfig.update_xaxes(title_text=\"x-axis in logarithmic scale\", type=\"log\", row=1, col=2)\nfig.update_yaxes(title_text=\"y-axis in logarithmic scale\", type=\"log\", row=1, col=2)\nfig.update_layout(showlegend=False)\nfig.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/50689359/django-natural-sort-queryset",
    "body": "from django.contrib.admin import ModelAdmin, register, SimpleListFilter\nfrom django.db.models.functions import Length, StrIndex, Substr, NullIf, Coalesce\nfrom django.db.models import Value as V\nfrom .models import Item\nclass AlphanumericSignatureFilter(SimpleListFilter):\n    title = 'Signature (alphanumeric)'\n    parameter_name = 'signature_alphanumeric'\n    def lookups(self, request, model_admin):\n        return (\n            ('signature', 'Signature (alphanumeric)'),\n        )\n    def queryset(self, request, queryset):\n        if self.value() == 'signature':\n            return queryset.order_by(\n                Coalesce(Substr('signature', V(0), NullIf(StrIndex('signature', V(' ')), V(0))), 'signature'),\n                Length('signature'),\n                'signature'\n            )\n@register(Item)\nclass Item(ModelAdmin):\n    list_filter = [AlphanumericSignatureFilter]"
  },
  {
    "url": "https://stackoverflow.com/questions/50689359/django-natural-sort-queryset",
    "body": "from django.contrib.admin import ModelAdmin, register, SimpleListFilter\nfrom django.db.models.functions import StrIndex, Concat\nfrom django.db.models import Value as V\nfrom natsort import natsorted\nfrom .models import Item\nclass AlphanumericTruePythonSignatureFilter(SimpleListFilter):\n    title = 'Signature (alphanumeric true python)'\n    parameter_name = 'signature_alphanumeric_python'\n    def lookups(self, request, model_admin):\n        return (\n            ('signature', 'Signature (alphanumeric)'),\n        )\n    def queryset(self, request, queryset):\n        if self.value() == 'signature':\n            all_ids = list(queryset.values_list('signature', flat=True))\n            # let's use \"!:!\" as a separator for signature values\n            all_ids_sorted = \"!:!\" + \"!:!\".join(natsorted(all_ids))\n            return queryset.order_by(\n                StrIndex(V(all_ids_sorted), Concat(V('!:!'), 'signature')),\n            )\n@register(Item)\nclass Item(ModelAdmin):\n    list_filter = [AlphanumericTruePythonSignatureFilter]"
  },
  {
    "url": "https://stackoverflow.com/questions/67549023/why-is-the-gnu-scientific-library-matrix-multiplication-slower-than-numpy-matmul",
    "body": "110:   lea          0x80(%rsp),%rsi\n       add          $0x60,%rsi\n       mov          %r12,%rax\n       sar          $0x3,%rax\n       cmp          $0x2,%rax\n     ↓ jl           d26\n       prefetcht0   0x200(%rdi)          # Data prefetching\n       vmovups      -0x60(%rsi),%ymm1\n       prefetcht0   0xa0(%rsi)\n       vbroadcastsd -0x80(%rdi),%ymm0    # Fast SIMD instruction (AVX)\n       prefetcht0   0xe0(%rsi)\n       vmovups      -0x40(%rsi),%ymm2\n       prefetcht0   0x120(%rsi)\n       vmovups      -0x20(%rsi),%ymm3\n       vmulpd       %ymm0,%ymm1,%ymm4\n       prefetcht0   0x160(%rsi)\n       vmulpd       %ymm0,%ymm2,%ymm8\n       vmulpd       %ymm0,%ymm3,%ymm12\n       prefetcht0   0x1a0(%rsi)\n       vbroadcastsd -0x78(%rdi),%ymm0\n       vmulpd       %ymm0,%ymm1,%ymm5\n       vmulpd       %ymm0,%ymm2,%ymm9\n       [...]"
  },
  {
    "url": "https://stackoverflow.com/questions/56284121/why-is-heap-slower-than-sort-for-k-closest-points-to-origin",
    "body": "import timeit\nimport matplotlib.pyplot as plt\ns = \"\"\"\nimport heapq\ndef k_heap(points, K):\n    return heapq.nsmallest(K, points, key = lambda P: P[0]**2 + P[1]**2)\ndef k_sort(points, K):\n    points.sort(key = lambda P: P[0]**2 + P[1]**2)\n    return points[:K]\n\"\"\"\nrandom.seed(1)\npoints = [(random.random(), random.random()) for _ in range(1000000)]\nr = list(range(11, 500000, 50000))\nheap_times = []\nsort_times = []\nfor i in r:\n    heap_times.append(timeit.timeit('k_heap({}, 10)'.format(points[:i]), setup=s, number=1))\n    sort_times.append(timeit.timeit('k_sort({}, 10)'.format(points[:i]), setup=s, number=1))\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\n#plt.plot(left, 0, marker='.')\nplt.plot(r, heap_times, marker='o')\nplt.plot(r, sort_times, marker='D')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/78056934/pandas-or-polars-find-index-of-previous-element-larger-than-current-one",
    "body": "from numba import njit, prange\n@njit(parallel=True)\ndef get_values(values):\n    out = np.zeros_like(values, dtype=np.float64)\n    for i in prange(len(values)):\n        idx = np.int64(i)\n        v = values[idx]\n        while idx > -1 and values[idx] <= v:\n            idx -= 1\n        if idx > -1:\n            out[i] = i - idx\n    out[0] = np.nan\n    return out\ndata = {\n    \"value\": [1, 9, 6, 7, 3, 2, 4, 5, 1, 9],\n    \"out\": [None, 0, 1, 2, 1, 1, 3, 4, 1, 0],\n}\ndf = pd.DataFrame(data)\ndf[\"out2\"] = get_values(df[\"value\"].values)\nprint(df)"
  },
  {
    "url": "https://stackoverflow.com/questions/55345608/instantiate-a-type-that-is-a-typevar",
    "body": "from typing import TypeVar, Generic, List, NewType, Type\nimport random\nclass PopMember:\n    def __init__(self):\n        self.x = random.randint(0, 100)\n    def __repr__(self):\n        return \"Pop({})\".format(self.x)\nTPopMember = TypeVar(\"TPopMember\")\nPopulation = NewType('Population', List[TPopMember])\nclass EvolutionaryAlgorithm(Generic[TPopMember]):\n    def __init__(self, member_class: Type[TPopMember], populationSize: int) -> None:\n        self.__population = Population([member_class() for _ in range(populationSize)])\n    def __repr__(self):\n        return \"EA({})\".format(self.__population)\nx = EvolutionaryAlgorithm(PopMember, 5)\nprint(x)"
  },
  {
    "url": "https://stackoverflow.com/questions/61741997/how-to-format-requirements-txt-when-package-source-is-from-specific-websites",
    "body": "$ pip install -r requirements.txt\nLooking in links: https://download.pytorch.org/whl/torch_stable.html, https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\nCollecting torch==1.5.0+cu101 (from -r requirements.txt (line 3))\n  Using cached https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp38-cp38-linux_x86_64.whl\nCollecting torchvision==0.6.0+cu101 (from -r requirements.txt (line 4))\n  Using cached https://download.pytorch.org/whl/cu101/torchvision-0.6.0%2Bcu101-cp38-cp38-linux_x86_64.whl\nCollecting detectron2 (from -r requirements.txt (line 8))\n  Using cached https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/detectron2-0.1.2%2Bcu101-cp38-cp38-linux_x86_64.whl\n..."
  },
  {
    "url": "https://stackoverflow.com/questions/55609339/how-to-perform-feature-selection-with-gridsearchcv-in-sklearn-in-python",
    "body": "from sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.33,\n                                                    random_state=42)\nfrom sklearn.pipeline import Pipeline\n#this is the classifier used for feature selection\nclf_featr_sele = RandomForestClassifier(n_estimators=30,\n                                        random_state=42,\n                                        class_weight=\"balanced\")\nrfecv = RFECV(estimator=clf_featr_sele,\n              step=1,\n              cv=5,\n              scoring = 'roc_auc')\n#you can have different classifier for your final classifier\nclf = RandomForestClassifier(n_estimators=10,\n                             random_state=42,\n                             class_weight=\"balanced\")\nCV_rfc = GridSearchCV(clf,\n                      param_grid={'max_depth':[2,3]},\n                      cv= 5, scoring = 'roc_auc')\npipeline  = Pipeline([('feature_sele',rfecv),\n                      ('clf_cv',CV_rfc)])\npipeline.fit(X_train, y_train)\npipeline.predict(X_test)"
  },
  {
    "url": "https://stackoverflow.com/questions/5408675/ascii-visualisation-of-a-graph-of-nodes-in-python",
    "body": "+-------------------+           +--------------------+\n| test_data.csv.dvc |           | train_data.csv.dvc |\n+-------------------+           +--------------------+\n                  **              **\n                    ***        ***\n                       **    **\n                +-------------------+\n                | featurization.dvc |\n                +-------------------+\n                  ***            ***\n                **                  ***\n              **                       **\n    +--------------+                     **\n    | training.dvc |                   **\n    +--------------+                ***\n                  ***            ***\n                     **        **\n                       **    **\n                     +---------+\n                     | Dvcfile |\n                     +---------+"
  },
  {
    "url": "https://stackoverflow.com/questions/62684468/pythons-requests-triggers-cloudflares-security-while-urllib-does-not",
    "body": "import requests\nfrom collections import OrderedDict\nfrom requests import Session\nimport socket\n# grab the address using socket.getaddrinfo\nanswers = socket.getaddrinfo('grimaldis.myguestaccount.com', 443)\n(family, type, proto, canonname, (address, port)) = answers[0]\ns = Session()\nheaders = OrderedDict({\n    'Accept-Encoding': 'gzip, deflate, br',\n    'Host': \"grimaldis.myguestaccount.com\",\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'\n})\ns.headers = headers\nresponse = s.get(f\"https://{address}/guest/accountlogin\", headers=headers, verify=False).text\nprint(response)"
  },
  {
    "url": "https://stackoverflow.com/questions/49084322/how-to-limit-field-access-on-a-model-based-on-user-type-on-graphene-django",
    "body": "class SetEmployee(graphene.Mutation):\n\n    class Arguments:\n        id = graphene.ID()\n        first_name = graphene.String()\n        last_name = graphene.String()\n        salary = graphene.String()\n\n    employee = graphene.Field(lambda: EmployeeType)\n\n\n    @classmethod\n    def mutate(cls, root, info, **args):\n        employee_id = args.get('employee_id')\n\n        # Fetch the employee object by id\n        employee = Employee.objects.get(id=employee_id)\n        first_name = args.get('first_name')\n        last_name = args.get('last_name')\n        salary = args.get('salary')\n\n        # Update the employee fields from the mutation inputs\n        if first_name:\n            employee.first_name = first_name\n        if last_name:\n            employee.last_name = last_name\n        if salary and info.context.user.has_perm('myapp.can_edit_salary'):\n            employee.salary = salary\n        employee.save()\n        return SetEmployee(employee=employee)"
  },
  {
    "url": "https://stackoverflow.com/questions/62249443/linking-pyenv-python-to-homebrew-in-order-to-avoid-homebrew-python3-8-installat",
    "body": "$  brew pyenv-sync --help\nUsage: brew pyenv-sync\nCreate symlinks for Homebrew's installed Python versions in ~/.pyenv/versions.\nNote that older patch version symlinks will be created and linked to the minor\nversion so e.g. Python 3.11.0 will also be symlinked to 3.11.3.\n  -d, --debug                      Display any debugging information.\n  -q, --quiet                      Make some output more quiet.\n  -v, --verbose                    Make some output more verbose.\n  -h, --help                       Show this message."
  },
  {
    "url": "https://stackoverflow.com/questions/62249443/linking-pyenv-python-to-homebrew-in-order-to-avoid-homebrew-python3-8-installat",
    "body": "$ ls -al $(brew --prefix black)/libexec/bin\ntotal 104\ndrwxr-xr-x  16 thecesrom  staff   512 Jun 11 08:32 .\ndrwxr-xr-x   6 thecesrom  staff   192 Jun 11 08:32 ..\n-rw-r--r--   1 thecesrom  staff  8834 Jun 10 15:27 Activate.ps1\n-rw-r--r--   1 thecesrom  staff  1916 Jun 11 08:32 activate\n-rw-r--r--   1 thecesrom  staff   865 Jun 11 08:32 activate.csh\n-rw-r--r--   1 thecesrom  staff  2005 Jun 11 08:32 activate.fish\n-rwxr-xr-x   1 thecesrom  staff   256 Jun 11 08:32 black\n-rwxr-xr-x   1 thecesrom  staff   251 Jun 11 08:32 black-primer\n-rwxr-xr-x   1 thecesrom  staff   257 Jun 11 08:32 blackd\n-rwxr-xr-x   1 thecesrom  staff  1000 Jun 11 08:32 chardetect\n-rwxr-xr-x   1 thecesrom  staff   257 Jun 11 08:32 pip\n-rwxr-xr-x   1 thecesrom  staff   257 Jun 11 08:32 pip3\n-rwxr-xr-x   1 thecesrom  staff   257 Jun 11 08:32 pip3.9\nlrwxr-xr-x   1 thecesrom  staff    84 Jun 10 15:27 python -> ../../../../../opt/python@3.9/Frameworks/Python.framework/Versions/3.9/bin/python3.9\nlrwxr-xr-x   1 thecesrom  staff    84 Jun 10 15:27 python3 -> ../../../../../opt/python@3.9/Frameworks/Python.framework/Versions/3.9/bin/python3.9\nlrwxr-xr-x   1 thecesrom  staff    84 Jun 10 15:27 python3.9 -> ../../../../../opt/python@3.9/Frameworks/Python.framework/Versions/3.9/bin/python3.9"
  },
  {
    "url": "https://stackoverflow.com/questions/70051773/polars-is-there-a-json-normalize-like-feature-in-polars",
    "body": "grades = {\n  \"name\": \"Ravi\",\n  \"Subjects\": {\n    \"Maths\": 92,\n    \"English\": 94,\n    \"Hindi\": 98\n  }}\ngrades_with_list = {key:[value] for key, value in grades.items()}\npl.DataFrame(grades_with_list)\n# Output\nshape: (1, 2)\n┌──────┬────────────┐\n│ name ┆ Subjects   │\n│ ---  ┆ ---        │\n│ str  ┆ struct[3]  │\n╞══════╪════════════╡\n│ Ravi ┆ {92,94,98} │\n└──────┴────────────┘\n# You can also un-nest the Subjets column, to get a separate column for each subject.\npl.DataFrame(grades_with_list).unnest('Subjects')\n# Output\nshape: (1, 4)\n┌──────┬───────┬─────────┬───────┐\n│ name ┆ Maths ┆ English ┆ Hindi │\n│ ---  ┆ ---   ┆ ---     ┆ ---   │\n│ str  ┆ i64   ┆ i64     ┆ i64   │\n╞══════╪═══════╪═════════╪═══════╡\n│ Ravi ┆ 92    ┆ 94      ┆ 98    │\n└──────┴───────┴─────────┴───────┘"
  },
  {
    "url": "https://stackoverflow.com/questions/57030125/automatically-adjusting-brightness-of-image-with-opencv",
    "body": "import cv2\nimport numpy as np\n# from matplotlib import pyplot as plt\n# Automatic brightness and contrast optimization with optional histogram clipping\ndef automatic_brightness_and_contrast(image, clip_hist_percent=25):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Calculate grayscale histogram\n    hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n    hist_size = len(hist)\n\n    # Calculate cumulative distribution from the histogram\n    accumulator = []\n    accumulator.append(float(hist[0]))\n    for index in range(1, hist_size):\n        accumulator.append(accumulator[index -1] + float(hist[index]))\n\n    # Locate points to clip\n    maximum = accumulator[-1]\n    clip_hist_percent *= (maximum/100.0)\n    clip_hist_percent /= 2.0\n\n    # Locate left cut\n    minimum_gray = 0\n    while accumulator[minimum_gray] < clip_hist_percent:\n        minimum_gray += 1\n\n    # Locate right cut\n    maximum_gray = hist_size -1\n    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n        maximum_gray -= 1\n\n    # Calculate alpha and beta values\n    alpha = 255 / (maximum_gray - minimum_gray)\n    beta = -minimum_gray * alpha\n\n    '''\n    # Calculate new histogram with desired range and show histogram\n    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])\n    plt.plot(hist)\n    plt.plot(new_hist)\n    plt.xlim([0,256])\n    plt.show()\n    '''\n    auto_result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n    return (auto_result, alpha, beta)\nimage = cv2.imread('1.png')\nauto_result, alpha, beta = automatic_brightness_and_contrast(image)\nprint('alpha', alpha)\nprint('beta', beta)\ncv2.imshow('auto_result', auto_result)\ncv2.imwrite('auto_result.png', auto_result)\ncv2.imshow('image', image)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/57030125/automatically-adjusting-brightness-of-image-with-opencv",
    "body": "import cv2\nimport numpy as np\n# from matplotlib import pyplot as plt\ndef convertScale(img, alpha, beta):\n    \"\"\"Add bias and gain to an image with saturation arithmetics. Unlike\n    cv2.convertScaleAbs, it does not take an absolute value, which would lead to\n    nonsensical results (e.g., a pixel at 44 with alpha = 3 and beta = -210\n    becomes 78 with OpenCV, when in fact it should become 0).\n    \"\"\"\n    new_img = img * alpha + beta\n    new_img[new_img < 0] = 0\n    new_img[new_img > 255] = 255\n    return new_img.astype(np.uint8)\n# Automatic brightness and contrast optimization with optional histogram clipping\ndef automatic_brightness_and_contrast(image, clip_hist_percent=25):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    # Calculate grayscale histogram\n    hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n    hist_size = len(hist)\n    # Calculate cumulative distribution from the histogram\n    accumulator = []\n    accumulator.append(float(hist[0]))\n    for index in range(1, hist_size):\n        accumulator.append(accumulator[index -1] + float(hist[index]))\n    # Locate points to clip\n    maximum = accumulator[-1]\n    clip_hist_percent *= (maximum/100.0)\n    clip_hist_percent /= 2.0\n    # Locate left cut\n    minimum_gray = 0\n    while accumulator[minimum_gray] < clip_hist_percent:\n        minimum_gray += 1\n    # Locate right cut\n    maximum_gray = hist_size -1\n    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n        maximum_gray -= 1\n    # Calculate alpha and beta values\n    alpha = 255 / (maximum_gray - minimum_gray)\n    beta = -minimum_gray * alpha\n    '''\n    # Calculate new histogram with desired range and show histogram\n    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])\n    plt.plot(hist)\n    plt.plot(new_hist)\n    plt.xlim([0,256])\n    plt.show()\n    '''\n    auto_result = convertScale(image, alpha=alpha, beta=beta)\n    return (auto_result, alpha, beta)\nimage = cv2.imread('1.jpg')\nauto_result, alpha, beta = automatic_brightness_and_contrast(image)\nprint('alpha', alpha)\nprint('beta', beta)\ncv2.imshow('auto_result', auto_result)\ncv2.imwrite('auto_result.png', auto_result)\ncv2.imshow('image', image)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/51241367/matplotlib-surface-plot-hides-scatter-points-which-should-be-in-front",
    "body": "import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\ndf = pd.DataFrame({10: {10: 1,15: 1,20: 1,25: 1,30: 1,35: 1,40: 1,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   15: {10: 4,15: 1,20: 1,25: 1,30: 1,35: 1,40: 1,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   20: {10: 6,15: 3,20: 1,25: 1,30: 1,35: 1,40: 1,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   25: {10: 7,15: 5,20: 3,25: 1,30: 1,35: 1,40: 1,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   30: {10: 9,15: 6,20: 4,25: 3,30: 1,35: 1,40: 1,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   35: {10: 10,15: 7,20: 5,25: 4,30: 2,35: 1,40: 1,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   40: {10: 11,15: 8,20: 6,25: 4,30: 3,35: 2,40: 1,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   45: {10: 12,15: 9,20: 7,25: 5,30: 4,35: 3,40: 2,45: 1,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   50: {10: 13,15: 9,20: 7,25: 6,30: 5,35: 4,40: 3,45: 2,50: 1,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   55: {10: 14,15: 10,20: 8,25: 7,30: 5,35: 4,40: 3,45: 3,50: 2,55: 1,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   60: {10: 15,15: 11,20: 9,25: 7,30: 6,35: 5,40: 4,45: 3,50: 3,55: 2,60: 1,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   65: {10: 16,15: 12,20: 9,25: 8,30: 6,35: 5,40: 5,45: 4,50: 3,55: 2,60: 2,65: 1,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   70: {10: 17,15: 12,20: 10,25: 8,30: 7,35: 6,40: 5,45: 4,50: 4,55: 3,60: 2,65: 2,70: 1,75: 1,80: 1,85: 1,90: 1},\n                   75: {10: 18,15: 13,20: 10,25: 9,30: 7,35: 6,40: 5,45: 5,50: 4,55: 3,60: 3,65: 2,70: 2,75: 1,80: 1,85: 1,90: 1},\n                   80: {10: 19,15: 14,20: 11,25: 9,30: 8,35: 7,40: 6,45: 5,50: 4,55: 4,60: 3,65: 3,70: 2,75: 2,80: 1,85: 1,90: 1},\n                   85: {10: 21,15: 14,20: 11,25: 10,30: 8,35: 7,40: 6,45: 6,50: 5,55: 4,60: 4,65: 3,70: 3,75: 2,80: 2,85: 1,90: 1},\n                   90: {10: 23,15: 15,20: 12,25: 10,30: 9,35: 8,40: 7,45: 6,50: 5,55: 5,60: 4,65: 3,70: 3,75: 3,80: 2,85: 2,90: 1}})\nxv, yv = np.meshgrid(df.index, df.columns)\nma = np.nanmax(df.values)\nnorm = matplotlib.colors.Normalize(vmin = 0, vmax = ma, clip = True)\nfig = plt.figure(1)\nax = Axes3D(fig, computed_zorder=False)\nax.scatter(10,70,4, c='k', depthshade=False, alpha = 1, s=100)\nsurf = ax.plot_surface(yv,xv,df, cmap='viridis_r', linewidth=0.3,\n                       alpha = 0.8, edgecolor = 'k', norm=norm)\nax.scatter(25,35,4, c='k', depthshade=False, alpha = 1, s=100)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/48941648/how-to-remove-a-word-completely-from-a-word2vec-model-in-gensim",
    "body": "def restrict_w2v(w2v, restricted_word_set):\n    new_vectors = []\n    new_vocab = {}\n    new_index2entity = []\n    new_vectors_norm = []\n    for i in range(len(w2v.vocab)):\n        word = w2v.index2entity[i]\n        vec = w2v.vectors[i]\n        vocab = w2v.vocab[word]\n        vec_norm = w2v.vectors_norm[i]\n        if word in restricted_word_set:\n            vocab.index = len(new_index2entity)\n            new_index2entity.append(word)\n            new_vocab[word] = vocab\n            new_vectors.append(vec)\n            new_vectors_norm.append(vec_norm)\n    w2v.vocab = new_vocab\n    w2v.vectors = new_vectors\n    w2v.index2entity = new_index2entity\n    w2v.index2word = new_index2entity\n    w2v.vectors_norm = new_vectors_norm"
  },
  {
    "url": "https://stackoverflow.com/questions/57417520/selecting-and-renaming-columns-at-the-same-time",
    "body": "data = {\n    'Commander': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n    'Date': ['2012, 02, 08', '2012, 02, 08', '2012, 02, 08',\n             '2012, 02, 08', '2012, 02, 08'],\n    'Score': [4, 24, 31, 2, 3],\n    'Team': ['Green', 'Yellow', 'Green', 'Yellow', 'Yellow'],\n}\ndf = pd.DataFrame(data, index=['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])\ndf\n           Commander          Date  Score    Team\nCochice        Jason  2012, 02, 08      4   Green\nPima           Molly  2012, 02, 08     24  Yellow\nSanta Cruz      Tina  2012, 02, 08     31   Green\nMaricopa        Jake  2012, 02, 08      2  Yellow\nYuma             Amy  2012, 02, 08      3  Yellow\nselector_d = {'Team': 'Team', 'Commander': 'Com', 'Score': 'Sco'}\ndf.rename(columns=selector_d)[[*selector_d.values()]]\n              Team    Com  Sco\nCochice      Green  Jason    4\nPima        Yellow  Molly   24\nSanta Cruz   Green   Tina   31\nMaricopa    Yellow   Jake    2\nYuma        Yellow    Amy    3"
  },
  {
    "url": "https://stackoverflow.com/questions/61292464/get-confidence-interval-from-sklearn-linear-regression-in-python",
    "body": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nalpha = 0.05 # for 95% confidence interval; use 0.01 for 99%-CI.\n# fit a sklearn LinearRegression model\nlin_model = LinearRegression().fit(X_train, Y_train)\n# the coefficients of the regression model\ncoefs = np.r_[[lin_model.intercept_], lin_model.coef_]\n# build an auxiliary dataframe with the constant term in it\nX_aux = X_train.copy()\nX_aux.insert(0, 'const', 1)\n# degrees of freedom\ndof = -np.diff(X_aux.shape)[0]\n# Student's t-distribution table lookup\nt_val = stats.t.isf(alpha/2, dof)\n# MSE of the residuals\nmse = np.sum((Y_train - lin_model.predict(X_train)) ** 2) / dof\n# inverse of the variance of the parameters\nvar_params = np.diag(np.linalg.inv(X_aux.T.dot(X_aux)))\n# distance between lower and upper bound of CI\ngap = t_val * np.sqrt(mse * var_params)\nconf_int = pd.DataFrame({'lower': coefs - gap, 'upper': coefs + gap}, index=X_aux.columns)"
  },
  {
    "url": "https://stackoverflow.com/questions/61292464/get-confidence-interval-from-sklearn-linear-regression-in-python",
    "body": "import numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\ndef get_conf_int(alpha, lr, X=X_train, y=Y_train):\n\n    \"\"\"\n    Returns (1-alpha) 2-sided confidence intervals\n    for sklearn.LinearRegression coefficients\n    as a pandas DataFrame\n    \"\"\"\n\n    coefs = np.r_[[lr.intercept_], lr.coef_]\n    X_aux = X.copy()\n    X_aux.insert(0, 'const', 1)\n    dof = -np.diff(X_aux.shape)[0]\n    mse = np.sum((y - lr.predict(X)) ** 2) / dof\n    var_params = np.diag(np.linalg.inv(X_aux.T.dot(X_aux)))\n    t_val = stats.t.isf(alpha/2, dof)\n    gap = t_val * np.sqrt(mse * var_params)\n    return pd.DataFrame({\n        'lower': coefs - gap, 'upper': coefs + gap\n    }, index=X_aux.columns)\n# for 95% confidence interval; use 0.01 for 99%-CI.\nalpha = 0.05\n# fit a sklearn LinearRegression model\nlin_model = LinearRegression().fit(X_train, Y_train)\nget_conf_int(alpha, lin_model, X_train, Y_train)"
  },
  {
    "url": "https://stackoverflow.com/questions/32397347/is-there-a-fast-way-to-return-sin-and-cos-of-the-same-value-in-python",
    "body": "import perfplot\nimport numpy as np\ndef sin_cos(x):\n    return np.sin(x), np.cos(x)\ndef exp_ix(x):\n    eix = np.exp(1j * x)\n    return eix.imag, eix.real\ndef cos_from_sin(x):\n    sin = np.sin(x)\n    abs_cos = np.sqrt(1 - sin**2)\n    sgn_cos = np.sign(((x - np.pi / 2) % (2 * np.pi)) - np.pi)\n    cos = abs_cos * sgn_cos\n    return sin, cos\nb = perfplot.bench(\n    setup=lambda n: np.linspace(0.0, 2 * np.pi, n),\n    kernels=[sin_cos, exp_ix, cos_from_sin],\n    n_range=[2**k for k in range(20)],\n    xlabel=\"n\",\n)\nb.save(\"out.png\")\nb.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/59341761/what-are-the-differences-between-a-classmethod-and-a-metaclass-method",
    "body": "class M1(type):\n    def clsmethod1(cls):\n        pass\nclass CLS1(metaclass=M1):\n    pass\ndef runtime_wrap(cls, method_name, wrapper):\n    mcls = type(cls)\n    setattr(mcls, method_name,  wrapper(getatttr(mcls, method_name)))\ndef wrapper(classmethod):\n    def new_method(cls):\n        print(\"wrapper called\")\n        return classmethod(cls)\n    return new_method\nruntime_wrap(cls1, \"clsmethod1\", wrapper)\nclass CLS2:\n    @classmethod\n    def classmethod2(cls):\n        pass\n def runtime_wrap2(cls, method_name, wrapper):\n    setattr(cls, method_name,  classmethod(\n                wrapper(getatttr(cls, method_name).__func__)\n        )\n    )\nruntime_wrap2(cls1, \"clsmethod1\", wrapper)"
  },
  {
    "url": "https://stackoverflow.com/questions/55426342/how-to-implement-indentation-based-code-folding-in-qscintilla",
    "body": "from PyQt5.Qsci import QsciScintilla\nfrom PyQt5.Qt import *\ndef set_fold(prev, line, fold, full):\n    if (prev[0] >= 0):\n        fmax = max(fold, prev[1])\n        for iter in range(prev[0], line + 1):\n            view.SendScintilla(view.SCI_SETFOLDLEVEL, iter,\n                fmax | (0, view.SC_FOLDLEVELHEADERFLAG)[iter + 1 < full])\ndef line_empty(line):\n    return view.SendScintilla(view.SCI_GETLINEENDPOSITION, line) \\\n        <= view.SendScintilla(view.SCI_GETLINEINDENTPOSITION, line)\ndef modify(position, modificationType, text, length, linesAdded,\n           line, foldLevelNow, foldLevelPrev, token, annotationLinesAdded):\n    full = view.SC_MOD_INSERTTEXT | view.SC_MOD_DELETETEXT\n    if (~modificationType & full == full):\n        return\n    prev = [-1, 0]\n    full = view.SendScintilla(view.SCI_GETLINECOUNT)\n    lbgn = view.SendScintilla(view.SCI_LINEFROMPOSITION, position)\n    lend = view.SendScintilla(view.SCI_LINEFROMPOSITION, position + length)\n    for iter in range(max(lbgn - 1, 0), -1, -1):\n        if ((iter == 0) or not line_empty(iter)):\n            lbgn = iter\n            break\n    for iter in range(min(lend + 1, full), full + 1):\n        if ((iter == full) or not line_empty(iter)):\n            lend = min(iter + 1, full)\n            break\n    for iter in range(lbgn, lend):\n        if (line_empty(iter)):\n            if (prev[0] == -1):\n                prev[0] = iter\n        else:\n            fold = view.SendScintilla(view.SCI_GETLINEINDENTATION, iter)\n            fold //= view.SendScintilla(view.SCI_GETTABWIDTH)\n            set_fold(prev, iter - 1, fold, full)\n            set_fold([iter, fold], iter, fold, full)\n            prev = [-1, fold]\n    set_fold(prev, lend - 1, 0, full)\nif __name__ == '__main__':\n    import sys\n    import textwrap\n    app = QApplication(sys.argv)\n    view = QsciScintilla()\n    view.SendScintilla(view.SCI_SETMULTIPLESELECTION, True)\n    view.SendScintilla(view.SCI_SETMULTIPASTE, 1)\n    view.SendScintilla(view.SCI_SETADDITIONALSELECTIONTYPING, True)\n    view.SendScintilla(view.SCI_SETINDENTATIONGUIDES, view.SC_IV_REAL);\n    view.SendScintilla(view.SCI_SETTABWIDTH, 4)\n    view.setFolding(view.BoxedFoldStyle)\n    view.SCN_MODIFIED.connect(modify)\n    NUM_CHUNKS = 20000\n    chunk = textwrap.dedent(\"\"\"\\\n        x = 1\n            x = 2\n            x = 3\n    \"\"\")\n    view.setText(\"\\n\".join([chunk for i in range(NUM_CHUNKS)]))\n    view.show()\n    app.exec_()"
  },
  {
    "url": "https://stackoverflow.com/questions/55672724/airflow-creating-dynamic-tasks-from-xcom",
    "body": "from airflow.models import TaskInstance\nfrom airflow.utils.db import provide_session\ndag = DAG(...)\n@provide_session\ndef get_files_list(session):\n    execution_date = dag.previous_schedule(datetime.now())\n    // Find previous task instance:\n    ti = session.query(TaskInstance).filter(\n        TaskInstance.dag_id == dag.dag_id,\n        TaskInstance.execution_date == execution_date,\n        TaskInstance.task_id == upstream_task_id).first()\n    if ti:\n        files_list = ti.xcom_pull()\n        if files_list:\n            return files_list\n    // Return default state:\n    return {...}\nfiles_list = get_files_list()\n// Generate tasks based on upstream task state:\ntask = PythonOperator(\n    ...\n    xcom_push=True,\n    dag=dag)"
  },
  {
    "url": "https://stackoverflow.com/questions/55455010/how-to-stop-pandas-dataframe-from-converting-int-to-float-for-no-reason",
    "body": "import pandas as pd\nimport numpy as np\nimport sys\nprint(sys.version)\nprint(pd.__version__)\nprint(\"int dtypes preserved\")\n# append on populated DataFrame\ndf = pd.DataFrame([[0, 0], [1,1]], index=['a', 'b'], columns=[\"col1\", \"col2\"])\ndf.loc[\"c\"] = np.int64(0)\n# slice existing rows\ndf.loc[\"a\":\"c\"] = np.int64(1)\ndf.loc[\"a\":\"c\", \"col1\":\"col2\":1] = np.int64(2)\nprint(df.dtypes)\n# no selection AND no data, remains np.int64 if defined as such\ndf = pd.DataFrame(columns=[\"col1\", \"col2\"], dtype=np.int64)\ndf.loc[:, \"col1\":\"col2\":1] = np.int64(0)\ndf.loc[:,:] = np.int64(0)\nprint(df.dtypes)\n# and works if no index but data\ndf = pd.DataFrame([[0, 0], [1,1]], columns=[\"col1\", \"col2\"])\ndf.loc[:,\"col1\":\"col2\":1] = np.int64(0)\nprint(df.dtypes)\n# the surprise... label based insertion for the entire row does not convert to float\ndf = pd.DataFrame(columns=[\"col1\", \"col2\"], dtype=np.int64)\ndf.loc[\"a\"] = np.int64(0)\nprint(df.dtypes)\n# a surprise because referring to all columns, as above, does convert to float\nprint(\"unexpectedly converted to float dtypes\")\ndf = pd.DataFrame(columns=[\"col1\", \"col2\"], dtype=np.int64)\ndf.loc[\"a\", \"col1\":\"col2\"] = np.int64(0)\nprint(df.dtypes)"
  },
  {
    "url": "https://stackoverflow.com/questions/65012892/how-to-specify-node-label-position-for-sankey-diagram-in-plotly",
    "body": "const TEXTPAD = 3; // constant used in Plotly.js\nfunction sankeyNodeLabelsAlign(position, forcePos) {\n  const textAnchor = {left: 'end', right: 'start', center: 'middle'}[position];\n  const nodes = gd.getElementsByClassName('sankey-node');\n  for (const node of nodes) {\n    const d = node.__data__;\n    const label = node.getElementsByClassName('node-label').item(0);\n    // Ensure to reset any previous modifications\n    label.setAttribute('x', 0);\n    if (!d.horizontal)\n      continue;\n    // This is how Plotly's default text positioning is computed (coordinates\n    // are relative to that of the cooresponding node).\n    const padX = d.nodeLineWidth / 2 + TEXTPAD;\n    const posX = padX + d.visibleWidth;\n    let x;\n    switch (position) {\n      case 'left':\n        if (d.left || d.node.originalLayer === 0 && !forcePos)\n          continue;\n        x = -posX - padX;\n        break;\n      case 'right':\n        if (!d.left || !forcePos)\n          continue;\n        x = posX + padX;\n        break;\n      case 'center':\n        if (!forcePos && (d.left || d.node.originalLayer === 0))\n          continue;\n        x = (d.nodeLineWidth + d.visibleWidth)/2 + (d.left ? padX : -posX);\n        break;\n    }\n    label.setAttribute('x', x);\n    label.setAttribute('text-anchor', textAnchor);\n  }\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/29745635/creating-wxslider-with-range-on-linux",
    "body": "import wx\ndef fraction_to_value(fraction, min_value, max_value):\n    return (max_value - min_value) * fraction + min_value\ndef value_to_fraction(value, min_value, max_value):\n    return float(value - min_value) / (max_value - min_value)\nclass SliderThumb:\n    def __init__(self, parent, value):\n        self.parent = parent\n        self.dragged = False\n        self.mouse_over = False\n        self.thumb_poly = ((0, 0), (0, 13), (5, 18), (10, 13), (10, 0))\n        self.thumb_shadow_poly = ((0, 14), (4, 18), (6, 18), (10, 14))\n        min_coords = [float('Inf'), float('Inf')]\n        max_coords = [-float('Inf'), -float('Inf')]\n        for pt in list(self.thumb_poly) + list(self.thumb_shadow_poly):\n            for i_coord, coord in enumerate(pt):\n                if coord > max_coords[i_coord]:\n                    max_coords[i_coord] = coord\n                if coord < min_coords[i_coord]:\n                    min_coords[i_coord] = coord\n        self.size = (max_coords[0] - min_coords[0],\n                     max_coords[1] - min_coords[1])\n        self.value = value\n        self.normal_color = wx.Colour((0, 120, 215))\n        self.normal_shadow_color = wx.Colour((120, 180, 228))\n        self.dragged_color = wx.Colour((204, 204, 204))\n        self.dragged_shadow_color = wx.Colour((222, 222, 222))\n        self.mouse_over_color = wx.Colour((23, 23, 23))\n        self.mouse_over_shadow_color = wx.Colour((132, 132, 132))\n    def GetPosition(self):\n        min_x = self.GetMin()\n        max_x = self.GetMax()\n        parent_size = self.parent.GetSize()\n        min_value = self.parent.GetMin()\n        max_value = self.parent.GetMax()\n        fraction = value_to_fraction(self.value, min_value, max_value)\n        pos = (fraction_to_value(fraction, min_x, max_x), parent_size[1] / 2 + 1)\n        return pos\n    def SetPosition(self, pos):\n        pos_x = pos[0]\n        # Limit movement by the position of the other thumb\n        who_other, other_thumb = self.GetOtherThumb()\n        other_pos = other_thumb.GetPosition()\n        if who_other == 'low':\n            pos_x = max(other_pos[0] + other_thumb.size[0]/2 + self.size[0]/2, pos_x)\n        else:\n            pos_x = min(other_pos[0] - other_thumb.size[0]/2 - self.size[0]/2, pos_x)\n        # Limit movement by slider boundaries\n        min_x = self.GetMin()\n        max_x = self.GetMax()\n        pos_x = min(max(pos_x, min_x), max_x)\n        fraction = value_to_fraction(pos_x, min_x, max_x)\n        self.value = fraction_to_value(fraction, self.parent.GetMin(), self.parent.GetMax())\n        # Post event notifying that position changed\n        self.PostEvent()\n    def GetValue(self):\n        return self.value\n    def SetValue(self, value):\n        self.value = value\n        # Post event notifying that value changed\n        self.PostEvent()\n    def PostEvent(self):\n        event = wx.PyCommandEvent(wx.EVT_SLIDER.typeId, self.parent.GetId())\n        event.SetEventObject(self.parent)\n        wx.PostEvent(self.parent.GetEventHandler(), event)\n    def GetMin(self):\n        min_x = self.parent.border_width + self.size[0] / 2\n        return min_x\n    def GetMax(self):\n        parent_size = self.parent.GetSize()\n        max_x = parent_size[0] - self.parent.border_width - self.size[0] / 2\n        return max_x\n    def IsMouseOver(self, mouse_pos):\n        in_hitbox = True\n        my_pos = self.GetPosition()\n        for i_coord, mouse_coord in enumerate(mouse_pos):\n            boundary_low = my_pos[i_coord] - self.size[i_coord] / 2\n            boundary_high = my_pos[i_coord] + self.size[i_coord] / 2\n            in_hitbox = in_hitbox and (boundary_low <= mouse_coord <= boundary_high)\n        return in_hitbox\n    def GetOtherThumb(self):\n        if self.parent.thumbs['low'] != self:\n            return 'low', self.parent.thumbs['low']\n        else:\n            return 'high', self.parent.thumbs['high']\n    def OnPaint(self, dc):\n        if self.dragged or not self.parent.IsEnabled():\n            thumb_color = self.dragged_color\n            thumb_shadow_color = self.dragged_shadow_color\n        elif self.mouse_over:\n            thumb_color = self.mouse_over_color\n            thumb_shadow_color = self.mouse_over_shadow_color\n        else:\n            thumb_color = self.normal_color\n            thumb_shadow_color = self.normal_shadow_color\n        my_pos = self.GetPosition()\n        # Draw thumb shadow (or anti-aliasing effect)\n        dc.SetBrush(wx.Brush(thumb_shadow_color, style=wx.BRUSHSTYLE_SOLID))\n        dc.SetPen(wx.Pen(thumb_shadow_color, width=1, style=wx.PENSTYLE_SOLID))\n        dc.DrawPolygon(points=self.thumb_shadow_poly,\n                       xoffset=my_pos[0] - self.size[0]/2,\n                       yoffset=my_pos[1] - self.size[1]/2)\n        # Draw thumb itself\n        dc.SetBrush(wx.Brush(thumb_color, style=wx.BRUSHSTYLE_SOLID))\n        dc.SetPen(wx.Pen(thumb_color, width=1, style=wx.PENSTYLE_SOLID))\n        dc.DrawPolygon(points=self.thumb_poly,\n                       xoffset=my_pos[0] - self.size[0] / 2,\n                       yoffset=my_pos[1] - self.size[1] / 2)\nclass RangeSlider(wx.Panel):\n    def __init__(self, parent, id=wx.ID_ANY, lowValue=None, highValue=None, minValue=0, maxValue=100,\n                 pos=wx.DefaultPosition, size=wx.DefaultSize, style=wx.SL_HORIZONTAL, validator=wx.DefaultValidator,\n                 name='rangeSlider'):\n        if style != wx.SL_HORIZONTAL:\n            raise NotImplementedError('Styles not implemented')\n        if validator != wx.DefaultValidator:\n            raise NotImplementedError('Validator not implemented')\n        super().__init__(parent=parent, id=id, pos=pos, size=size, name=name)\n        self.SetMinSize(size=(max(50, size[0]), max(26, size[1])))\n        if minValue > maxValue:\n            minValue, maxValue = maxValue, minValue\n        self.min_value = minValue\n        self.max_value = maxValue\n        if lowValue is None:\n            lowValue = self.min_value\n        if highValue is None:\n            highValue = self.max_value\n        if lowValue > highValue:\n            lowValue, highValue = highValue, lowValue\n        lowValue = max(lowValue, self.min_value)\n        highValue = min(highValue, self.max_value)\n        self.border_width = 8\n        self.thumbs = {\n            'low': SliderThumb(parent=self, value=lowValue),\n            'high': SliderThumb(parent=self, value=highValue)\n        }\n        self.thumb_width = self.thumbs['low'].size[0]\n        # Aesthetic definitions\n        self.slider_background_color = wx.Colour((231, 234, 234))\n        self.slider_outline_color = wx.Colour((214, 214, 214))\n        self.selected_range_color = wx.Colour((0, 120, 215))\n        self.selected_range_outline_color = wx.Colour((0, 120, 215))\n        # Bind events\n        self.Bind(wx.EVT_LEFT_DOWN, self.OnMouseDown)\n        self.Bind(wx.EVT_LEFT_UP, self.OnMouseUp)\n        self.Bind(wx.EVT_MOTION, self.OnMouseMotion)\n        self.Bind(wx.EVT_MOUSE_CAPTURE_LOST, self.OnMouseLost)\n        self.Bind(wx.EVT_ENTER_WINDOW, self.OnMouseEnter)\n        self.Bind(wx.EVT_LEAVE_WINDOW, self.OnMouseLeave)\n        self.Bind(wx.EVT_PAINT, self.OnPaint)\n        self.Bind(wx.EVT_ERASE_BACKGROUND, self.OnEraseBackground)\n        self.Bind(wx.EVT_SIZE, self.OnResize)\n    def Enable(self, enable=True):\n        super().Enable(enable)\n        self.Refresh()\n    def Disable(self):\n        super().Disable()\n        self.Refresh()\n    def SetValueFromMousePosition(self, click_pos):\n        for thumb in self.thumbs.values():\n            if thumb.dragged:\n                thumb.SetPosition(click_pos)\n    def OnMouseDown(self, evt):\n        if not self.IsEnabled():\n            return\n        click_pos = evt.GetPosition()\n        for thumb in self.thumbs.values():\n            if thumb.IsMouseOver(click_pos):\n                thumb.dragged = True\n                thumb.mouse_over = False\n                break\n        self.SetValueFromMousePosition(click_pos)\n        self.CaptureMouse()\n        self.Refresh()\n    def OnMouseUp(self, evt):\n        if not self.IsEnabled():\n            return\n        self.SetValueFromMousePosition(evt.GetPosition())\n        for thumb in self.thumbs.values():\n            thumb.dragged = False\n        if self.HasCapture():\n            self.ReleaseMouse()\n        self.Refresh()\n    def OnMouseLost(self, evt):\n        for thumb in self.thumbs.values():\n            thumb.dragged = False\n            thumb.mouse_over = False\n        self.Refresh()\n    def OnMouseMotion(self, evt):\n        if not self.IsEnabled():\n            return\n        refresh_needed = False\n        mouse_pos = evt.GetPosition()\n        if evt.Dragging() and evt.LeftIsDown():\n            self.SetValueFromMousePosition(mouse_pos)\n            refresh_needed = True\n        else:\n            for thumb in self.thumbs.values():\n                old_mouse_over = thumb.mouse_over\n                thumb.mouse_over = thumb.IsMouseOver(mouse_pos)\n                if old_mouse_over != thumb.mouse_over:\n                    refresh_needed = True\n        if refresh_needed:\n            self.Refresh()\n    def OnMouseEnter(self, evt):\n        if not self.IsEnabled():\n            return\n        mouse_pos = evt.GetPosition()\n        for thumb in self.thumbs.values():\n            if thumb.IsMouseOver(mouse_pos):\n                thumb.mouse_over = True\n                self.Refresh()\n                break\n    def OnMouseLeave(self, evt):\n        if not self.IsEnabled():\n            return\n        for thumb in self.thumbs.values():\n            thumb.mouse_over = False\n        self.Refresh()\n    def OnResize(self, evt):\n        self.Refresh()\n    def OnPaint(self, evt):\n        w, h = self.GetSize()\n        # BufferedPaintDC should reduce flickering\n        dc = wx.BufferedPaintDC(self)\n        background_brush = wx.Brush(self.GetBackgroundColour(), wx.SOLID)\n        dc.SetBackground(background_brush)\n        dc.Clear()\n        # Draw slider\n        track_height = 12\n        dc.SetPen(wx.Pen(self.slider_outline_color, width=1, style=wx.PENSTYLE_SOLID))\n        dc.SetBrush(wx.Brush(self.slider_background_color, style=wx.BRUSHSTYLE_SOLID))\n        dc.DrawRectangle(self.border_width, h/2 - track_height/2, w - 2 * self.border_width, track_height)\n        # Draw selected range\n        if self.IsEnabled():\n            dc.SetPen(wx.Pen(self.selected_range_outline_color, width=1, style=wx.PENSTYLE_SOLID))\n            dc.SetBrush(wx.Brush(self.selected_range_color, style=wx.BRUSHSTYLE_SOLID))\n        else:\n            dc.SetPen(wx.Pen(self.slider_outline_color, width=1, style=wx.PENSTYLE_SOLID))\n            dc.SetBrush(wx.Brush(self.slider_outline_color, style=wx.BRUSHSTYLE_SOLID))\n        low_pos = self.thumbs['low'].GetPosition()[0]\n        high_pos = self.thumbs['high'].GetPosition()[0]\n        dc.DrawRectangle(low_pos, h / 2 - track_height / 4, high_pos - low_pos, track_height / 2)\n        # Draw thumbs\n        for thumb in self.thumbs.values():\n            thumb.OnPaint(dc)\n        evt.Skip()\n    def OnEraseBackground(self, evt):\n        # This should reduce flickering\n        pass\n    def GetValues(self):\n        return self.thumbs['low'].value, self.thumbs['high'].value\n    def SetValues(self, lowValue, highValue):\n        if lowValue > highValue:\n            lowValue, highValue = highValue, lowValue\n        lowValue = max(lowValue, self.min_value)\n        highValue = min(highValue, self.max_value)\n        self.thumbs['low'].SetValue(lowValue)\n        self.thumbs['high'].SetValue(highValue)\n        self.Refresh()\n    def GetMax(self):\n        return self.max_value\n    def GetMin(self):\n        return self.min_value\n    def SetMax(self, maxValue):\n        if maxValue < self.min_value:\n            maxValue = self.min_value\n        _, old_high = self.GetValues()\n        if old_high > maxValue:\n            self.thumbs['high'].SetValue(maxValue)\n        self.max_value = maxValue\n        self.Refresh()\n    def SetMin(self, minValue):\n        if minValue > self.max_value:\n            minValue = self.max_value\n        old_low, _ = self.GetValues()\n        if old_low < minValue:\n            self.thumbs['low'].SetValue(minValue)\n        self.min_value = minValue\n        self.Refresh()\nclass TestFrame(wx.Frame):\n    def __init__(self):\n        wx.Frame.__init__(self, None, -1, 'Range Slider Demo', size=(300, 100))\n        panel = wx.Panel(self)\n        b = 6\n        vbox = wx.BoxSizer(orient=wx.VERTICAL)\n        vbox.Add(wx.StaticText(parent=panel, label='Custom Range Slider:'), flag=wx.ALIGN_LEFT | wx.ALL, border=b)\n        self.rangeslider = RangeSlider(parent=panel, lowValue=20, highValue=80, minValue=0, maxValue=100,\n                                       size=(300, 26))\n        self.rangeslider.Bind(wx.EVT_SLIDER, self.rangeslider_changed)\n        vbox.Add(self.rangeslider, proportion=1, flag=wx.EXPAND | wx.ALL, border=b)\n        self.rangeslider_static = wx.StaticText(panel)\n        vbox.Add(self.rangeslider_static, flag=wx.ALIGN_LEFT | wx.ALL, border=b)\n        vbox.Add(wx.StaticText(parent=panel, label='Regular Slider with wx.SL_SELRANGE style:'),\n                 flag=wx.ALIGN_LEFT | wx.ALL, border=b)\n        self.slider = wx.Slider(parent=panel, style=wx.SL_SELRANGE)\n        self.slider.SetSelection(20, 40)\n        self.slider.Bind(wx.EVT_SLIDER, self.slider_changed)\n        vbox.Add(self.slider, proportion=1, flag=wx.EXPAND | wx.ALL, border=b)\n        self.slider_static = wx.StaticText(panel)\n        vbox.Add(self.slider_static, flag=wx.ALIGN_LEFT | wx.ALL, border=b)\n        self.button_toggle = wx.Button(parent=panel, label='Disable')\n        self.button_toggle.Bind(wx.EVT_BUTTON, self.toggle_slider_enable)\n        vbox.Add(self.button_toggle, flag=wx.ALIGN_CENTER | wx.ALL, border=b)\n        panel.SetSizerAndFit(vbox)\n        box = wx.BoxSizer()\n        box.Add(panel, proportion=1, flag=wx.EXPAND)\n        self.SetSizerAndFit(box)\n    def slider_changed(self, evt):\n        obj = evt.GetEventObject()\n        val = obj.GetValue()\n        self.slider_static.SetLabel('Value: {}'.format(val))\n    def rangeslider_changed(self, evt):\n        obj = evt.GetEventObject()\n        lv, hv = obj.GetValues()\n        self.rangeslider_static.SetLabel('Low value: {:.0f}, High value: {:.0f}'.format(lv, hv))\n    def toggle_slider_enable(self, evt):\n        if self.button_toggle.GetLabel() == 'Disable':\n            self.slider.Enable(False)\n            self.rangeslider.Enable(False)\n            self.button_toggle.SetLabel('Enable')\n        else:\n            self.slider.Enable(True)\n            self.rangeslider.Enable(True)\n            self.button_toggle.SetLabel('Disable')\ndef main():\n    app = wx.App()\n    TestFrame().Show()\n    app.MainLoop()\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/883370/python-multiprocessing-atexit-error-error-in-atexit-run-exitfuncs",
    "body": "import sys\nimport time\nfrom multiprocessing import Process\ndef main():\n    # Set up inputs..\n    # Spawn processes\n    try:\n        processes = [Proc(1), Proc(2)]\n        [p.start() for p in processes]\n        [p.join() for p in processes]\n    except KeyboardInterrupt:\n        pass\nclass Proc(Process):\n    def __init__(self, procNum):\n        self.id = procNum\n        Process.__init__(self)\n    def run(self):\n        while True:\n            try:\n                # Do work...\n                time.sleep(1)\n                sys.stdout.write('.')\n            except KeyboardInterrupt:\n                print(\"User aborted.\")\n                break\n# Main Entry\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/67093166/pylint-pylint-django-does-not-work-when-django-settings-module-flag-is-speci",
    "body": "Traceback (most recent call last):\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint_django/checkers/foreign_key_strings.py\", line 92, in open\n    django.setup()\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/django/__init__.py\", line 19, in setup\n    configure_logging(settings.LOGGING_CONFIG, settings.LOGGING)\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/django/conf/__init__.py\", line 102, in __getattr__\n    self._setup(name)\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/django/conf/__init__.py\", line 82, in _setup\n    raise ImproperlyConfigured(\ndjango.core.exceptions.ImproperlyConfigured: Requested setting LOGGING_CONFIG, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint_django/checkers/foreign_key_strings.py\", line 120, in open\n    settings.configure(Settings(self.config.django_settings_module))\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/django/conf/__init__.py\", line 217, in __init__\n    mod = importlib.import_module(self.SETTINGS_MODULE)\n  File \"/usr/local/python/3.10.12/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'backend.settings'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/workspaces/checkout/backend/.venv/bin/pylint\", line 8, in <module>\n    sys.exit(run_pylint())\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/__init__.py\", line 36, in run_pylint\n    PylintRun(argv or sys.argv[1:])\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/lint/run.py\", line 215, in __init__\n    linter.check(args)\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/lint/pylinter.py\", line 713, in check\n    with self._astroid_module_checker() as check_astroid_module:\n  File \"/usr/local/python/3.10.12/lib/python3.10/contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/lint/pylinter.py\", line 1015, in _astroid_module_checker\n    checker.open()\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint_django/checkers/foreign_key_strings.py\", line 125, in open\n    self.add_message(\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/checkers/base_checker.py\", line 164, in add_message\n    self.linter.add_message(\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/lint/pylinter.py\", line 1341, in add_message\n    self._add_one_message(\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/lint/pylinter.py\", line 1274, in _add_one_message\n    self.stats.increase_single_module_message_count(\n  File \"/workspaces/checkout/backend/.venv/lib/python3.10/site-packages/pylint/utils/linterstats.py\", line 315, in increase_single_module_message_count\n    self.by_module[modname][type_name] += increase\nKeyError: 'Command line or configuration file'"
  },
  {
    "url": "https://stackoverflow.com/questions/64689342/plotly-how-to-add-volume-to-a-candlestick-chart",
    "body": "import pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n# data\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\n# Create subplots and mention plot grid size\nfig = make_subplots(rows=2, cols=1, shared_xaxes=True,\n               vertical_spacing=0.03, subplot_titles=('OHLC', 'Volume'),\n               row_width=[0.2, 0.7])\n# Plot OHLC on 1st row\nfig.add_trace(go.Candlestick(x=df[\"Date\"], open=df[\"AAPL.Open\"], high=df[\"AAPL.High\"],\n                low=df[\"AAPL.Low\"], close=df[\"AAPL.Close\"], name=\"OHLC\"),\n                row=1, col=1\n)\n# Bar trace for volumes on 2nd row without legend\nfig.add_trace(go.Bar(x=df['Date'], y=df['AAPL.Volume'], showlegend=False), row=2, col=1)\n# Do not show OHLC's rangeslider plot\nfig.update(layout_xaxis_rangeslider_visible=False)\nfig.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/12526606/callback-for-celery-apply-async",
    "body": "from celery import Task\nclass CallbackTask(Task):\n    def on_success(self, retval, task_id, args, kwargs):\n        '''\n        retval – The return value of the task.\n        task_id – Unique id of the executed task.\n        args – Original arguments for the executed task.\n        kwargs – Original keyword arguments for the executed task.\n        '''\n        pass\n\n    def on_failure(self, exc, task_id, args, kwargs, einfo):\n        '''\n        exc – The exception raised by the task.\n        task_id – Unique id of the failed task.\n        args – Original arguments for the task that failed.\n        kwargs – Original keyword arguments for the task that failed.\n        '''\n        pass"
  },
  {
    "url": "https://stackoverflow.com/questions/15899861/efficient-term-document-matrix-with-nltk",
    "body": "import textmining\n\n# Create some very short sample documents\ndoc1 = 'John and Bob are brothers.'\ndoc2 = 'John went to the store. The store was closed.'\ndoc3 = 'Bob went to the store too.'\n# Initialize class to create term-document matrix\ntdm = textmining.TermDocumentMatrix()\n# Add the documents\ntdm.add_doc(doc1)\ntdm.add_doc(doc2)\ntdm.add_doc(doc3)\n# Write matrix file -- cutoff=1 means words in 1+ documents are retained\ntdm.write_csv('matrix.csv', cutoff=1)\n# Instead of writing the matrix, access its rows directly\nfor row in tdm.rows(cutoff=1):\n    print row"
  },
  {
    "url": "https://stackoverflow.com/questions/7755501/embed-picture-in-email",
    "body": "from email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText  # Added\nfrom email.mime.image import MIMEImage\nattachment = 'bob.jpg'\nmsg = MIMEMultipart()\nmsg[\"To\"] = to_addr\nmsg[\"From\"] = from_addr\nmsg[\"Subject\"] = subject\nmsgText = MIMEText('<b>%s</b><br/><img src=\"cid:%s\"/><br/>' % (body, attachment), 'html')\nmsg.attach(msgText)   # Added, and edited the previous line\nwith open(attachment, 'rb') as fp:\n    img = MIMEImage(fp.read())\nimg.add_header('Content-ID', '<{}>'.format(attachment))\nmsg.attach(img)\nprint(msg.as_string()) # or go ahead and send it"
  },
  {
    "url": "https://stackoverflow.com/questions/61366664/how-to-upsert-pandas-dataframe-to-postgresql-table",
    "body": "import sqlalchemy as sa\n# …\nwith engine.begin() as conn:\n    # step 0.0 - create test environment\n    conn.exec_driver_sql(\"DROP TABLE IF EXISTS main_table\")\n    conn.exec_driver_sql(\n        \"CREATE TABLE main_table (id int primary key, txt varchar(50))\"\n    )\n    conn.exec_driver_sql(\n        \"INSERT INTO main_table (id, txt) VALUES (1, 'row 1 old text')\"\n    )\n    # step 0.1 - create DataFrame to UPSERT\n    df = pd.DataFrame(\n        [(2, \"new row 2 text\"), (1, \"row 1 new text\")], columns=[\"id\", \"txt\"]\n    )\n\n    # step 1 - create temporary table and upload DataFrame\n    conn.exec_driver_sql(\n        \"CREATE TEMPORARY TABLE temp_table AS SELECT * FROM main_table WHERE false\"\n    )\n    df.to_sql(\"temp_table\", conn, index=False, if_exists=\"append\")\n    # step 2 - merge temp_table into main_table\n    conn.exec_driver_sql(\n        \"\"\"\\\n        INSERT INTO main_table (id, txt)\n        SELECT id, txt FROM temp_table\n        ON CONFLICT (id) DO\n            UPDATE SET txt = EXCLUDED.txt\n        \"\"\"\n    )\n    # step 3 - confirm results\n    result = conn.exec_driver_sql(\"SELECT * FROM main_table ORDER BY id\").all()\n    print(result)  # [(1, 'row 1 new text'), (2, 'new row 2 text')]"
  },
  {
    "url": "https://stackoverflow.com/questions/44385652/add-senders-name-in-the-from-field-of-the-email-in-python",
    "body": "import smtplib\nfrom email.mime.text import MIMEText\ndef send_email(to=['example@example.com'],\n               f_host='example.example.com',\n               f_port=587,\n               f_user='example@example.com',\n               f_passwd='example-pass',\n               subject='default subject',\n               message='content message'):\n    smtpserver = smtplib.SMTP(f_host, f_port)\n    smtpserver.ehlo()\n    smtpserver.starttls()\n    smtpserver.ehlo\n    smtpserver.login(f_user, f_passwd)  # from email credential\n    msg = MIMEText(message, 'html')\n    msg['Subject'] = 'My custom Subject'\n    msg['From'] = \"Your name <Your email>\"\n    msg['To'] = ','.join(to)\n    for t in to:\n        smtpserver.sendmail(f_user, t, msg.as_string())  # you just need to add\n                                                         # this in for loop in\n                                                         # your code.\n    smtpserver.close()\n    print('Mail is sent successfully!!')\n    cont = \"\"\"\n    <html>\n    <head></head>\n    <body>\n    <p>Hi!<br>\n      How are you?<br>\n      Here is the <a href=\"http://www.google.com\">link</a> you wanted.\n    </p>\n    </body>\n    </html>\n    \"\"\"\n    try:\n        send_email(message=cont)\n    except:\n        print('Mail could not be sent')"
  },
  {
    "url": "https://stackoverflow.com/questions/55749899/training-a-simple-model-in-tensorflow-gpu-slower-than-cpu",
    "body": "import tensorflow as tf\nimport time\ncpu_times = []\nsizes = [1, 10, 100, 500, 1000, 2000, 3000, 4000, 5000, 8000, 10000]\nfor size in sizes:\n    tf.reset_default_graph()\n    start = time.time()\n    with tf.device('cpu:0'):\n        v1 = tf.Variable(tf.random_normal((size, size)))\n        v2 = tf.Variable(tf.random_normal((size, size)))\n        op = tf.matmul(v1, v2)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(op)\n    cpu_times.append(time.time() - start)\n    print('cpu time took: {0:.4f}'.format(time.time() - start))\nimport tensorflow as tf\nimport time\ngpu_times = []\nfor size in sizes:\n    tf.reset_default_graph()\n    start = time.time()\n    with tf.device('gpu:0'):\n        v1 = tf.Variable(tf.random_normal((size, size)))\n        v2 = tf.Variable(tf.random_normal((size, size)))\n        op = tf.matmul(v1, v2)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(op)\n    gpu_times.append(time.time() - start)\n    print('gpu time took: {0:.4f}'.format(time.time() - start))\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(sizes, gpu_times, label='GPU')\nax.plot(sizes, cpu_times, label='CPU')\nplt.xlabel('MATRIX SIZE')\nplt.ylabel('TIME (sec)')\nplt.legend()\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/12435765/can-a-decorated-function-access-variables-of-the-decorator",
    "body": "def funcDec(func):\n    localVariable = \"I'm a local string\"\n    # Local variable(s) to inject into wrapped func.\n    context = {'localVariable': localVariable}\n    def wrapped(*args):\n        func_globals = func.__globals__\n        # Save copy of any global values that will be replaced.\n        saved_values = {key: func_globals[key] for key in context if key in func_globals}\n        func_globals.update(context)\n        print(f'Calling localVariable from funcDec: {localVariable!r}')\n        try:\n            func(*args)\n        finally:\n            func_globals.update(saved_values)  # Restore any replaced globals.\n        print(f'done with calling {func.__name__}()')\n    return wrapped\n@funcDec\ndef f1(x, y):\n    print(x + y)\n    print(f'Calling funcDec localVariable from f1: {localVariable!r}')\nf1(2, 3)"
  },
  {
    "url": "https://stackoverflow.com/questions/62722416/plot-confusion-matrix-for-multilabel-classifcation-python",
    "body": "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.tree import DecisionTreeClassifier\nX, y = make_multilabel_classification(n_samples=1000,\n                                      n_classes=15, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)\ntree = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\ny_pred = tree.predict(X_test)\nf, axes = plt.subplots(3, 5, figsize=(25, 15))\naxes = axes.ravel()\nfor i in range(15):\n    disp = ConfusionMatrixDisplay(confusion_matrix(y_test[:, i],\n                                                   y_pred[:, i]),\n                                  display_labels=[0, i])\n    disp.plot(ax=axes[i], values_format='.4g')\n    disp.ax_.set_title(f'class {i}')\n    if i<10:\n        disp.ax_.set_xlabel('')\n    if i%5!=0:\n        disp.ax_.set_ylabel('')\n    disp.im_.colorbar.remove()\nplt.subplots_adjust(wspace=0.10, hspace=0.1)\nf.colorbar(disp.im_, ax=axes)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/55578387/email-verification-in-django",
    "body": "from django.contrib.auth import get_user_model\nfrom django.utils.http import urlsafe_base64_encode, urlsafe_base64_decode\nfrom django.contrib.sites.shortcuts import get_current_site\nfrom .tokens import account_activation_token\nfrom django.core.mail import send_mail\nfrom django.utils.encoding import force_bytes\nfrom django.template.loader import render_to_string\ndef signup(request):\n    User = get_user_model()\n    if request.method == 'POST':\n        form = SignupForm(request.POST)\n        if form.is_valid():\n            email = form.cleaned_data.get('email')\n            if Yourmodel.objects.filter(email__iexact=email).count() == 1:\n                user = form.save(commit=False)\n                user.is_active = False\n                user.save()\n                current_site = get_current_site(request)\n                mail_subject = 'Activate your account.'\n                message = render_to_string('email_template.html', {\n                            'user': user,\n                            'domain': current_site.domain,\n                            'uid': urlsafe_base64_encode(force_bytes(user.pk)),\n                            'token': account_activation_token.make_token(user),\n                        })\n                to_email = form.cleaned_data.get('email')\n                send_mail(mail_subject, message, 'youremail', [to_email])\n                return HttpResponse('Please confirm your email address to complete the registration')\n     else:\n        form = SignupForm()\n    return render(request, 'regform.html', {'form': form})\ndef activate(request, uidb64, token):\n    User = get_user_model()\n    try:\n        uid = force_text(urlsafe_base64_decode(uidb64))\n        user = User.objects.get(pk=uid)\n    except(TypeError, ValueError, OverflowError, User.DoesNotExist):\n        user = None\n    if user is not None and account_activation_token.check_token(user, token):\n        user.is_active = True\n        user.save()\n        return HttpResponse('Thank you for your email confirmation. Now you can login your account.')\n    else:\n        return HttpResponse('Activation link is invalid!')"
  },
  {
    "url": "https://stackoverflow.com/questions/56086764/section-divider-in-spyder",
    "body": "#%% Notes\n#### Defining Code Cells\n# A “code cell” in Spyder is a block of lines, typically in a script, that can be\n# easily executed all at once.\n# You can separate cells by lines starting with either:\n# 1) #%% (standard cell separator)\n# 2) # %% (standard cell separator, when file has been edited with Eclipse)\n# 3) # <codecell> (IPython notebook cell separator)\n#### Cell heirarchy\n# To nest navigable sections within a cell, use \"#### ~some heading~\"\n# To nest subcells within a cell, use \"#%% >> #%%% >> #%%%% .... \""
  },
  {
    "url": "https://stackoverflow.com/questions/41569206/flask-sqlalchemy-foreign-key-relationships",
    "body": "class Request(db.Model):\n    __tablename__ = 'request'\n    id = db.Column(db.Integer, primary_key=True)\n    applicationdate = db.Column(db.DateTime)\nclass Agent(db.Model):\n    __tablename__ = 'agent'\n    id = db.Column(db.Integer, primary_key=True)\n    request_id = db.Column(db.Integer, db.ForeignKey('request.id'))\n    request = db.relationship(\"Request\", backref=backref(\"request\", uselist=False))\n    name = db.Column(db.String(80))\n    company = db.Column(db.String(80))\n    address = db.Column(db.String(180))"
  },
  {
    "url": "https://stackoverflow.com/questions/26108436/how-can-i-get-the-matplotlib-rgb-color-given-the-colormap-name-boundrynorm-an",
    "body": "import numpy as np\n# setup the plot\nfig, ax = plt.subplots(1,1, figsize=(6,6))\n# define the data between 0 and 20\nNUM_VALS = 20\nx = np.random.uniform(0, NUM_VALS, size=NUM_VALS)\ny = np.random.uniform(0, NUM_VALS, size=NUM_VALS)\n# define the color chart between 2 and 10 using the 'autumn_r' colormap, so\n#   y <= 2  is yellow\n#   y >= 10 is red\n#   2 < y < 10 is between from yellow to red, according to its value\nCOL = MplColorHelper('autumn_r', 2, 10)\nscat = ax.scatter(x,y,s=300, c=COL.get_rgb(y))\nax.set_title('Well defined discrete colors')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/21356439/how-to-load-and-play-a-video-in-pygame",
    "body": "import pygame\nimport cv2\nvideo = cv2.VideoCapture(\"video.mp4\")\nsuccess, video_image = video.read()\nfps = video.get(cv2.CAP_PROP_FPS)\nwindow = pygame.display.set_mode(video_image.shape[1::-1])\nclock = pygame.time.Clock()\nrun = success\nwhile run:\n    clock.tick(fps)\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            run = False\n\n    success, video_image = video.read()\n    if success:\n        video_surf = pygame.image.frombuffer(\n            video_image.tobytes(), video_image.shape[1::-1], \"BGR\")\n    else:\n        run = False\n    window.blit(video_surf, (0, 0))\n    pygame.display.flip()\npygame.quit()\nexit()"
  },
  {
    "url": "https://stackoverflow.com/questions/69527239/what-is-context-variable-in-airflow-operators",
    "body": "        return {\n            'conf': conf,\n            'dag': task.dag,\n            'dag_run': dag_run,\n            'ds': ds,\n            'ds_nodash': ds_nodash,\n            'execution_date': pendulum.instance(self.execution_date),\n            'inlets': task.inlets,\n            'macros': macros,\n            'next_ds': next_ds,\n            'next_ds_nodash': next_ds_nodash,\n            'next_execution_date': next_execution_date,\n            'outlets': task.outlets,\n            'params': params,\n            'prev_ds': prev_ds,\n            'prev_ds_nodash': prev_ds_nodash,\n            'prev_execution_date': prev_execution_date,\n            'prev_execution_date_success': lazy_object_proxy.Proxy(\n                lambda: self.get_previous_execution_date(state=State.SUCCESS)\n            ),\n            'prev_start_date_success': lazy_object_proxy.Proxy(\n                lambda: self.get_previous_start_date(state=State.SUCCESS)\n            ),\n            'run_id': run_id,\n            'task': task,\n            'task_instance': self,\n            'task_instance_key_str': ti_key_str,\n            'test_mode': self.test_mode,\n            'ti': self,\n            'tomorrow_ds': tomorrow_ds,\n            'tomorrow_ds_nodash': tomorrow_ds_nodash,\n            'ts': ts,\n            'ts_nodash': ts_nodash,\n            'ts_nodash_with_tz': ts_nodash_with_tz,\n            'var': {\n                'json': VariableJsonAccessor(),\n                'value': VariableAccessor(),\n            },\n            'yesterday_ds': yesterday_ds,\n            'yesterday_ds_nodash': yesterday_ds_nodash,\n        }"
  },
  {
    "url": "https://stackoverflow.com/questions/67476156/pass-commandline-arguments-to-a-python-script-installed-with-poetry",
    "body": "import argparse\ndef some_function(target, end=\"!\"):\n    \"\"\"Some example funcion\"\"\"\n    msg = \"hi \" + target + end\n    print(msg)\ndef start():\n    # All the logic of argparse goes in this function\n    parser = argparse.ArgumentParser(description='Say hi.')\n    parser.add_argument('target', type=str, help='the name of the target')\n    parser.add_argument('--end', dest='end', default=\"!\",\n                    help='sum the integers (default: find the max)')\n    args = parser.parse_args()\n    some_function(args.target, end=args.end)"
  },
  {
    "url": "https://stackoverflow.com/questions/67476156/pass-commandline-arguments-to-a-python-script-installed-with-poetry",
    "body": "# run with poetry\n$ poetry run my-script\n# install the proyect (this will create a virtualenv if you didn't have it created)\n$ poetry install\n# activate the virtualenv\n$ poetry shell\n# run the script\n$ my-script --help\nusage: my-script [-h] [--end END] target\nSay hi.\npositional arguments:\n  target      the name of the target\noptional arguments:\n  -h, --help  show this help message and exit\n  --end END   sum the integers (default: find the max)\n$ my-script \"spanish inquisition\" --end \"?\"\nhi spanish inquisition?"
  },
  {
    "url": "https://stackoverflow.com/questions/45473501/getting-pil-pillow-4-2-1-to-upload-properly-to-aws-lambda-py3-6",
    "body": "#!/bin/sh\nset -ex\ncd /code/\nif [ ! -d \"PIL\" ]; then\n\t# Create virtual env, activate it and install PIL\n\tvirtualenv env && source env/bin/activate && pip install pillow requests\n\t# Copy necessary files to the root folder\n\trm -f build-output.zip\n\t#mkdir PIL\n\tcp -f -r env/lib/python3.6/site-packages/PIL .\n\tcp -f -r env/lib/python3.6/site-packages/requests .\n\n\t# below are the dependencies for the requests pkg\n\tcp -f -r env/lib/python3.6/site-packages/urllib3 .\n\tcp -f -r env/lib/python3.6/site-packages/certifi .\n\tcp -f -r env/lib/python3.6/site-packages/chardet .\n\tcp -f -r env/lib/python3.6/site-packages/idna .\n\n\t# Remove temp files\n\trm -r env\nfi\n# ZIP it\nzip -9 build-output *.py\nzip -9 -r build-output PIL\nzip -9 -r build-output requests\nzip -9 -r build-output urllib3\nzip -9 -r build-output certifi\nzip -9 -r build-output chardet\nzip -9 -r build-output idna"
  },
  {
    "url": "https://stackoverflow.com/questions/36194865/configure-a-first-cell-by-default-in-jupyter-notebooks",
    "body": "    define([\n        'base/js/namespace'\n    ], function(\n        Jupyter\n    ) {\n        function load_ipython_extension() {\n          if (Jupyter.notebook.get_cells().length===1){\n       //do your thing\n            Jupyter.notebook.insert_cell_above('code', 0).set_text(\"# Scientific libraries\\nimport numpy as np\\nimport scipy\\n\\n# import Pandas\\n\\nimport pandas as pd\\n\\n# Graphic libraries\\n\\nimport matplotlib as plt\\n%matplotlib inline\\nimport seaborn as sns\\nfrom plotly.offline import init_notebook_mode, iplot, download_plotlyjs\\ninit_notebook_mode()\\nimport plotly.graph_objs as go\\n\\n# Extra options \\n\\npd.options.display.max_rows = 10\\npd.set_option('max_columns', 50)\\nsns.set(style='ticks', context='talk')\\n\\n# Creating alias for magic commands\\n%alias_magic t time\");\n          }\n        }\n        return {\n            load_ipython_extension: load_ipython_extension\n        };\n    });"
  },
  {
    "url": "https://stackoverflow.com/questions/36194865/configure-a-first-cell-by-default-in-jupyter-notebooks",
    "body": "    default_cells\n    =========\n\n    Add default cells to each new notebook. You have to modify this line in the main.js file to change your default cell. For example\n\n    `Jupyter.notebook.insert_cell_above('code', 0).set_text(\"import numpy as np/nimportpandas as pd\")`\n\n\n    You can also add another default cell by creating a new line just below :\n\n    `Jupyter.notebook.insert_cell_above('code', 1).set_text(\"from sklearn.meatrics import mean_squared_error\")`\n\n    **Don't forget to increment 1 if you want more than one extra cell. **"
  },
  {
    "url": "https://stackoverflow.com/questions/57287400/find-all-the-numbers-in-one-file-that-are-not-in-another-file-in-python",
    "body": "i1 = iter(open(\"3k.txt\"))\ni2 = iter(open(\"2k.txt\"))\na = int(next(i1))\nb = int(next(i2))\naNotB = []\n# bNotA = []\nwhile True:\n    try:\n        if a < b:\n            aNotB += [a]\n            a = int(next(i1, None))\n        elif a > b:\n            # bNotA += [a]\n            b = int(next(i2, None))\n        elif a == b:\n            a = int(next(i1, None))\n            b = int(next(i2, None))\n    except TypeError:\n        if not b:\n            aNotB += list(i1)\n            break\n        else:\n            # bNotA += list(i1)\n            break\nprint(aNotB)"
  },
  {
    "url": "https://stackoverflow.com/questions/54264073/what-is-the-use-and-when-to-use-classmethod-in-python",
    "body": "class Person():\n    species='homo_sapiens' # This is class variable\n    def __init__(self, name, age):\n        self.name = name # This is instance variable\n        self.age = age\n    def show(self):\n        print('Name: {}, age: {}.'.format(self.name, date.today().year - self.age))\n\n    @classmethod\n    def create_with_birth_year(cls, name, birth_year):\n        return cls(name, date.today().year - birth_year)\n\t@classmethod\n\tdef print_species(cls):\n\t    print('species: {}'.format(cls.species))\n\n    @staticmethod\n    def get_birth_year(age):\n        return date.today().year - age\nclass Teacher(Person):\n    pass"
  },
  {
    "url": "https://stackoverflow.com/questions/41796290/unhashable-type-error-in-pandas-dataframe",
    "body": "PS C:\\Users\\rascoussier\\python\\stackoverflow\\type_error_with_df_loc> C:\\Users\\rascoussier\\Anaconda3\\envs\\elastic-1\\python.exe .\\main.py\n   A  B  C  D  E  F\n0  1  4  7  1  5  7\n1  2  5  8  3  3  4\n2  3  6  9  5  6  3\nTraceback (most recent call last):\n  File \"C:\\Users\\rascoussier\\python\\stackoverflow\\type_error_with_df_loc\\main.py\", line 12, in <module>\n    print(df.loc(\n  File \"C:\\Users\\rascoussier\\Anaconda3\\envs\\elastic-1\\lib\\site-packages\\pandas\\core\\indexing.py\", line 634, in __call__\n    axis = self.obj._get_axis_number(axis)\n  File \"C:\\Users\\rascoussier\\Anaconda3\\envs\\elastic-1\\lib\\site-packages\\pandas\\core\\generic.py\", line 550, in _get_axis_number\n    return cls._AXIS_TO_AXIS_NUMBER[axis]\nTypeError: unhashable type: 'Series'"
  },
  {
    "url": "https://stackoverflow.com/questions/25764201/how-to-work-with-the-scrapy-contracts",
    "body": "F..\n======================================================================\nFAIL: [example] parse (@returns post-hook)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/adnauseum/.virtualenvs/scrapy_testing-CfFR3tdG/lib/python3.7/site-packages/scrapy/contracts/__init__.py\", line 151, in wrapper\n    self.post_process(output)\n  File \"/Users/adnauseum/.virtualenvs/scrapy_testing-CfFR3tdG/lib/python3.7/site-packages/scrapy/contracts/default.py\", line 90, in post_process\n    (occurrences, self.obj_name, expected))\nscrapy.exceptions.ContractFail: Returned 10 items, expected 0\n----------------------------------------------------------------------"
  },
  {
    "url": "https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab",
    "body": "import threading\nimport time\nimport os\nimport asyncio\nfrom pyngrok import ngrok\nimport threading\nimport queue\nimport time\nfrom threading import Thread\n# Get your ngrok token from your ngrok account:\n# https://dashboard.ngrok.com/get-started/your-authtoken\ntoken=\"your token goes here - don't forget to replace this with it!\"\nngrok.set_auth_token(token)\n# set up a stoppable thread (not mandatory, but cleaner if you want to stop this later\nclass StoppableThread(threading.Thread):\n    def __init__(self, *args, **kwargs):\n        super(StoppableThread, self).__init__(*args, **kwargs)\n        self._stop_event = threading.Event()\n    def stop(self):\n        self._stop_event.set()\n    def is_stopped(self):\n        return self._stop_event.is_set()\ndef start_ngrok(q, stop_event):\n    try:\n        # Start an HTTP tunnel on the specified port\n        public_url = ngrok.connect(11434)\n        # Put the public URL in the queue\n        q.put(public_url)\n        # Keep the thread alive until stop event is set\n        while not stop_event.is_set():\n            time.sleep(1)  # Adjust sleep time as needed\n    except Exception as e:\n        print(f\"Error in start_ngrok: {e}\")"
  },
  {
    "url": "https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab",
    "body": "import os\nimport asyncio\n# NB: You may need to set these depending and get cuda working depending which backend you are running.\n# Set environment variable for NVIDIA library\n# Set environment variables for CUDA\nos.environ['PATH'] += ':/usr/local/cuda/bin'\n# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\nos.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\nasync def run_process(cmd):\n    print('>>> starting', *cmd)\n    process = await asyncio.create_subprocess_exec(\n        *cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE\n    )\n    # define an async pipe function\n    async def pipe(lines):\n        async for line in lines:\n            print(line.decode().strip())\n        await asyncio.gather(\n            pipe(process.stdout),\n            pipe(process.stderr),\n        )\n    # call it\n    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"
  },
  {
    "url": "https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab",
    "body": "import asyncio\nimport threading\nasync def start_ollama_serve():\n    await run_process(['ollama', 'serve'])\ndef run_async_in_thread(loop, coro):\n    asyncio.set_event_loop(loop)\n    loop.run_until_complete(coro)\n    loop.close()\n# Create a new event loop that will run in a new thread\nnew_loop = asyncio.new_event_loop()\n# Start ollama serve in a separate thread so the cell won't block execution\nthread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\nthread.start()"
  },
  {
    "url": "https://stackoverflow.com/questions/65041691/is-python-dictionary-async-safe",
    "body": "# correct (1), using double check\nif key not in d:\n    value = await read_value()\n    # Check again whether the key is vacant. Since there are no awaits\n    # between this check and the update, the operation is atomic.\n    if key not in d:\n        d[key] = value\n# correct (2), using a shared asyncio.Lock:\nasync with d_lock:\n    # Single check is sufficient because the lock ensures that\n    # no one can modify the dict while we're reading the value.\n    if key not in d:\n        d[key] = await read_value()"
  },
  {
    "url": "https://stackoverflow.com/questions/63264888/pydantic-using-property-getter-decorator-for-a-field-with-an-alias",
    "body": "class PropertyBaseModel(BaseModel):\n    \"\"\"\n    Workaround for serializing properties with pydantic until\n    https://github.com/samuelcolvin/pydantic/issues/935\n    is solved\n    \"\"\"\n    @classmethod\n    def get_properties(cls):\n        return [prop for prop in dir(cls) if isinstance(getattr(cls, prop), property) and prop not in (\"__values__\", \"fields\")]\n    def dict(\n        self,\n        *,\n        include: Union['AbstractSetIntStr', 'MappingIntStrAny'] = None,\n        exclude: Union['AbstractSetIntStr', 'MappingIntStrAny'] = None,\n        by_alias: bool = False,\n        skip_defaults: bool = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -> 'DictStrAny':\n        attribs = super().dict(\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            skip_defaults=skip_defaults,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none\n        )\n        props = self.get_properties()\n        # Include and exclude properties\n        if include:\n            props = [prop for prop in props if prop in include]\n        if exclude:\n            props = [prop for prop in props if prop not in exclude]\n        # Update the attribute dict with the properties\n        if props:\n            attribs.update({prop: getattr(self, prop) for prop in props})\n        return attribs"
  },
  {
    "url": "https://stackoverflow.com/questions/58089300/python-how-to-override-type-hint-on-an-instance-attribute-in-a-subclass",
    "body": "from abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar\nSomethingT = TypeVar('SomethingT', bound='Something')\n...\nclass Foo(ABC, Generic[SomethingT]):\n    my_class: SomethingT\n    def __init__(self):\n        self.my_class = self.get_something()\n    @abstractmethod\n    def get_something(self) -> SomethingT:\n        pass\nclass SubclassOfFoo(Foo[SubclassOfSomething]):\n    def __init__(self):\n        super().__init__()\n    def get_something(self) -> SubclassOfSomething:\n        return SubclassOfSomething()\n    def do_something_special(self):\n        # inferred type of `self.my_class` will be `SubclassOfSomething`\n        self.my_class.something_special()"
  },
  {
    "url": "https://stackoverflow.com/questions/69046990/how-to-pass-dependency-files-to-sagemaker-sklearnprocessor-and-use-it-in-pipelin",
    "body": "from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\nsklearn_processor = SKLearnProcessor(\n    framework_version=\"0.20.0\",\n    role=role,\n    instance_type=\"ml.m5.xlarge\",\n    instance_count=1,\n)\nsklearn_processor.run(\n    code=\"preprocessing.py\",  # <- this gets uploaded as /opt/ml/processing/input/code/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='/opt/ml/processing/input'),\n        # Send my_package as /opt/ml/processing/input/code/my_package/\n        ProcessingInput(source='my_package/', destination=\"/opt/ml/processing/input/code/my_package/\")\n    ],\n    outputs=[\n        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n    ],\n    arguments=[\"--train-test-split-ratio\", \"0.2\"],\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/40118037/how-can-i-detect-gaps-and-consecutive-periods-in-a-time-series-in-pandas",
    "body": "from datetime import datetime, timedelta\nimport pandas as pd\n# Construct dummy dataframe\ndates = pd.to_datetime([\n    '2016-08-03',\n    '2016-08-04',\n    '2016-08-05',\n    '2016-08-17',\n    '2016-09-05',\n    '2016-09-06',\n    '2016-09-07',\n    '2016-09-19'])\ndf = pd.DataFrame(dates, columns=['date'])\n# Take the diff of the first column (drop 1st row since it's undefined)\ndeltas = df['date'].diff()[1:]\n# Filter diffs (here days > 1, but could be seconds, hours, etc)\ngaps = deltas[deltas > timedelta(days=1)]\n# Print results\nprint(f'{len(gaps)} gaps with average gap duration: {gaps.mean()}')\nfor i, g in gaps.iteritems():\n    gap_start = df['date'][i - 1]\n    print(f'Start: {datetime.strftime(gap_start, \"%Y-%m-%d\")} | '\n          f'Duration: {str(g.to_pytimedelta())}')"
  },
  {
    "url": "https://stackoverflow.com/questions/71658991/how-to-apply-a-custom-function-in-polars-that-does-the-processing-row-by-row",
    "body": "def my_complicated_function(row: dict) -> int:\n    \"\"\"\n    A function that cannot utilize polars expressions.\n    This should be avoided.\n    \"\"\"\n    # a dict with column names as keys\n    print(f\"[DEBUG]: {row=}\")\n\n    # do some work\n    return row[\"foo\"] + row[\"bar\"] + row[\"baz\"]\ndf = pl.DataFrame({\n    \"foo\": [1, 2, 3],\n    \"bar\": [4, 5, 6],\n    \"baz\": [7, 8, 9]\n})\ndf = df.with_columns(\n    pl.struct(pl.all())\n      .map_elements(my_complicated_function, return_dtype=pl.Int64)\n      .alias(\"foo + bar + baz\")\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/67205522/set-order-on-sns-histplot",
    "body": "import pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame({\n    'Label' : ['A','B','C','B','B','C','C','B','B','A','C','A','B','A','C','A'],\n    'Item' : ['Up','Left','Up','Left','Down','Right','Up','Down','Right','Down','Right','Up','Up','Right','Down','Left'],\n   })\ndf['Item'] = pd.Categorical(df['Item'], ['Up','Down','Left','Right'])\ng = sns.histplot(data = df,\n            x = 'Item',\n            hue = 'Label',\n            #order = ['Up','Down','Left','Right'],\n            multiple = 'fill',\n            shrink = 0.8,\n            discrete = True,\n            legend = True,\n            )"
  },
  {
    "url": "https://stackoverflow.com/questions/63483246/how-to-call-an-api-from-another-api-in-fastapi",
    "body": "import requests\ndef test_function(request: Request, path_parameter: path_param):\n    request_example = {\"test\" : \"in\"}\n    host = request.client.host\n    data_source_id = path_parameter.id\n    get_test_url= f\"http://{host}/test/{id}/\"\n    get_inp_url = f\"http://{host}/test/{id}/inp\"\n    test_get_response = requests.get(get_test_url)\n    inp_post_response = requests.post(get_inp_url , json=request_example)\n    if inp_post_response .status_code == 200:\n        print(json.loads(test_get_response.content.decode('utf-8')))"
  },
  {
    "url": "https://stackoverflow.com/questions/52133482/how-to-omit-remove-virtual-environment-venv-from-python-coverage-unit-testin",
    "body": "$ coverage run --omit 'venv/*' -m unittest tests/*.py && coverage report -m\n........\n----------------------------------------------------------------------\nRan 8 tests in 0.023s\nOK\nName                      Stmts   Miss  Cover   Missing\n-------------------------------------------------------\nruterstop.py                 84      8    90%   177, 188, 191-197, 207\ntests/test_ruterstop.py     108      0   100%\n-------------------------------------------------------\nTOTAL                       192      8    96%"
  },
  {
    "url": "https://stackoverflow.com/questions/63000388/how-to-include-simpleimputer-before-countvectorizer-in-a-scikit-learn-pipeline",
    "body": "import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'text':['abc def', 'abc ghi', np.nan]})\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy='constant')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n# CREATE TRANSFORMER\nfrom sklearn.preprocessing import FunctionTransformer\none_dim = FunctionTransformer(np.reshape, kw_args={'newshape':-1})\n# INCLUDE TRANSFORMER IN PIPELINE\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(imp, one_dim, vect)\npipe.fit_transform(df[['text']]).toarray()"
  },
  {
    "url": "https://stackoverflow.com/questions/48091874/downloading-multiple-s3-objects-in-parallel-in-python",
    "body": "#!/usr/bin/env python3\nimport multiprocessing\nimport boto3\nimport sys\n# make a per process s3_client\ns3_client = None\ndef initialize():\n  global s3_client\n  s3_client = boto3.client('s3')\n# the work function of each process which will fetch something from s3\ndef download(job):\n  bucket, key, filename = job\n  s3_client.download_file(bucket, key, filename)\nif __name__ == '__main__':\n  # make the jobs, arguments to program are: bucket s3_key_0 s3_key_1 ... s3_key_n\n  bucket = sys.argv[1]\n  jobs = [(bucket, key, key.replace('/', '_')) for key in sys.argv[2:] ]\n  # make a process pool to do the work\n  pool = multiprocessing.Pool(multiprocessing.cpu_count(), initialize)\n  pool.map(download, jobs)\n  pool.close()\n  pool.join()"
  },
  {
    "url": "https://stackoverflow.com/questions/18832763/drawing-directions-fields",
    "body": "import matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\nimport numpy as np\nfig = plt.figure()\ndef vf(x, t):\n    dx = np.zeros(2)\n    dx[0] = 1.0\n    dx[1] = x[0] ** 2 - x[0] - 2.0\n    return dx\n# Solution curves\nt0 = 0.0\ntEnd = 10.0\n# Vector field\nX, Y = np.meshgrid(np.linspace(-5, 5, 20), np.linspace(-10, 10, 20))\nU = 1.0\nV = X ** 2 - X - 2\n# Normalize arrows\nN = np.sqrt(U ** 2 + V ** 2)\nU = U / N\nV = V / N\nplt.quiver(X, Y, U, V, angles=\"xy\")\nt = np.linspace(t0, tEnd, 100)\nfor y0 in np.linspace(-5.0, 0.0, 10):\n    y_initial = [y0, -10.0]\n    y = odeint(vf, y_initial, t)\n    plt.plot(y[:, 0], y[:, 1], \"-\")\nplt.xlim([-5, 5])\nplt.ylim([-10, 10])\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$y$\")"
  },
  {
    "url": "https://stackoverflow.com/questions/51817148/dynamically-add-new-wtforms-fieldlist-entries-from-user-interface",
    "body": "<form method=\"POST\" action=\"\">\n            {{ form.hidden_tag() }}\n            <fieldset class=\"form-group\">\n                <legend class=\"border-bottom mb-4\">XYZ</legend>\n                <div>\n                    {{ form.standard.label(class=\"form-control-label\") }}\n                    {{ form.standard(class=\"form-control form-control-lg\") }}\n                </div>\n                <div>\n                    {{ form.wps.label(class=\"form-control-label\") }}\n                    {{ form.wps(class=\"form-control form-control-lg\") }}\n                </div>\n            ...\n            </fieldset>\n            <div class=\"form-group\">\n                {{ form.submit(class='btn btn-outline-success') }}\n            </div>\n        </form>"
  },
  {
    "url": "https://stackoverflow.com/questions/51817148/dynamically-add-new-wtforms-fieldlist-entries-from-user-interface",
    "body": "<script>\n $(document).ready(function(){\n            $(\"#standard\").change(function(){\n                var std = $(this).find('option:selected').val(); //capture value from form.\n                $.getJSON(\"{{url_for('modifywps')}}\",{'std':std}, function(result){ //use captured value to sent to view via a dictionary {'key':val}, via url_for('view_name')\n\n                    $.each(result, function(i, field){ //value you want will be back here and gone through..\n                        $.each(field, function(j,k){\n                            $(\"#wps\").append('<option value=\"'+j+'\">'+k+'</option>'); // use jQuery to insert it back into the form, via the form name you gave in jinja templating\n                        });\n                    });\n                });\n            });\n            $(\"#wps\").change(function(){\n\n                    var std = $('#wps').find('option:selected').val(); // capture value from form.\n                    $.getJSON(\"{{url_for('modifyprocess')}}\",{'std':std}, function(result)\n\n                        $.each(result, function(i, field){\n                            $.each(field, function(j,k){\n                                $(\"#processes\").append('<option value=\"'+j+'\">'+k+'</option>');\n                            });\n                        });\n                    });\n            });\n</script>"
  },
  {
    "url": "https://stackoverflow.com/questions/62274412/cv2-approxpolydp-cv2-arclength-how-these-works",
    "body": "import cv2\nimport imutils\n# edged is the edge detected image\ncnts = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\ncnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n# loop over the contours\nfor c in cnts:\n\t# approximate the contour\n\tperi = cv2.arcLength(c, True)\n\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)\n\t# if our approximated contour has four points, then we\n\t# can assume that we have found our screen\n\tif len(approx) == 4:\n\t\tscreenCnt = approx\n\t\tbreak"
  },
  {
    "url": "https://stackoverflow.com/questions/60248319/how-to-set-column-width-to-bestfit-in-openpyxl",
    "body": "# Imorting the necessary modules\ntry:\n        from openpyxl.cell import get_column_letter\nexcept ImportError:\n        from openpyxl.utils import get_column_letter\n        from openpyxl.utils import column_index_from_string\nfrom openpyxl import load_workbook\nimport openpyxl\nfrom openpyxl import Workbook\nfor column_cells in sheet.columns:\n    new_column_length = max(len(str(cell.value)) for cell in column_cells)\n    new_column_letter = (get_column_letter(column_cells[0].column))\n    if new_column_length > 0:\n        sheet.column_dimensions[new_column_letter].width = new_column_length*1.23"
  },
  {
    "url": "https://stackoverflow.com/questions/76686267/what-is-the-new-way-to-declare-mongo-objectid-with-pydantic-v2-0",
    "body": "from typing import Annotated, Any\nfrom bson import ObjectId\nfrom pydantic_core import core_schema\nfrom pydantic import BaseModel\nfrom pydantic.json_schema import JsonSchemaValue\nclass ObjectIdPydanticAnnotation:\n    @classmethod\n    def validate_object_id(cls, v: Any, handler) -> ObjectId:\n        if isinstance(v, ObjectId):\n            return v\n        s = handler(v)\n        if ObjectId.is_valid(s):\n            return ObjectId(s)\n        else:\n            raise ValueError(\"Invalid ObjectId\")\n    @classmethod\n    def __get_pydantic_core_schema__(cls, source_type, _handler) -> core_schema.CoreSchema:\n        assert source_type is ObjectId\n        return core_schema.no_info_wrap_validator_function(\n            cls.validate_object_id,\n            core_schema.str_schema(),\n            serialization=core_schema.to_string_ser_schema(),\n        )\n    @classmethod\n    def __get_pydantic_json_schema__(cls, _core_schema, handler) -> JsonSchemaValue:\n        return handler(core_schema.str_schema())\nclass Model(BaseModel):\n    id: Annotated[ObjectId, ObjectIdPydanticAnnotation]\nprint(Model(id='64b7abdecf2160b649ab6085'))\nprint(Model(id='64b7abdecf2160b649ab6085').model_dump_json())\nprint(Model(id=ObjectId()))\nprint(Model.model_json_schema())\nprint(Model(id='foobar'))  # will error"
  },
  {
    "url": "https://stackoverflow.com/questions/52627739/how-to-merge-numerical-and-embedding-sequential-models-to-treat-categories-in-rn",
    "body": "__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to\n==================================================================================================\n cat1_input (InputLayer)     [(None, 100)]                0         []\n\n cat2_input (InputLayer)     [(None, 100)]                0         []\n\n cat3_input (InputLayer)     [(None, 100)]                0         []\n\n embedding_14 (Embedding)    (None, 100, 50)              50000     ['cat1_input[0][0]']\n\n embedding_15 (Embedding)    (None, 100, 10)              5000      ['cat2_input[0][0]']\n\n embedding_16 (Embedding)    (None, 100, 100)             10000     ['cat3_input[0][0]']\n\n numeric_input (InputLayer)  [(None, 100, 10)]            0         []\n\n concatenate_26 (Concatenat  (None, 100, 160)             0         ['embedding_14[0][0]',\n e)                                                                  'embedding_15[0][0]',\n                                                                     'embedding_16[0][0]']\n\n concatenate_27 (Concatenat  (None, 100, 170)             0         ['numeric_input[0][0]',\n e)                                                                  'concatenate_26[0][0]']\n\n lstm_5 (LSTM)               (None, 64)                   60160     ['concatenate_27[0][0]']\n\n==================================================================================================\nTotal params: 125160 (488.91 KB)\nTrainable params: 125160 (488.91 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________"
  },
  {
    "url": "https://stackoverflow.com/questions/56852581/how-to-drop-duplicates-but-keep-the-rows-if-a-particular-other-column-is-not-nul",
    "body": "import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar'],\n                   'bank': [np.nan, 'abc', 'xyz']})\nuniq_indx = (df.sort_values(by=\"bank\", na_position='last').dropna(subset=['firstname', 'lastname', 'email'])\n             .applymap(lambda s: s.lower() if type(s) == str else s)\n             .applymap(lambda x: x.replace(\" \", \"\") if type(x) == str else x)\n             .drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n# save unique records\ndfiban_uniq = df.loc[uniq_indx]\nprint(dfiban_uniq)"
  },
  {
    "url": "https://stackoverflow.com/questions/31514136/how-to-add-punctuation-to-text-using-python",
    "body": "from rpunct import RestorePuncts\n# The default language is 'english'\nrpunct = RestorePuncts(use_cuda=False)\nrpunct.punctuate(\"\"\"in 2018 cornell researchers built a high-powered detector that in combination with an algorithm-driven process called ptychography set a world record\nby tripling the resolution of a state-of-the-art electron microscope as successful as it was that approach had a weakness it only worked with ultrathin samples that were\na few atoms thick anything thicker would cause the electrons to scatter in ways that could not be disentangled now a team again led by david muller the samuel b eckert\nprofessor of engineering has bested its own record by a factor of two with an electron microscope pixel array detector empad that incorporates even more sophisticated\n3d reconstruction algorithms the resolution is so fine-tuned the only blurring that remains is the thermal jiggling of the atoms themselves\"\"\")"
  },
  {
    "url": "https://stackoverflow.com/questions/31514136/how-to-add-punctuation-to-text-using-python",
    "body": ">>> from deepmultilingualpunctuation import PunctuationModel\n>>> model = PunctuationModel()\nDownloading config.json: 100%|█████████████████████████████████████████████████████████████████| 892/892 [00:00<00:00, 335kB/s]\nDownloading pytorch_model.bin: 100%|██████████████████████████████████████████████████████| 2.08G/2.08G [04:54<00:00, 7.60MB/s]\nDownloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████| 406/406 [00:00<00:00, 216kB/s]\nDownloading sentencepiece.bpe.model: 100%|████████████████████████████████████████████████| 4.83M/4.83M [00:00<00:00, 8.08MB/s]\nDownloading special_tokens_map.json: 100%|█████████████████████████████████████████████████████| 239/239 [00:00<00:00, 158kB/s]\n/opt/anaconda3/envs/punct/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n  warnings.warn(\n>>> text = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller\"\n>>> result = model.restore_punctuation(text)\n>>> print(result)\nMy name is Clara and I live in Berkeley, California. Ist das eine Frage, Frau Müller?"
  },
  {
    "url": "https://stackoverflow.com/questions/55677165/python-flask-as-windows-service",
    "body": "# -*- mode: python -*-\nblock_cipher = None\na = Analysis(['win32_service.py'],\n             pathex=['C:\\\\Users\\\\Win7\\\\Desktop\\\\FaaS'],\n             binaries=[],\n             datas=[],\n             hiddenimports=['win32timezone',\n                            'altgraph',\n                            'Click'\n                            'Flask',\n                            'future',\n                            'itsdangerous',\n                            'Jinja2',\n                            'macholib',\n                            'MarkupSafe',\n                            'pefile',\n                            'PyInstaller',\n                            'pyodbc',\n                            'pywin32',\n                            'pywin32-ctypes',\n                            'Werkzeug',],\n             hookspath=[],\n             runtime_hooks=[],\n             excludes=[],\n             win_no_prefer_redirects=False,\n             win_private_assemblies=False,\n             cipher=block_cipher,\n             noarchive=False)\npyz = PYZ(a.pure, a.zipped_data,\n             cipher=block_cipher)\nexe = EXE(pyz,\n          a.scripts,\n          a.binaries,\n          a.zipfiles,\n          a.datas,\n          [],\n          name='win32_service',\n          debug=False,\n          bootloader_ignore_signals=False,\n          strip=False,\n          upx=True,\n          runtime_tmpdir=None,\n          console=True )"
  },
  {
    "url": "https://stackoverflow.com/questions/39817641/how-to-send-a-json-object-using-tcp-socket-in-python",
    "body": "import socket\nimport sys\nimport json\nHOST, PORT = \"localhost\", 9999\n#m ='{\"id\": 2, \"name\": \"abc\"}'\nm = {\"id\": 2, \"name\": \"abc\"} # a real dict.\ndata = json.dumps(m)\n# Create a socket (SOCK_STREAM means a TCP socket)\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ntry:\n    # Connect to server and send data\n    sock.connect((HOST, PORT))\n    sock.sendall(bytes(data,encoding=\"utf-8\"))\n    # Receive data from the server and shut down\n    received = sock.recv(1024)\n    received = received.decode(\"utf-8\")\n\nfinally:\n    sock.close()\nprint \"Sent:     {}\".format(data)\nprint \"Received: {}\".format(received)"
  },
  {
    "url": "https://stackoverflow.com/questions/39309046/what-is-the-proper-way-of-testing-throttling-in-drf",
    "body": "from unittest.mock import patch\nfrom django.core.cache import cache\nfrom rest_framework import status\nclass Tests(SimpleTestCase):\n    def setUp(self):\n        cache.clear()\n    @patch('path.to.AuthRateThrottle.get_rate')\n    def test_throttling(self, mock):\n        mock.return_value = '1/day'\n        response = self.client.post(self.url, {})\n        self.assertEqual(\n            response.status_code,\n            status.HTTP_400_BAD_REQUEST,  # some fields are required\n        )\n        response = self.client.post(self.url, {})\n        self.assertEqual(\n            response.status_code,\n            status.HTTP_429_TOO_MANY_REQUESTS,\n        )"
  },
  {
    "url": "https://stackoverflow.com/questions/69115825/remove-white-borders-from-segmented-images",
    "body": "import cv2\nimport numpy as np\nimage = cv2.imread('1.png')\nhighlight = image.copy()\noriginal = image.copy()\n# Convert image to grayscale, Otsu's threshold, and find contours\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\ncontours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncontours = contours[0] if len(contours) == 2 else contours[1]\n# Create black mask to extract desired objects\nmask = np.zeros(image.shape, dtype=np.uint8)\n# Search for objects by filtering using contour area and aspect ratio\nfor c in contours:\n    # Contour area\n    area = cv2.contourArea(c)\n    # Contour perimeter\n    peri = cv2.arcLength(c, True)\n    # Contour approximation\n    approx = cv2.approxPolyDP(c, 0.035 * peri, True)\n    (x, y, w, h) = cv2.boundingRect(approx)\n    aspect_ratio = w / float(h)\n    # Draw filled contour onto mask if passes filter\n    # These are arbitary values, may need to change depending on input image\n    if aspect_ratio <= 1.2 or area < 5000:\n        cv2.drawContours(highlight, [c], 0, (0,255,0), -1)\n        cv2.drawContours(mask, [c], 0, (255,255,255), -1)\n# Convert 3-channel mask to grayscale then bitwise-and with original image for result\nmask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\nresult = cv2.bitwise_and(original, original, mask=mask)\n# Uncomment if you want background to be white instead of black\n# result[mask==0] = (255,255,255)\n# Display\ncv2.imshow('gray', gray)\ncv2.imshow('thresh', thresh)\ncv2.imshow('highlight', highlight)\ncv2.imshow('mask', mask)\ncv2.imshow('result', result)\n# Save images\n# cv2.imwrite('gray.png', gray)\n# cv2.imwrite('thresh.png', thresh)\n# cv2.imwrite('highlight.png', highlight)\n# cv2.imwrite('mask.png', mask)\n# cv2.imwrite('result.png', result)\ncv2.waitKey(0)"
  },
  {
    "url": "https://stackoverflow.com/questions/54413434/type-hinting-with-descriptors",
    "body": "from typing import Callable, Generic, Type, TypeVar, overload, Union\nInstance = TypeVar('Instance')\nValue = TypeVar('Value')\nAttribute = TypeVar('Attribute')\nclass Descriptor(Generic[Instance, Attribute, Value]):\n    def __init__(self, method: Callable[[Instance, Attribute], Value]):\n        \"\"\" Called on initialisation of descriptor \"\"\"\n    @overload\n    def __get__(self, instance: None, owner: Type[Instance]) -> 'Descriptor':\n        \"\"\" Called when an attribute is accessed via class not an instance \"\"\"\n    @overload\n    def __get__(self, instance: Instance, owner: Type[Instance]) -> Value:\n        \"\"\" Called when an attribute is accessed on an instance variable \"\"\"\n    def __get__(self, instance: Union[Instance, None], owner: Type[Instance]) -> Union[Value, 'Descriptor']:\n        \"\"\" Full implementation is declared here \"\"\"\n        ...\n    def __set__(self, instance: Instance, value: Value):\n        \"\"\" Called when setting a value.\"\"\""
  },
  {
    "url": "https://stackoverflow.com/questions/48079364/wrapping-text-not-working-in-matplotlib",
    "body": "import matplotlib.pyplot as plt\nimport matplotlib.text as mtext\nclass WrapText(mtext.Text):\n    def __init__(self,\n                 x=0, y=0, text='',\n                 width=0,\n                 **kwargs):\n        mtext.Text.__init__(self,\n                 x=x, y=y, text=text,\n                 wrap=True,\n                 **kwargs)\n        self.width = width  # in screen pixels. You could do scaling first\n    def _get_wrap_line_width(self):\n        return self.width\nfig = plt.figure(1, clear=True)\nax = fig.add_subplot(111)\ntext = ('Lorem ipsum dolor sit amet, consectetur adipiscing elit, '\n        'sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. ')\n# Create artist object. Note clip_on is True by default\n# The axes doesn't have this method, so the object is created separately\n# and added afterwards.\nwtxt = WrapText(.8, .4, text, width=500, va='top', clip_on=False,\n                bbox=dict(boxstyle='square', fc='w', ec='b'))\n# Add artist to the axes\nax.add_artist(wtxt)\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/18515275/can-cython-code-be-compiled-to-a-dll-so-c-application-can-call-it",
    "body": "//cyfun_dll.c\n#define BUILDING_DLL\n#include \"cyfun_dll.h\"\n#define PY_SSIZE_T_CLEAN\n#include <Python.h>\n#include \"cyfun.h\"\nDLL_PUBLIC int cyfun_init(){\n  int status=PyImport_AppendInittab(\"cyfun\", PyInit_cyfun);\n  if(status==-1){\n    return -1;//error\n  }\n  Py_Initialize();\n  PyObject *module = PyImport_ImportModule(\"cyfun\");\n  if(module==NULL){\n     Py_Finalize();\n     return -1;//error\n  }\n  return 0;\n}\nDLL_PUBLIC void cyfun_finalize(){\n   Py_Finalize();\n}\nDLL_PUBLIC int cyfun_double_me(int me){\n    return double_me(me);\n}\nDLL_PUBLIC void cyfun_print_me(int me){\n    print_me(me);\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/18515275/can-cython-code-be-compiled-to-a-dll-so-c-application-can-call-it",
    "body": "BOOL WINAPI DllMain(\n    HINSTANCE hinstDLL,  // handle to DLL module\n    DWORD fdwReason,     // reason for calling function\n    LPVOID lpReserved )  // reserved\n{\n    // Perform actions based on the reason for calling.\n    switch( fdwReason )\n    {\n        case DLL_PROCESS_ATTACH:\n            return cyfun_init()==0;\n        case DLL_PROCESS_DETACH:\n            cyfun_finalize();\n            break;\n        case DLL_THREAD_ATTACH:\n         // Do thread-specific initialization.\n            break;\n        case DLL_THREAD_DETACH:\n         // Do thread-specific cleanup.\n            break;\n    }\n    return TRUE;\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/75363733/sqlalchemy-2-0-orm-model-datetime-insertion",
    "body": "import datetime\nfrom sqlalchemy import Integer, String, DateTime\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.sql import func\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import Session\nclass Base(DeclarativeBase):\n    pass\nclass MyTable(Base):\n    __tablename__ = \"my_table\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    name: Mapped[str] = mapped_column(String)\n    created_date: Mapped[datetime.datetime] = mapped_column(\n        DateTime(timezone=True), server_default=func.now()\n    )\ndef initialize_engine(filename):\n    return create_engine(f\"sqlite+pysqlite:///{filename}\", echo=True)\ndef initialize_tables(engine):\n    Base.metadata.create_all(engine)\ndef add_row(engine, name):\n    this_row = MyTable(name=name)\n    print(this_row)\n    with Session(engine) as session:\n        session.add(this_row)\n        session.commit()\nmy_file = \"test.db\"\nmy_engine = initialize_engine(my_file)\ninitialize_tables(my_engine)\nadd_row(my_engine, \"Dave\")"
  },
  {
    "url": "https://stackoverflow.com/questions/75363733/sqlalchemy-2-0-orm-model-datetime-insertion",
    "body": "python datetest.py\n2023-02-06 11:02:41,157 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-06 11:02:41,158 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"my_table\")\n2023-02-06 11:02:41,158 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-02-06 11:02:41,158 INFO sqlalchemy.engine.Engine COMMIT\n<__main__.MyTable object at 0x000002CC767ECD50>\n2023-02-06 11:02:41,159 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-06 11:02:41,160 INFO sqlalchemy.engine.Engine INSERT INTO my_table (name) VALUES (?) RETURNING id, created_date\n2023-02-06 11:02:41,160 INFO sqlalchemy.engine.Engine [generated in 0.00020s] ('Dave',)\n2023-02-06 11:02:41,171 INFO sqlalchemy.engine.Engine COMMIT"
  },
  {
    "url": "https://stackoverflow.com/questions/58966874/adding-attention-on-top-of-simple-lstm-layer-in-tensorflow-2-0",
    "body": "model = keras.models.Sequential()\n        model.add(keras.layers.LSTM(cfg.LSTM, input_shape=(cfg.TIMESTEPS,\n                  cfg.FEATURES),\n                  return_sequences=True))\n        model.add(SeqSelfAttention(attention_width=cfg.ATTNWIDTH,\n                attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n                attention_activation='softmax',\n                name='Attention'))\n        model.add(keras.layers.Dense(cfg.DENSE))\n        model.add(keras.layers.Dense(cfg.OUTPUT, activation='sigmoid'))"
  },
  {
    "url": "https://stackoverflow.com/questions/45275141/tensorflow-is-there-a-way-to-convert-a-frozen-graph-into-a-checkpoint-model",
    "body": "import numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.graph_editor as ge\nconst_var_name_pairs = []\nwith tf_graph.as_default() as g:\n    for name in to_convert:\n        tensor = g.get_tensor_by_name('{}:0'.format(name))\n        with tf.Session() as sess:\n            tensor_as_numpy_array = sess.run(tensor)\n        var_shape = tensor.get_shape()\n        # Give each variable a name that doesn't already exist in the graph\n        var_name = '{}_turned_var'.format(name)\n        # Create TensorFlow variable initialized by values of original const.\n        var = tf.get_variable(name=var_name, dtype='float32', shape=var_shape, \\\n                      initializer=tf.constant_initializer(tensor_as_numpy_array))\n        # We want to keep track of our variables names for later.\n        const_var_name_pairs.append((name, var_name))\n    # At this point, we added a bunch of tf.Variables to the graph, but they're\n    # not connected to anything.\n    # The magic: we use TF Graph Editor to swap the Constant nodes' outputs with\n    # the outputs of our newly created Variables.\n    for const_name, var_name in const_var_name_pairs:\n        const_op = g.get_operation_by_name(const_name)\n        var_reader_op = g.get_operation_by_name(var_name + '/read')\n        ge.swap_outputs(ge.sgv(const_op), ge.sgv(var_reader_op))"
  },
  {
    "url": "https://stackoverflow.com/questions/6850798/why-doesnt-filter-attached-to-the-root-logger-propagate-to-descendant-loggers",
    "body": "import logging\nclass MyFilter(logging.Filter):\n    def filter(self, record):\n        record.msg = 'MY FILTER: ' + record.msg\n        return 1\nmyfilter = MyFilter()\nmyformatter = logging.Formatter(\"MY HANDLER: %(name)s - %(message)s\")\nmyhandler = logging.StreamHandler()\nmyhandler.setFormatter(myformatter)\nmyhandler.addFilter(myfilter)\nfoo_logger = logging.getLogger('foo')\nfoo_logger.addHandler(myhandler)\nfoo_bar_logger = logging.getLogger('foo.bar')\nfoo_logger.error('asdfasdf')\nfoo_bar_logger.error('zxcvzxcv')"
  },
  {
    "url": "https://stackoverflow.com/questions/63918342/refresh-token-using-fastapi-and-swagger",
    "body": "# source: /fastapi/security/oauth2.py\n#\nclass OAuth2AuthorizationCodeBearer(OAuth2):\n    def __init__(\n        self,\n        authorizationUrl: str,\n        tokenUrl: str,\n        refreshUrl: Optional[str] = None,\n        scheme_name: Optional[str] = None,\n        scopes: Optional[Dict[str, str]] = None,\n        auto_error: bool = True,\n    ):\n        if not scopes:\n            scopes = {}\n        flows = OAuthFlowsModel(\n            authorizationCode={\n                \"authorizationUrl\": authorizationUrl,\n                \"tokenUrl\": tokenUrl,\n                \"refreshUrl\": refreshUrl,\n                \"scopes\": scopes,\n            }\n        )\n        super().__init__(flows=flows, scheme_name=scheme_name, auto_error=auto_error)\n    async def __call__(self, request: Request) -> Optional[str]:\n        authorization: str = request.headers.get(\"Authorization\")\n        scheme, param = get_authorization_scheme_param(authorization)\n        if not authorization or scheme.lower() != \"bearer\":\n            if self.auto_error:\n                raise HTTPException(\n                    status_code=HTTP_401_UNAUTHORIZED,\n                    detail=\"Not authenticated\",\n                    headers={\"WWW-Authenticate\": \"Bearer\"},\n                )\n            else:\n                return None  # pragma: nocover\n        return param"
  },
  {
    "url": "https://stackoverflow.com/questions/54511769/running-lstm-with-multiple-gpus-gets-input-and-hidden-tensors-are-not-at-the-sa",
    "body": "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nclass MyModule(nn.Module):\n    # ... __init__, other methods, etc.\n    # padded_input is of shape [B x T x *] (batch_first mode) and contains\n    # the sequences sorted by lengths\n    #   B is the batch size\n    #   T is max sequence length\n    def forward(self, padded_input, input_lengths):\n        total_length = padded_input.size(1)  # get the max sequence length\n        packed_input = pack_padded_sequence(padded_input, input_lengths,\n                                            batch_first=True)\n        packed_output, _ = self.my_lstm(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True,\n                                        total_length=total_length)\n        return output\nm = MyModule().cuda()\ndp_m = nn.DataParallel(m)"
  },
  {
    "url": "https://stackoverflow.com/questions/26937213/how-to-store-python-objects-in-cython-c-containers",
    "body": "cdef class PortValue:\n    cdef cadevs.PortValue[PythonObject, PythonObject]* _c_portvalue\n    # Add fields to keep stored Python objects alive.\n    cdef object port_ref_holder\n    cdef object value_ref_holder\n    def __cinit__(self, object port, object value):\n        self._c_portvalue = new cadevs.PortValue[PythonObject, PythonObject](\n            <PyObject *>port, <PyObject *>value\n        )\n        # Assign objects here to keep them alive.\n        port_ref_holder = port\n        value_ref_holder = value"
  },
  {
    "url": "https://stackoverflow.com/questions/26937213/how-to-store-python-objects-in-cython-c-containers",
    "body": "from cpython.ref cimport PyObject, Py_INCREF, Py_DECREF\ncdef extern from *:\n    \"\"\"\n    class PyRef {\n        PyObject* obj;\n    public:\n\n        PyObject* get() {return obj;}\n        PyRef() {obj = NULL;}\n        PyRef(PyObject* set_obj) {\n            Py_XINCREF(set_obj);\n            obj = set_obj;}\n\n        ~PyRef() {\n            Py_XDECREF(obj);obj = NULL;\n        }\n        PyRef(const PyRef& other)  {\n            Py_XINCREF(other.obj);\n            obj = other.obj;\n        }\n        PyRef(PyRef&& other) {obj = other.obj; other.obj = NULL;}\n        PyRef& operator=(const PyRef& other) {\n            Py_XDECREF(obj);\n            Py_XINCREF(other.obj);\n            obj = other.obj;\n            return *this;\n        }\n        PyRef& operator=(PyRef&& other) {\n            Py_XDECREF(obj);\n            obj = other.obj;\n            other.obj = NULL;\n            return *this;\n        }\n    };\n    \"\"\"\n    cdef cppclass PyRef:\n        PyRef() except +\n        PyRef(PyObject* set_obj) except +\n        PyObject* get() except +"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "from fastapi import FastAPI, Request, HTTPException\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom urllib.parse import unquote\nimport aiofiles\nimport os\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n@app.post('/upload')\nasync def upload(request: Request):\n    try:\n        filename = request.headers['filename']\n        filename = unquote(filename)\n        filepath = os.path.join('./', os.path.basename(filename))\n        async with aiofiles.open(filepath, 'wb') as f:\n            async for chunk in request.stream():\n                await f.write(chunk)\n    except Exception:\n        raise HTTPException(status_code=500, detail='Something went wrong')\n\n    return {\"message\": f\"Successfuly uploaded: {filename}\"}\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def main(request: Request):\n    return templates.TemplateResponse(request=request, name=\"index.html\")"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "<!DOCTYPE html>\n<html>\n   <body>\n      <label for=\"fileInput\">Choose file(s) to upload</label>\n      <input type=\"file\" id=\"fileInput\" name=\"fileInput\" onchange=\"reset()\" multiple><br>\n      <input type=\"button\" value=\"Submit\" onclick=\"go()\">\n      <p id=\"response\"></p>\n      <script>\n         var resp = document.getElementById(\"response\");\n\n         function reset() {\n            resp.innerHTML = \"\";\n         }\n\n         function go() {\n            var fileInput = document.getElementById('fileInput');\n            if (fileInput.files[0]) {\n               for (const file of fileInput.files) {\n                  let reader = new FileReader();\n                  reader.onload = function () {\n                     uploadFile(reader.result, file.name);\n                  }\n                  reader.readAsArrayBuffer(file);\n               }\n            }\n         }\n\n         function uploadFile(contents, filename) {\n            var headers = new Headers();\n\t\t\tfilename = encodeURI(filename);\n            headers.append(\"filename\", filename);\n            fetch('/upload', {\n                  method: 'POST',\n                  headers: headers,\n                  body: contents,\n               })\n               .then(response => response.json()) // or, response.text(), etc.\n               .then(data => {\n                  resp.innerHTML += JSON.stringify(data); // data is a JSON object\n               })\n               .catch(error => {\n                  console.error(error);\n               });\n         }\n      </script>\n   </body>\n</html>"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "from fastapi import FastAPI, Request, HTTPException, status\nfrom streaming_form_data import StreamingFormDataParser\nfrom streaming_form_data.targets import FileTarget, ValueTarget\nfrom streaming_form_data.validators import MaxSizeValidator\nimport streaming_form_data\nfrom starlette.requests import ClientDisconnect\nfrom urllib.parse import unquote\nimport os\nMAX_FILE_SIZE = 1024 * 1024 * 1024 * 4  # = 4GB\nMAX_REQUEST_BODY_SIZE = MAX_FILE_SIZE + 1024\napp = FastAPI()\nclass MaxBodySizeException(Exception):\n    def __init__(self, body_len: str):\n        self.body_len = body_len\nclass MaxBodySizeValidator:\n    def __init__(self, max_size: int):\n        self.body_len = 0\n        self.max_size = max_size\n    def __call__(self, chunk: bytes):\n        self.body_len += len(chunk)\n        if self.body_len > self.max_size:\n            raise MaxBodySizeException(body_len=self.body_len)\n\n@app.post('/upload')\nasync def upload(request: Request):\n    body_validator = MaxBodySizeValidator(MAX_REQUEST_BODY_SIZE)\n    filename = request.headers.get('filename')\n\n    if not filename:\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail='Filename header is missing')\n    try:\n        filename = unquote(filename)\n        filepath = os.path.join('./', os.path.basename(filename))\n        file_ = FileTarget(filepath, validator=MaxSizeValidator(MAX_FILE_SIZE))\n        data = ValueTarget()\n        parser = StreamingFormDataParser(headers=request.headers)\n        parser.register('file', file_)\n        parser.register('data', data)\n\n        async for chunk in request.stream():\n            body_validator(chunk)\n            parser.data_received(chunk)\n    except ClientDisconnect:\n        print(\"Client Disconnected\")\n    except MaxBodySizeException as e:\n        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,\n           detail=f'Maximum request body size limit ({MAX_REQUEST_BODY_SIZE} bytes) exceeded ({e.body_len} bytes read)')\n    except streaming_form_data.validators.ValidationError:\n        raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,\n            detail=f'Maximum file size limit ({MAX_FILE_SIZE} bytes) exceeded')\n    except Exception:\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail='There was an error uploading the file')\n\n    if not file_.multipart_filename:\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail='File is missing')\n    print(data.value.decode())\n    print(file_.multipart_filename)\n\n    return {\"message\": f\"Successfuly uploaded {filename}\"}"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "from fastapi import FastAPI, Request, HTTPException, status\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom starlette.requests import ClientDisconnect\nfrom urllib.parse import unquote\nimport streaming_form_data\nfrom streaming_form_data import StreamingFormDataParser\nfrom streaming_form_data.targets import FileTarget, ValueTarget\nimport os\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n@app.get(\"/\", response_class=HTMLResponse)\nasync def main(request: Request):\n    return templates.TemplateResponse(request=request, name=\"index.html\")\n@app.post('/upload')\nasync def upload(request: Request):\n    try:\n        parser = StreamingFormDataParser(headers=request.headers)\n        data = ValueTarget()\n        parser.register('data', data)\n        headers = dict(request.headers)\n        filenames = []\n        i = 0\n        while True:\n            filename =  headers.get(f'filename{i}', None)\n            if filename is None:\n                break\n            filename = unquote(filename)\n            filenames.append(filename)\n            filepath = os.path.join('./', os.path.basename(filename))\n            file_ = FileTarget(filepath)\n            parser.register(f'file{i}', file_)\n            i += 1\n        async for chunk in request.stream():\n            parser.data_received(chunk)\n    except ClientDisconnect:\n        print(\"Client Disconnected\")\n    except Exception:\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail='There was an error uploading the file')\n    print(data.value.decode())\n    return {\"message\": f\"Successfuly uploaded {filenames}\"}"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "<!DOCTYPE html>\n<html>\n   <body>\n      <input type=\"file\" id=\"fileInput\" name=\"files\" onchange=\"reset()\" multiple><br>\n      <input type=\"button\" value=\"Submit\" onclick=\"submitUsingFetch()\">\n      <p id=\"response\"></p>\n      <script>\n         var resp = document.getElementById(\"response\");\n\n         function reset() {\n            resp.innerHTML = \"\";\n         }\n\n         function submitUsingFetch() {\n            var fileInput = document.getElementById('fileInput');\n            if (fileInput.files[0]) {\n               var formData = new FormData();\n               var headers = new Headers();\n               formData.append(\"data\", \"Hello World!\");\n\n               var i = 0;\n               for (const file of fileInput.files) {\n                  filename = encodeURI(file.name);\n                  headers.append(`filename${i}`, filename);\n                  formData.append(`file${i}`, file, filename);\n                  i++;\n               }\n\n               fetch('/upload', {\n                     method: 'POST',\n                     headers: headers,\n                     body: formData,\n                  })\n                  .then(response => response.json())  // or, response.text(), etc.\n                  .then(data => {\n                     resp.innerHTML = JSON.stringify(data);  // data is a JSON object\n                  })\n                  .catch(error => {\n                     console.error(error);\n                  });\n            }\n         }\n      </script>\n   </body>\n</html>"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "import httpx\nimport time\nfrom urllib.parse import quote\nurl ='http://127.0.0.1:8000/upload'\nfilename0 = 'bigFile.zip'\nfilename1 = 'otherBigFile.zip'\nheaders = {'filename0': quote(filename0), 'filename1': quote(filename1)}\nfiles = [('file0', open(filename0, 'rb')), ('file1', open(filename1, 'rb'))]\ndata = {'data': 'Hello World!'}\nwith httpx.Client() as client:\n    start = time.time()\n    r = client.post(url, data=data, files=files, headers=headers)\n    end = time.time()\n    print(f'Time elapsed: {end - start}s')\n    print(r.status_code, r.json(), sep=' ')"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "#...\nfrom fastapi import Form\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Optional\nfrom fastapi.encoders import jsonable_encoder\n#...\nclass Base(BaseModel):\n    name: str\n    point: Optional[float] = None\n    is_accepted: Optional[bool] = False\n\ndef checker(data: str = Form(...)):\n    try:\n        return Base.model_validate_json(data)\n    except ValidationError as e:\n        raise HTTPException(detail=jsonable_encoder(e.errors()), status_code=status.HTTP_422_UNPROCESSABLE_ENTITY)\n\n@app.post('/upload')\nasync def upload(request: Request):\n\t#...\n\n    # place the below after the try-except block in the example given earlier\n\tmodel = checker(data.value.decode())\n\tprint(dict(model))"
  },
  {
    "url": "https://stackoverflow.com/questions/73442335/how-to-upload-a-large-file-%e2%89%a53gb-to-fastapi-backend",
    "body": "from fastapi import FastAPI, File, UploadFile, Form, HTTPException, status\nimport aiofiles\nimport os\nCHUNK_SIZE = 1024 * 1024  # adjust the chunk size as desired\napp = FastAPI()\n@app.post(\"/upload\")\nasync def upload(file: UploadFile = File(...), data: str = Form(...)):\n    try:\n        filepath = os.path.join('./', os.path.basename(file.filename))\n        async with aiofiles.open(filepath, 'wb') as f:\n            while chunk := await file.read(CHUNK_SIZE):\n                await f.write(chunk)\n    except Exception:\n        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail='There was an error uploading the file')\n    finally:\n        await file.close()\n    return {\"message\": f\"Successfuly uploaded {file.filename}\"}"
  },
  {
    "url": "https://stackoverflow.com/questions/56781635/how-to-find-extreme-outer-points-in-an-image-with-python-opencv",
    "body": "import cv2\nimport numpy as np\n# Load image, grayscale, Gaussian blur, threshold\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(gray, (3,3), 0)\nthresh = cv2.threshold(blur, 220, 255, cv2.THRESH_BINARY_INV)[1]\n# Find contours\ncnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nc = max(cnts, key=cv2.contourArea)\n# Obtain outer coordinates\nleft = tuple(c[c[:, :, 0].argmin()][0])\nright = tuple(c[c[:, :, 0].argmax()][0])\ntop = tuple(c[c[:, :, 1].argmin()][0])\nbottom = tuple(c[c[:, :, 1].argmax()][0])\n# Draw dots onto image\ncv2.drawContours(image, [c], -1, (36, 255, 12), 2)\ncv2.circle(image, left, 8, (0, 50, 255), -1)\ncv2.circle(image, right, 8, (0, 255, 255), -1)\ncv2.circle(image, top, 8, (255, 50, 0), -1)\ncv2.circle(image, bottom, 8, (255, 255, 0), -1)\nprint('left: {}'.format(left))\nprint('right: {}'.format(right))\nprint('top: {}'.format(top))\nprint('bottom: {}'.format(bottom))\ncv2.imshow('thresh', thresh)\ncv2.imshow('image', image)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/71862398/install-python-3-6-on-mac-m1",
    "body": "#Install Rosetta\n/usr/sbin/softwareupdate --install-rosetta --agree-to-license\n# Install x86_64 brew\narch -x86_64 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n# Set up x86_64 homebrew and pyenv and temporarily set aliases\nalias brew86=\"arch -x86_64 /usr/local/bin/brew\"\nalias pyenv86=\"arch -x86_64 pyenv\"\n# Install required packages and flags for building this particular python version through emulation\nbrew86 install pyenv gcc libffi gettext\nexport CPPFLAGS=\"-I$(brew86 --prefix libffi)/include -I$(brew86 --prefix openssl)/include -I$(brew86 --prefix readline)/lib\"\nexport CFLAGS=\"-I$(brew86 --prefix openssl)/include -I$(brew86 --prefix bzip2)/include -I$(brew86 --prefix readline)/include -I$(xcrun --show-sdk-path)/usr/include -Wno-implicit-function-declaration\"\nexport LDFLAGS=\"-L$(brew86 --prefix openssl)/lib -L$(brew86 --prefix readline)/lib -L$(brew86 --prefix zlib)/lib -L$(brew86 --prefix bzip2)/lib -L$(brew86 --prefix gettext)/lib -L$(brew86 --prefix libffi)/lib\"\n# Providing an incorrect openssl version forces a proper openssl version to be downloaded and linked during the build\nexport PYTHON_BUILD_HOMEBREW_OPENSSL_FORMULA=openssl@1.0\n# Install Python 3.6\npyenv86 install --patch 3.6.15 <<(curl -sSL https://raw.githubusercontent.com/pyenv/pyenv/master/plugins/python-build/share/python-build/patches/3.6.15/Python-3.6.15/0008-bpo-45405-Prevent-internal-configure-error-when-runn.patch\\?full_index\\=1)"
  },
  {
    "url": "https://stackoverflow.com/questions/69660200/how-to-render-svg-image-to-png-file-in-python",
    "body": "import cairosvg\n# read svg file -> write png file\ncairosvg.svg2png(url=input_svg_path, write_to=output_png_path, output_width=width, output_height=height)\n# read svg file -> png data\npng_data = cairosvg.svg2png(url=input_svg_path, output_width=width, output_height=height)\n# svg string -> write png file\ncairosvg.svg2png(bytestring=svg_str.encode(), write_to=output_png_path, output_width=width, output_height=height)\n# svg string -> png data\npng_data = cairosvg.svg2png(bytestring=svg_str.encode(), output_width=width, output_height=height)"
  },
  {
    "url": "https://stackoverflow.com/questions/69660200/how-to-render-svg-image-to-png-file-in-python",
    "body": "import subprocess\ninkscape = ... # path to inkscape executable\n# read svg file -> write png file\nsubprocess.run([inkscape, '--export-type=png', f'--export-filename={output_png_path}', f'--export-width={width}', f'--export-height={height}', input_svg_path])\n# read svg file -> png data\nresult = subprocess.run([inkscape, '--export-type=png', '--export-filename=-', f'--export-width={width}', f'--export-height={height}', input_svg_path], capture_output=True)\n#   (result.stdout will have the png data)\n# svg string -> write png file\nsubprocess.run([inkscape, '--export-type=png', f'--export-filename={output_png_path}', f'--export-width={width}', f'--export-height={height}', '--pipe'], input=svg_str.encode())\n# svg string -> png data\nresult = subprocess.run([inkscape, '--export-type=png', '--export-filename=-', f'--export-width={width}', f'--export-height={height}', '--pipe'], input=svg_str.encode(), capture_output=True)\n#   (result.stdout will have the png data)"
  },
  {
    "url": "https://stackoverflow.com/questions/69660200/how-to-render-svg-image-to-png-file-in-python",
    "body": "from wand.image import Image\nfrom wand.Color import Color\nwith Color('#00000000') as bgcolor,\\\n  # to read input from a file:\n  Image(filename=input_svg_path, width=width, height=height, background=bgcolor) as img:\n  # or, to use input from a string:\n  Image(blob=svg_str.encode(), format='svg', width=width, height=height, background=bgcolor) as img:\n    # to save output to a file:\n    with img.convert('png') as output_img:\n        output_img.save(filename=output_png_path)\n    # or, to get the output data in a variable:\n    png_data = img.make_blob(format='png')"
  },
  {
    "url": "https://stackoverflow.com/questions/4126348/how-can-this-function-be-rewritten-to-implement-ordereddict",
    "body": "import collections\nclass OrderedDefaultdict(collections.OrderedDict):\n    \"\"\" A defaultdict with OrderedDict as its base class. \"\"\"\n    def __init__(self, default_factory=None, *args, **kwargs):\n        if not (default_factory is None or callable(default_factory)):\n            raise TypeError('first argument must be callable or None')\n        super(OrderedDefaultdict, self).__init__(*args, **kwargs)\n        self.default_factory = default_factory  # called by __missing__()\n    def __missing__(self, key):\n        if self.default_factory is None:\n            raise KeyError(key,)\n        self[key] = value = self.default_factory()\n        return value\n    def __reduce__(self):  # Optional, for pickle support.\n        args = (self.default_factory,) if self.default_factory else tuple()\n        return self.__class__, args, None, None, iter(self.items())\n    def __repr__(self):  # Optional.\n        return '%s(%r, %r)' % (self.__class__.__name__, self.default_factory, self.items())\ndef simplexml_load_file(file):\n    from lxml import etree\n    tree = etree.parse(file)\n    root = tree.getroot()\n    def xml_to_item(el):\n        item = el.text or None\n        child_dicts = OrderedDefaultdict(list)\n        for child in el.getchildren():\n            child_dicts[child.tag].append(xml_to_item(child))\n        return collections.OrderedDict(child_dicts) or item\n    def xml_to_dict(el):\n        return {el.tag: xml_to_item(el)}\n    return xml_to_dict(root)\nx = simplexml_load_file('routines/test.xml')\nprint(x)\nfor y in x['root']:\n    print(y)"
  },
  {
    "url": "https://stackoverflow.com/questions/11921188/how-to-send-email-with-pdf-attachment-in-python",
    "body": "def send_email_pdf_figs(path_to_pdf, subject, message, destination, password_path=None):\n    ## credits: http://linuxcursor.com/python-programming/06-how-to-send-pdf-ppt-attachment-with-html-body-in-python-script\n    from socket import gethostname\n    #import email\n    from email.mime.application import MIMEApplication\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    import smtplib\n    import json\n    server = smtplib.SMTP('smtp.gmail.com', 587)\n    server.starttls()\n    with open(password_path) as f:\n        config = json.load(f)\n        server.login('me@gmail.com', config['password'])\n        # Craft message (obj)\n        msg = MIMEMultipart()\n        message = f'{message}\\nSend from Hostname: {gethostname()}'\n        msg['Subject'] = subject\n        msg['From'] = 'me@gmail.com'\n        msg['To'] = destination\n        # Insert the text to the msg going by e-mail\n        msg.attach(MIMEText(message, \"plain\"))\n        # Attach the pdf to the msg going by e-mail\n        with open(path_to_pdf, \"rb\") as f:\n            #attach = email.mime.application.MIMEApplication(f.read(),_subtype=\"pdf\")\n            attach = MIMEApplication(f.read(),_subtype=\"pdf\")\n        attach.add_header('Content-Disposition','attachment',filename=str(path_to_pdf))\n        msg.attach(attach)\n        # send msg\n        server.send_message(msg)"
  },
  {
    "url": "https://stackoverflow.com/questions/65331736/how-can-i-publish-python-packages-to-codeartifact-using-poetry",
    "body": "# This will give the repo url without the /simple/ part\n# Example: https://<my-domain>-<domain-owner-id>.d.codeartifact.<region>.amazonaws.com/pypi/<my-repo>/\n# Note the lack of the \"aws:auth-token@\" part\nexport CODEARTIFACT_REPOSITORY_URL=`aws codeartifact get-repository-endpoint --domain my-domain --domain-owner domain-owner-id --repository my-repo --format pypi --query repositoryEndpoint --output text`\n# This will give the token to access the repo\nexport CODEARTIFACT_AUTH_TOKEN=`aws codeartifact get-authorization-token --domain my-domain --domain-owner domain-owner-id --query authorizationToken --output text`\n# This specifies the user who accesses the repo\nexport CODEARTIFACT_USER=aws\n# Now use all of these when configuring the repo in poetry\npoetry config repositories.<my-repo-name-for-poetry> $CODEARTIFACT_REPOSITORY_URL\npoetry config http-basic.<my-repo-name-for-poetry> $CODEARTIFACT_USER $CODEARTIFACT_AUTH_TOKEN"
  },
  {
    "url": "https://stackoverflow.com/questions/72482384/how-to-read-emails-from-gmail",
    "body": "import os.path\nimport base64\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nSCOPES = ['https://www.googleapis.com/auth/gmail.readonly','https://www.googleapis.com/auth/gmail.modify']\ndef readEmails():\n    \"\"\"Shows basic usage of the Gmail API.\n    Lists the user's Gmail labels.\n    \"\"\"\n    creds = None\n    # The file token.json stores the user's access and refresh tokens, and is\n    # created automatically when the authorization flow completes for the first\n    # time.\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    # If there are no (valid) credentials available, let the user log in.\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                # your creds file here. Please create json file as here https://cloud.google.com/docs/authentication/getting-started\n                'my_cred_file.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        # Save the credentials for the next run\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n    try:\n        # Call the Gmail API\n        service = build('gmail', 'v1', credentials=creds)\n        results = service.users().messages().list(userId='me', labelIds=['INBOX'], q=\"is:unread\").execute()\n        messages = results.get('messages',[]);\n        if not messages:\n            print('No new messages.')\n        else:\n            message_count = 0\n            for message in messages:\n                msg = service.users().messages().get(userId='me', id=message['id']).execute()\n                email_data = msg['payload']['headers']\n                for values in email_data:\n                    name = values['name']\n                    if name == 'From':\n                        from_name= values['value']\n                        for part in msg['payload']['parts']:\n                            try:\n                                data = part['body'][\"data\"]\n                                byte_code = base64.urlsafe_b64decode(data)\n                                text = byte_code.decode(\"utf-8\")\n                                print (\"This is the message: \"+ str(text))\n                                # mark the message as read (optional)\n                                msg  = service.users().messages().modify(userId='me', id=message['id'], body={'removeLabelIds': ['UNREAD']}).execute()\n                            except BaseException as error:\n                                pass\n    except Exception as error:\n        print(f'An error occurred: {error}')"
  },
  {
    "url": "https://stackoverflow.com/questions/61819120/how-to-get-the-endpoint-of-a-linestring-in-shapely",
    "body": "from shapely.wkt import loads\nfirst_line = loads(\"LINESTRING (51.2176008 4.4177154, 51.21758 4.4178548, 51.2175729 4.4179023, 51.21745162000732 4.41871738126533)\")\nsecond_line = loads(\"LINESTRING (51.21745162000732 4.41871738126533, 51.2174025 4.4190475, 51.217338 4.4194807, 51.2172511 4.4200562, 51.2172411 4.4201077, 51.2172246 4.4201654, 51.2172067 4.420205, 51.2171806 4.4202355, 51.2171074 4.4202929, 51.2170063 4.4203409, 51.2169564 4.4203641, 51.2168076 4.4204243, 51.2166588 4.4204833, 51.2159018 4.420431, 51.2154117 4.4203843)\")\nfirst_line = LineString(first_line.coords[:-1])\nsecond_line = LineString(second_line.coords[1:])\nprint(first_line.boundary[1], second_line.boundary[0])\n# POINT (51.2175729 4.4179023) POINT (51.2174025 4.4190475)"
  },
  {
    "url": "https://stackoverflow.com/questions/56858924/multivariate-input-lstm-in-pytorch",
    "body": "import random\nimport numpy as np\nimport torch\n# multivariate data preparation\nfrom numpy import array\nfrom numpy import hstack\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([x for x in range(0,100,10)])\nin_seq2 = array([x for x in range(5,105,10)])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))"
  },
  {
    "url": "https://stackoverflow.com/questions/56858924/multivariate-input-lstm-in-pytorch",
    "body": "class MV_LSTM(torch.nn.Module):\n    def __init__(self,n_features,seq_length):\n        super(MV_LSTM, self).__init__()\n        self.n_features = n_features\n        self.seq_len = seq_length\n        self.n_hidden = 20 # number of hidden states\n        self.n_layers = 1 # number of LSTM layers (stacked)\n\n        self.l_lstm = torch.nn.LSTM(input_size = n_features,\n                                 hidden_size = self.n_hidden,\n                                 num_layers = self.n_layers,\n                                 batch_first = True)\n        # according to pytorch docs LSTM output is\n        # (batch_size,seq_len, num_directions * hidden_size)\n        # when considering batch_first = True\n        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n\n\n    def init_hidden(self, batch_size):\n        # even with batch_first = True this remains same as docs\n        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n        self.hidden = (hidden_state, cell_state)\n\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.size()\n\n        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n        # lstm_out(with batch_first = True) is\n        # (batch_size,seq_len,num_directions * hidden_size)\n        # for following linear layer we want to keep batch_size dimension and merge rest\n        # .contiguous() -> solves tensor compatibility error\n        x = lstm_out.contiguous().view(batch_size,-1)\n        return self.l_linear(x)"
  },
  {
    "url": "https://stackoverflow.com/questions/56858924/multivariate-input-lstm-in-pytorch",
    "body": "mv_net.train()\nfor t in range(train_episodes):\n    for b in range(0,len(X),batch_size):\n        inpt = X[b:b+batch_size,:,:]\n        target = y[b:b+batch_size]\n\n        x_batch = torch.tensor(inpt,dtype=torch.float32)\n        y_batch = torch.tensor(target,dtype=torch.float32)\n\n        mv_net.init_hidden(x_batch.size(0))\n    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)\n    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n        output = mv_net(x_batch)\n        loss = criterion(output.view(-1), y_batch)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    print('step : ' , t , 'loss : ' , loss.item())"
  },
  {
    "url": "https://stackoverflow.com/questions/55576314/conv1d-with-kernel-size-1-vs-linear-layer",
    "body": "def count_parameters(model):\n    \"\"\"Count the number of parameters in a model.\"\"\"\n    return sum([p.numel() for p in model.parameters()])\nconv = torch.nn.Conv1d(8,32,1)\nprint(count_parameters(conv))\n# 288\nlinear = torch.nn.Linear(8,32)\nprint(count_parameters(linear))\n# 288\nprint(conv.weight.shape)\n# torch.Size([32, 8, 1])\nprint(linear.weight.shape)\n# torch.Size([32, 8])\n# use same initialization\nlinear.weight = torch.nn.Parameter(conv.weight.squeeze(2))\nlinear.bias = torch.nn.Parameter(conv.bias)\ntensor = torch.randn(128,256,8)\npermuted_tensor = tensor.permute(0,2,1).clone().contiguous()\nout_linear = linear(tensor)\nprint(out_linear.mean())\n# tensor(0.0067, grad_fn=<MeanBackward0>)\nout_conv = conv(permuted_tensor)\nprint(out_conv.mean())\n# tensor(0.0067, grad_fn=<MeanBackward0>)"
  },
  {
    "url": "https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model",
    "body": "class my_model(keras.Model):\n    def __init__(self, dim):\n        super(my_model, self).__init__()\n        self.Base  = keras.keras.applications.VGG16(\n            input_shape=(dim),\n            include_top = False,\n            weights = 'imagenet'\n        )\n        self.GAP   = L.GlobalAveragePooling2D()\n        self.BAT   = L.BatchNormalization()\n        self.DROP  = L.Dropout(rate=0.1)\n        self.DENS  = L.Dense(256, activation='relu', name = 'dense_A')\n        self.OUT   = L.Dense(1, activation='sigmoid')\n\n    def call(self, inputs):\n        x  = self.Base(inputs)\n        g  = self.GAP(x)\n        b  = self.BAT(g)\n        d  = self.DROP(b)\n        d  = self.DENS(d)\n        return self.OUT(d)\n\n    # AFAIK: The most convenient method to print model.summary()\n    # similar to the sequential or functional API like.\n    def build_graph(self):\n        x = Input(shape=(dim))\n        return Model(inputs=[x], outputs=self.call(x))\ndim = (124,124,3)\nmodel = my_model((dim))\nmodel.build((None, *dim))\nmodel.build_graph().summary()"
  },
  {
    "url": "https://stackoverflow.com/questions/61427583/how-do-i-plot-a-keras-tensorflow-subclassing-api-model",
    "body": "Layer (type)                 Output Shape              Param #\n=================================================================\ninput_67 (InputLayer)        [(None, 124, 124, 3)]     0\n_________________________________________________________________\nvgg16 (Functional)           (None, 3, 3, 512)         14714688\n_________________________________________________________________\nglobal_average_pooling2d_32  (None, 512)               0\n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 512)               2048\n_________________________________________________________________\ndropout_5 (Dropout)          (None, 512)               0\n_________________________________________________________________\ndense_A (Dense)              (None, 256)               402192\n_________________________________________________________________\ndense_7 (Dense)              (None, 1)                 785\n=================================================================\nTotal params: 14,848,321\nTrainable params: 14,847,297\nNon-trainable params: 1,024"
  },
  {
    "url": "https://stackoverflow.com/questions/59979467/accessing-microsoft-sharepoint-files-and-data-using-python",
    "body": "from office365.runtime.auth.authentication_context import AuthenticationContext\nfrom office365.sharepoint.client_context import ClientContext\nfrom office365.sharepoint.files.file import File\n####inputs########\n# This will be the URL that points to your sharepoint site.\n# Make sure you change only the parts of the link that start with \"Your\"\nurl_shrpt = 'https://YourOrganisation.sharepoint.com/sites/YourSharepointSiteName'\nusername_shrpt = 'YourUsername'\npassword_shrpt = 'YourPassword'\nfolder_url_shrpt = '/sites/YourSharepointSiteName/Shared%20Documents/YourSharepointFolderName/'\n#######################\n###Authentication###For authenticating into your sharepoint site###\nctx_auth = AuthenticationContext(url_shrpt)\nif ctx_auth.acquire_token_for_user(username_shrpt, password_shrpt):\n  ctx = ClientContext(url_shrpt, ctx_auth)\n  web = ctx.web\n  ctx.load(web)\n  ctx.execute_query()\n  print('Authenticated into sharepoint as: ',web.properties['Title'])\nelse:\n  print(ctx_auth.get_last_error())\n############################\n\n\n\n\n####Function for extracting the file names of a folder in sharepoint###\n###If you want to extract the folder names instead of file names, you have to change \"sub_folders = folder.files\" to \"sub_folders = folder.folders\" in the below function\nglobal print_folder_contents\ndef print_folder_contents(ctx, folder_url):\n    try:\n\n        folder = ctx.web.get_folder_by_server_relative_url(folder_url)\n        fold_names = []\n        sub_folders = folder.files #Replace files with folders for getting list of folders\n        ctx.load(sub_folders)\n        ctx.execute_query()\n\n        for s_folder in sub_folders:\n\n            fold_names.append(s_folder.properties[\"Name\"])\n        return fold_names\n    except Exception as e:\n        print('Problem printing out library contents: ', e)\n######################################################\n\n\n# Call the function by giving your folder URL as input\nfilelist_shrpt=print_folder_contents(ctx,folder_url_shrpt)\n#Print the list of files present in the folder\nprint(filelist_shrpt)"
  },
  {
    "url": "https://stackoverflow.com/questions/11161901/how-to-install-python-modules-in-blender",
    "body": "# 1. launch next 2 lines of code in blender python interpreter\nimport pip\npip.main(['install', 'tqdm', '--user'])\nimport sys\n# 2. watch blender's python path in console output at this moment\n# 3. insert the path to packages_path below and uncomment\n# packages_path = \"C:\\\\Users\\\\<Username>\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\" + \"\\\\..\\\\site-packages\" # the path you see in console\n# 4. uncomment the next code and launch script in blender interpreter again\n# sys.path.insert(0, packages_path )\n# import tqdm\n# use installed packages here"
  },
  {
    "url": "https://stackoverflow.com/questions/57734298/how-can-i-provide-shared-state-to-my-flask-app-with-multiple-workers-without-dep",
    "body": "from multiprocessing import Lock\nfrom multiprocessing.managers import AcquirerProxy, BaseManager, DictProxy\ndef get_shared_state(host, port, key):\n    shared_dict = {}\n    shared_lock = Lock()\n    manager = BaseManager((host, port), key)\n    manager.register(\"get_dict\", lambda: shared_dict, DictProxy)\n    manager.register(\"get_lock\", lambda: shared_lock, AcquirerProxy)\n    try:\n        manager.get_server()\n        manager.start()\n    except OSError:  # Address already in use\n        manager.connect()\n    return manager.get_dict(), manager.get_lock()"
  },
  {
    "url": "https://stackoverflow.com/questions/25972482/how-to-move-files-in-google-cloud-storage-from-one-bucket-to-another-bucket-by-p",
    "body": "from google.cloud import storage\nimport os\n\n    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"path_to_your_creds.json\"\ndef mv_blob(bucket_name, blob_name, new_bucket_name, new_blob_name):\n    \"\"\"\n    Function for moving files between directories or buckets. it will use GCP's copy\n    function then delete the blob from the old location.\n\n    inputs\n    -----\n    bucket_name: name of bucket\n    blob_name: str, name of file\n        ex. 'data/some_location/file_name'\n    new_bucket_name: name of bucket (can be same as original if we're just moving around directories)\n    new_blob_name: str, name of file in new directory in target bucket\n        ex. 'data/destination/file_name'\n    \"\"\"\n    storage_client = storage.Client()\n    source_bucket = storage_client.get_bucket(bucket_name)\n    source_blob = source_bucket.blob(blob_name)\n    destination_bucket = storage_client.get_bucket(new_bucket_name)\n    # copy to new destination\n    new_blob = source_bucket.copy_blob(\n        source_blob, destination_bucket, new_blob_name)\n    # delete in old destination\n    source_blob.delete()\n\n    print(f'File moved from {source_blob} to {new_blob_name}')"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI\nfrom fastapi.responses import RedirectResponse, HTMLResponse\nimport uvicorn\napp = FastAPI()\n\n@app.get('/', response_class=HTMLResponse)\nasync def home():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n       <body>\n          <h2>Click the \"Submit\" button to be redirected to domain B</h2>\n          <form method=\"POST\" action=\"/submit\">\n             <input type=\"submit\" value=\"Submit\">\n          </form>\n       </body>\n    </html>\n    \"\"\"\n\n@app.post(\"/submit\")\nasync def submit():\n    token = 'MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3'\n    redirect_url = f'http://example.test:8001/submit?token={token}'\n    response = RedirectResponse(redirect_url)\n    response.set_cookie(key='access-token', value=token, httponly=True)  # set cookie for domain A too\n    return response\n\nif __name__ == '__main__':\n    uvicorn.run(app, host='0.0.0.0', port=8000)"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI, Request, status\nfrom fastapi.responses import RedirectResponse\nimport uvicorn\napp = FastAPI()\n@app.get('/')\nasync def home(request: Request):\n    token = request.cookies.get('access-token')\n    return 'You have successfully been redirected to domain B!' \\\n           f' Your access token is: {token}'\n\n@app.post('/submit')\nasync def submit(request: Request, token: str):\n    redirect_url = request.url_for('home')\n    response = RedirectResponse(redirect_url, status_code=status.HTTP_303_SEE_OTHER)\n    response.set_cookie(key='access-token', value=token, httponly=True)\n    return response\n\nif __name__ == '__main__':\n    uvicorn.run(app, host='0.0.0.0', port=8001)"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI, Request, Response\nfrom fastapi.responses import HTMLResponse\napp = FastAPI()\n@app.get('/', response_class=HTMLResponse)\nasync def home():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n       <body>\n          <h2>Click the \"Submit\" button to be redirected to domain B</h2>\n          <input type=\"button\" value=\"Submit\" onclick=\"submit()\"><br>\n          <iframe id=\"cross_domain_page\" src=\"http://example.test:8001/iframe\"  frameborder=\"0\" scrolling=\"no\" style=\"background:transparent;margin:auto;display:block\"></iframe>\n          <script>\n             function submit() {\n                fetch('/submit', {\n                     method: 'POST',\n                  })\n                  .then(res => {\n                     authHeader = res.headers.get('Authorization');\n                     if (authHeader.startsWith(\"Bearer \"))\n                        token = authHeader.substring(7, authHeader.length);\n                     return res.text();\n                  })\n                  .then(data => {\n                     document.getElementById('cross_domain_page').contentWindow.postMessage(token, \"http://example.test:8001\");\n                  })\n                  .catch(error => {\n                     console.error(error);\n                  });\n             }\n\n             window.addEventListener(\"message\", (event) => {\n                if (event.origin !== \"http://example.test:8001\")\n                  return;\n\n                if (event.data == \"cookie is set\")\n                  window.location.href = 'http://example.test:8001/';\n             }, false);\n          </script>\n       </body>\n    </html>\n    \"\"\"\n@app.post('/submit')\nasync def submit():\n    token = 'MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3'\n    headers = {'Authorization': f'Bearer {token}'}\n    response = Response('success', headers=headers)\n    response.set_cookie(key='access-token', value=token, httponly=True)  # set cookie for domain A too\n    return response\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host='0.0.0.0', port=8000)"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI, Request, Response\nfrom fastapi.responses import HTMLResponse\napp = FastAPI()\n@app.get('/iframe', response_class=HTMLResponse)\nasync def iframe():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n       <head>\n          <script>\n             window.addEventListener(\"message\", (event) => {\n                if (event.origin !== \"http://127.0.0.1:8000\" && event.origin !== \"http://localhost:8000\")\n                   return;\n\n                fetch('/submit', {\n                      method: 'POST',\n                      headers: {\n                         'Authorization': `Bearer ${event.data}`\n                      }\n                   })\n                   .then(res => res.text())\n                   .then(data => {\n                      event.source.postMessage(\"cookie is set\", event.origin);\n                   })\n                   .catch(error => {\n                      console.error(error);\n                   })\n             }, false);\n          </script>\n       </head>\n    </html>\n    \"\"\"\n\n@app.get('/')\nasync def home(request: Request):\n    token = request.cookies.get('access-token')\n    return 'You have been successfully redirected to domain B!' \\\n           f' Your access token is: {token}'\n@app.post('/submit')\nasync def submit(request: Request, response: Response):\n    authHeader = request.headers.get('Authorization')\n    if authHeader.startswith(\"Bearer \"):\n        token = authHeader[7:]\n    response.set_cookie(key='access-token', value=token, samesite='none', secure=True, httponly=True)\n    return 'success'\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host='0.0.0.0', port=8001)"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI, Request, Response\nfrom fastapi.responses import HTMLResponse\napp = FastAPI()\n\n@app.get('/', response_class=HTMLResponse)\nasync def home():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n       <body>\n          <h2>Click the \"Submit\" button to be redirected to domain B</h2>\n          <input type=\"button\" value=\"Submit\" onclick=\"submit()\"><br>\n          <iframe id=\"cross_domain_page\" src=\"http://example.test:8001/iframe\"  frameborder=\"0\" scrolling=\"no\" style=\"background:transparent;margin:auto;display:block\"></iframe>\n          <script>\n             function submit() {\n                fetch('/submit', {\n                     method: 'POST',\n                  })\n                  .then(res => {\n                     authHeader = res.headers.get('Authorization');\n                     if (authHeader.startsWith(\"Bearer \"))\n                        token = authHeader.substring(7, authHeader.length);\n                     return res.text();\n                  })\n                  .then(data => {\n                     document.getElementById('cross_domain_page').contentWindow.postMessage(token, \"http://example.test:8001\");\n                  })\n                  .catch(error => {\n                     console.error(error);\n                  });\n             }\n\n             window.addEventListener(\"message\", (event) => {\n                if (event.origin !== \"http://example.test:8001\")\n                  return;\n\n                if (event.data == \"token stored\")\n                  window.location.href = 'http://example.test:8001/redirect';\n             }, false);\n\n          </script>\n       </body>\n    </html>\n    \"\"\"\n@app.post('/submit')\nasync def submit():\n    token = 'MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3'\n    headers = {'Authorization': f'Bearer {token}'}\n    response = Response('success', headers=headers)\n    response.set_cookie(key='access-token', value=token, httponly=True)  # set cookie for domain A too\n    return response\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host='0.0.0.0', port=8000)"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI, Request, Response\nfrom fastapi.responses import HTMLResponse\napp = FastAPI()\n@app.get('/iframe', response_class=HTMLResponse)\nasync def iframe():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n       <head>\n          <script>\n             window.addEventListener(\"message\", (event) => {\n                if (event.origin !== \"http://127.0.0.1:8000\" && event.origin !== \"http://localhost:8000\")\n                   return;\n\n                localStorage.setItem('token', event.data);\n                event.source.postMessage(\"token stored\", event.origin);\n             }, false);\n          </script>\n       </head>\n    </html>\n    \"\"\"\n\n@app.get('/redirect', response_class=HTMLResponse)\nasync def redirect():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n       <head>\n          <script>\n            const token = localStorage.getItem('token');\n            localStorage.removeItem(\"token\");\n            fetch('/submit', {\n                  method: 'POST',\n                  headers: {\n                     'Authorization': `Bearer ${token}`\n                  }\n               })\n               .then(res => res.text())\n               .then(data => {\n                  window.location.href = 'http://example.test:8001/';\n               })\n               .catch(error => {\n                  console.error(error);\n               })\n          </script>\n       </head>\n    </html>\n    \"\"\"\n\n\n@app.get('/')\nasync def home(request: Request):\n    token = request.cookies.get('access-token')\n    return 'You have been successfully redirected to domain B!' \\\n           f' Your access token is: {token}'\n@app.post('/submit')\nasync def submit(request: Request, response: Response):\n    authHeader = request.headers.get('Authorization')\n    if authHeader.startswith(\"Bearer \"):\n        token = authHeader[7:]\n    response.set_cookie(key='access-token', value=token, httponly=True)\n    return 'success'\n\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host='0.0.0.0', port=8001)"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI, Response\nfrom fastapi.responses import HTMLResponse\napp = FastAPI()\n@app.get('/', response_class=HTMLResponse)\nasync def home():\n    return \"\"\"\n    <!DOCTYPE html>\n    <html>\n       <body>\n          <h2>Click the \"Submit\" button to be redirected to domain B</h2>\n          <input type=\"button\" value=\"Submit\" onclick=\"submit()\"><br>\n          <script>\n            function submit() {\n               fetch('/submit', {\n                     method: 'POST',\n                  })\n                  .then(res => {\n                     authHeader = res.headers.get('Authorization');\n                     if (authHeader.startsWith(\"Bearer \"))\n                        token = authHeader.substring(7, authHeader.length);\n                     return res.text();\n                  })\n                  .then(data => {\n                     var url = 'http://example.test:8001/submit?token=' + encodeURIComponent(token);\n                     var img = document.createElement('img');\n                     img.style = 'display:none';\n                     img.crossOrigin = 'use-credentials'; // needed for CORS\n                     img.onerror = function(){\n\t\t\t\t\t\twindow.location.href = 'http://example.test:8001/';\n                     }\n                     img.src = url;\n                  })\n                  .catch(error => {\n                     console.error(error);\n                  });\n            }\n          </script>\n       </body>\n    </html>\n    \"\"\"\n\n@app.post('/submit')\nasync def submit():\n    token = 'MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3'\n    headers = {'Authorization': f'Bearer {token}'}\n    response = Response('success', headers=headers)\n    response.set_cookie(key='access-token', value=token, httponly=True)  # set cookie for domain A too\n    return response\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host='0.0.0.0', port=8000)"
  },
  {
    "url": "https://stackoverflow.com/questions/73547776/how-to-redirect-from-one-domain-to-another-and-set-cookies-or-headers-for-the-ot",
    "body": "from fastapi import FastAPI, Request, Response\nfrom fastapi.responses import RedirectResponse\nfrom fastapi.middleware.cors import CORSMiddleware\napp = FastAPI()\norigins = ['http://localhost:8000', 'http://127.0.0.1:8000',\n           'https://localhost:8000', 'https://127.0.0.1:8000']\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n@app.get('/')\nasync def home(request: Request):\n    token = request.cookies.get('access-token')\n    return 'You have been successfully redirected to domain B!' \\\n           f' Your access token is: {token}'\n\n@app.get('/submit')\nasync def submit(request: Request, response: Response, token: str):\n    response.set_cookie(key='access-token', value=token, samesite='none', secure=True, httponly=True)\n    return 'success'\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host='0.0.0.0', port=8001)"
  },
  {
    "url": "https://stackoverflow.com/questions/63721614/unhashable-type-in-fastapi-request",
    "body": "from pydantic import BaseModel\nfrom typing import Set\nclass MyBaseModel(BaseModel):\n    def __hash__(self):  # make hashable BaseModel subclass\n        return hash((type(self),) + tuple(self.__dict__.values()))\nclass Customer(MyBaseModel):  # Use hashable sublclass for your model\n    UID: str\n    CustName: str\nclass PackageIn(BaseModel):\n    lead_id: str\n    parties: Set[Customer]\n    # threshold: Optional[int] = 85\ndata = {\n    \"lead_id\": 'LD123',\n    \"parties\": [\n       {\n           \"UID\": 123123,\n           \"CustName\": \"JOhn Doe\",\n       }]}\nPackageIn.parse_obj(data) # This part fastapi will make on post request, just for test\n> <PackageIn lead_id='LD123' parties={<Customer UID='123123' CustName='JOhn Doe'>}>"
  },
  {
    "url": "https://stackoverflow.com/questions/55819330/catching-exceptions-raised-in-qapplication",
    "body": "import sys\nimport traceback\nfrom PyQt5 import QtWidgets, QtGui, QtCore\nclass ErrorApp:\n    # ...\n    def raise_error(self):\n        assert False\ndef excepthook(exc_type, exc_value, exc_tb):\n    tb = \"\".join(traceback.format_exception(exc_type, exc_value, exc_tb))\n    print(\"error caught!:\")\n    print(\"error message:\\n\", tb)\n    QtWidgets.QApplication.quit()\n    # or QtWidgets.QApplication.exit(0)\nsys.excepthook = excepthook\ne = ErrorApp()\nret = e.app.exec_()\nprint(\"event loop exited\")\nsys.exit(ret)"
  },
  {
    "url": "https://stackoverflow.com/questions/64294962/how-to-implement-learning-to-rank-using-lightgbm",
    "body": "query_id      var1      var2      var3  relevance\n0           0  0.624776  0.191463  0.598358          0\n1           0  0.258280  0.658307  0.148386          0\n2           0  0.893683  0.059482  0.340426          0\n3           0  0.879514  0.526022  0.712648          1\n4           0  0.188580  0.279471  0.062942          0\n..        ...       ...       ...       ...        ...\n995        99  0.509672  0.552873  0.166913          0\n996        99  0.244307  0.356738  0.925570          0\n997        99  0.827925  0.827747  0.695029          1\n998        99  0.476761  0.390823  0.670150          0\n999        99  0.241392  0.944994  0.671594          0\n[1000 rows x 5 columns]"
  },
  {
    "url": "https://stackoverflow.com/questions/71581197/what-is-the-loss-function-used-in-trainer-from-the-transformers-library-of-huggi",
    "body": "if labels is not None:\n    if self.config.problem_type is None:\n        if self.num_labels == 1:\n            self.config.problem_type = \"regression\"\n        elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n            self.config.problem_type = \"single_label_classification\"\n        else:\n            self.config.problem_type = \"multi_label_classification\"\n    if self.config.problem_type == \"regression\":\n        loss_fct = MSELoss()\n        if self.num_labels == 1:\n            loss = loss_fct(logits.squeeze(), labels.squeeze())\n        else:\n            loss = loss_fct(logits, labels)\n    elif self.config.problem_type == \"single_label_classification\":\n        loss_fct = CrossEntropyLoss()\n        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n    elif self.config.problem_type == \"multi_label_classification\":\n        loss_fct = BCEWithLogitsLoss()\n        loss = loss_fct(logits, labels)"
  },
  {
    "url": "https://stackoverflow.com/questions/60637120/detect-circles-in-opencv",
    "body": "import cv2\nimport numpy as np\n# Load image, grayscale, median blur, Otsus threshold\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nblur = cv2.medianBlur(gray, 11)\nthresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n# Morph open\nkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\nopening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=3)\n# Find contours and filter using contour area and aspect ratio\ncnts = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    peri = cv2.arcLength(c, True)\n    approx = cv2.approxPolyDP(c, 0.04 * peri, True)\n    area = cv2.contourArea(c)\n    if len(approx) > 5 and area > 1000 and area < 500000:\n        ((x, y), r) = cv2.minEnclosingCircle(c)\n        cv2.circle(image, (int(x), int(y)), int(r), (36, 255, 12), 2)\ncv2.imshow('thresh', thresh)\ncv2.imshow('opening', opening)\ncv2.imshow('image', image)\ncv2.waitKey()"
  },
  {
    "url": "https://stackoverflow.com/questions/34593609/modifying-rules-for-a-given-ec2-security-group-with-boto3",
    "body": "import boto3\nclient = boto3.client('ec2')\nsg_rules_list = [{'SecurityGroupRuleId': 'sgr-07de36a0521f39c8b',\n                  'SecurityGroupRule': {\n                      'IpProtocol': 'tcp',\n                      'FromPort': 22,\n                      'ToPort': 22,\n                      'CidrIpv4': '3.3.3.3/32',\n                      'Description': 'added ssh port'\n                  }\n                  }\n                 ]\nresponse = client.modify_security_group_rules(GroupId='sg-00f3b9232325b20fb',\n                                              SecurityGroupRules=sg_rules_list)"
  },
  {
    "url": "https://stackoverflow.com/questions/62988494/adjust-width-of-dropdown-menu-option-in-dash-plotly",
    "body": "app.layout = html.Div(\n    children=[\n        html.H1(children=\"Welcome to Portfolio Construction Engine!\"),\n        html.Div(\n            children=\"What would you like to do?\",\n            style={\"font-style\": \"italic\", \"font-weight\": \"bold\"},\n        ),\n        html.Div(\n            [\n                dcc.Dropdown(\n                    id=\"demo-dropdown\",\n                    options=[\n                        {\"label\": \"Upload Scores\", \"value\": \"upload_scores\"},\n                        {\"label\": \"Analyze Portfolios\", \"value\": \"analyze_portfoliio\"},\n                        {\n                            \"label\": \"Generate Data for IC Engine\",\n                            \"value\": \"gen_data_ic_engine\",\n                        },\n                    ],\n                    placeholder=\"Select a task...\",\n                    # style={\"width\": \"50%\"},  NOT HERE\n                ),\n                html.Div(id=\"dd-output-container\"),\n            ],\n            style={\"width\": \"50%\"},\n        ),\n    ]\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/61112684/how-to-subclass-a-dictionary-so-it-supports-generic-type-hints",
    "body": "from collections import abc  # Used for isinstance check in `update()`.\nfrom typing import Dict, Iterator, MutableMapping, TypeVar\nKT = TypeVar('KT')\nVT = TypeVar('VT')\nclass MyDict(MutableMapping[KT, VT]):\n    def __init__(self, dictionary=None, /, **kwargs) -> None:\n        self.data: Dict[KT, VT] = {}\n        if dictionary is not None:\n            self.update(dictionary)\n        if kwargs:\n            self.update(kwargs)\n\n    def __contains__(self, key: KT) -> bool:\n        return key in self.data\n    def __delitem__(self, key: KT) -> None:\n        del self.data[key]\n    def __getitem__(self, key: KT) -> VT:\n        if key in self.data:\n            return self.data[key]\n        raise KeyError(key)\n    def __len__(self) -> int:\n        return len(self.data)\n    def __iter__(self) -> Iterator[KT]:\n        return iter(self.data)\n    def __setitem__(self, key: KT, value: VT) -> None:\n        self.data[key] = value\n\n    @classmethod\n    def fromkeys(cls, iterable: Iterable[KT], value: VT) -> \"MyDict\":\n        \"\"\"Create a new dictionary with keys from `iterable` and values set\n        to `value`.\n        Args:\n            iterable: A collection of keys.\n            value: The default value. All of the values refer to just a single\n                instance, so it generally does not make sense for `value` to be a\n                mutable object such as an empty list. To get distinct values, use\n                a dict comprehension instead.\n        Returns:\n            A new instance of MyDict.\n        \"\"\"\n        d = cls()\n        for key in iterable:\n            d[key] = value\n        return d\n    def update(self, other=(), /, **kwds) -> None:\n        \"\"\"Updates the dictionary from an iterable or mapping object.\"\"\"\n        if isinstance(other, abc.Mapping):\n            for key in other:\n                self.data[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self.data[key] = other[key]\n        else:\n            for key, value in other:\n                self.data[key] = value\n        for key, value in kwds.items():\n            self.data[key] = value"
  },
  {
    "url": "https://stackoverflow.com/questions/44760221/pyodbc-connect-works-but-not-sqlalchemy-create-engine-connect",
    "body": "import pandas as pd\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import URL\nconnection_string = (\n    r\"Driver=ODBC Driver 17 for SQL Server;\"\n    r\"Server=(local)\\SQLEXPRESS;\"\n    r\"Database=myDb;\"\n    r\"Trusted_Connection=yes;\"\n)\nconnection_url = URL.create(\n    \"mssql+pyodbc\",\n    query={\"odbc_connect\": connection_string}\n)\nengine = create_engine(connection_url)\ndf = pd.DataFrame([(1, \"foo\")], columns=[\"id\", \"txt\"])\npd.to_sql(\"test_table\", engine, if_exists=\"replace\", index=False)"
  },
  {
    "url": "https://stackoverflow.com/questions/70866415/how-to-install-python-specific-version-on-docker",
    "body": "# compile python from source - avoid unsupported library problems\nRUN apt update -y && sudo apt upgrade -y && \\\n    apt-get install -y wget build-essential checkinstall  libreadline-gplv2-dev  libncursesw5-dev  libssl-dev  libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev libffi-dev zlib1g-dev && \\\n    cd /usr/src && \\\n    sudo wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz && \\\n    sudo tar xzf Python-3.8.10.tgz && \\\n    cd Python-3.8.10 && \\\n    sudo ./configure --enable-optimizations && \\\n    sudo make altinstall"
  },
  {
    "url": "https://stackoverflow.com/questions/57909401/what-are-the-command-line-arguments-passed-to-grpc-tools-protoc",
    "body": "$ python -m grpc.tools.protoc -h\nUsage: /usr/local/google/home/lidiz/.local/lib/python2.7/site-packages/grpc_tools/protoc.py [OPTION] PROTO_FILES\nParse PROTO_FILES and generate output based on the options given:\n  -IPATH, --proto_path=PATH   Specify the directory in which to search for\n                              imports.  May be specified multiple times;\n                              directories will be searched in order.  If not\n                              given, the current working directory is used.\n  --version                   Show version info and exit.\n  -h, --help                  Show this text and exit.\n  --encode=MESSAGE_TYPE       Read a text-format message of the given type\n                              from standard input and write it in binary\n                              to standard output.  The message type must\n                              be defined in PROTO_FILES or their imports.\n  --decode=MESSAGE_TYPE       Read a binary message of the given type from\n                              standard input and write it in text format\n                              to standard output.  The message type must\n                              be defined in PROTO_FILES or their imports.\n  --decode_raw                Read an arbitrary protocol message from\n                              standard input and write the raw tag/value\n                              pairs in text format to standard output.  No\n                              PROTO_FILES should be given when using this\n                              flag.\n  --descriptor_set_in=FILES   Specifies a delimited list of FILES\n                              each containing a FileDescriptorSet (a\n                              protocol buffer defined in descriptor.proto).\n                              The FileDescriptor for each of the PROTO_FILES\n                              provided will be loaded from these\n                              FileDescriptorSets. If a FileDescriptor\n                              appears multiple times, the first occurrence\n                              will be used.\n  -oFILE,                     Writes a FileDescriptorSet (a protocol buffer,\n    --descriptor_set_out=FILE defined in descriptor.proto) containing all of\n                              the input files to FILE.\n  --include_imports           When using --descriptor_set_out, also include\n                              all dependencies of the input files in the\n                              set, so that the set is self-contained.\n  --include_source_info       When using --descriptor_set_out, do not strip\n                              SourceCodeInfo from the FileDescriptorProto.\n                              This results in vastly larger descriptors that\n                              include information about the original\n                              location of each decl in the source file as\n                              well as surrounding comments.\n  --dependency_out=FILE       Write a dependency output file in the format\n                              expected by make. This writes the transitive\n                              set of input file paths to FILE\n  --error_format=FORMAT       Set the format in which to print errors.\n                              FORMAT may be 'gcc' (the default) or 'msvs'\n                              (Microsoft Visual Studio format).\n  --print_free_field_numbers  Print the free field numbers of the messages\n                              defined in the given proto files. Groups share\n                              the same field number space with the parent\n                              message. Extension ranges are counted as\n                              occupied fields numbers.\n  --plugin=EXECUTABLE         Specifies a plugin executable to use.\n                              Normally, protoc searches the PATH for\n                              plugins, but you may specify additional\n                              executables not in the path using this flag.\n                              Additionally, EXECUTABLE may be of the form\n                              NAME=PATH, in which case the given plugin name\n                              is mapped to the given executable even if\n                              the executable's own name differs.\n  --grpc_python_out=OUT_DIR   Generate Python source file.\n  --python_out=OUT_DIR        Generate Python source file.\n  @<filename>                 Read options and filenames from file. If a\n                              relative file path is specified, the file\n                              will be searched in the working directory.\n                              The --proto_path option will not affect how\n                              this argument file is searched. Content of\n                              the file will be expanded in the position of\n                              @<filename> as in the argument list. Note\n                              that shell expansion is not applied to the\n                              content of the file (i.e., you cannot use\n                              quotes, wildcards, escapes, commands, etc.).\n                              Each line corresponds to a single argument,\n                              even if it contains spaces."
  },
  {
    "url": "https://stackoverflow.com/questions/48925086/choosing-subset-of-farthest-points-in-given-set-of-points",
    "body": "import numpy as np\nfrom scipy.spatial import ConvexHull\nfrom scipy.spatial.distance import cdist\np = 300\nN = 16000000\n# Find a convex hull in O(N log N)\npoints = np.random.rand(N, 3)   # N random points in 3-D\n# Returned 420 points in testing\nhull = ConvexHull(points)\n# Extract the points forming the hull\nhullpoints = points[hull.vertices,:]\n# Naive way of finding the best pair in O(H^2) time if H is number of points on\n# hull\nhdist = cdist(hullpoints, hullpoints, metric='euclidean')\n# Get the farthest apart points\nbestpair = np.unravel_index(hdist.argmax(), hdist.shape)\nP = np.array([hullpoints[bestpair[0]],hullpoints[bestpair[1]]])\n# Now we have a problem\nprint(\"Finding optimal set...\")\nwhile len(P)<p:\n  print(\"P size = {0}\".format(len(P)))\n  distance_to_P        = cdist(points, P)\n  minimum_to_each_of_P = np.min(distance_to_P, axis=1)\n  best_new_point_idx   = np.argmax(minimum_to_each_of_P)\n  best_new_point = np.expand_dims(points[best_new_point_idx,:],0)\n  P = np.append(P,best_new_point,axis=0)\nprint(P)"
  },
  {
    "url": "https://stackoverflow.com/questions/36937461/how-can-i-send-an-email-using-python-loggings-smtphandler-and-ssl",
    "body": "from email.message import EmailMessage\nimport email.utils\nclass SSLSMTPHandler(SMTPHandler):\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n        \"\"\"\n        try:\n            port = self.mailport\n            if not port:\n                port = smtplib.SMTP_PORT\n            smtp = smtplib.SMTP_SSL(self.mailhost, port)\n            msg = EmailMessage()\n            msg['From'] = self.fromaddr\n            msg['To'] = ','.join(self.toaddrs)\n            msg['Subject'] = self.getSubject(record)\n            msg['Date'] = email.utils.localtime()\n            msg.set_content(self.format(record))\n            if self.username:\n                smtp.login(self.username, self.password)\n            smtp.send_message(msg, self.fromaddr, self.toaddrs)\n            smtp.quit()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)"
  },
  {
    "url": "https://stackoverflow.com/questions/72610552/most-replayed-data-of-youtube-video-via-api",
    "body": "{\n    \"kind\": \"youtube#videoListResponse\",\n    \"etag\": \"NotImplemented\",\n    \"items\": [\n        {\n            \"kind\": \"youtube#video\",\n            \"etag\": \"NotImplemented\",\n            \"id\": \"XiCrniLQGYc\",\n            \"mostReplayed\": {\n                \"markers\": [\n                    {\n                        \"startMillis\": 0,\n                        \"intensityScoreNormalized\": 1\n                    },\n                    {\n                        \"startMillis\": 2580,\n                        \"intensityScoreNormalized\": 0.7083409245967562\n                    },\n                    {\n                        \"startMillis\": 5160,\n                        \"intensityScoreNormalized\": 0.6381007317793738\n                    },\n                    ...\n                    {\n                        \"startMillis\": 255420,\n                        \"intensityScoreNormalized\": 0.012864077773078256\n                    }\n                ],\n                \"timedMarkerDecorations\": [\n                    {\n                        \"visibleTimeRangeStartMillis\": 0,\n                        \"visibleTimeRangeEndMillis\": 10320\n                    }\n                ]\n            }\n        }\n    ]\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/59996493/does-await-always-give-other-tasks-a-chance-to-execute",
    "body": "import asyncio\nimport time\nasync def caller1():\n    for i in range(5):\n        await callee1()\nasync def callee1():\n    await asyncio.sleep(1)\n    print(f\"called at {time.strftime('%X')}\")\nasync def caller2():\n    for i in range(5):\n        await callee2()\nasync def callee2():\n    time.sleep(1)\n    print(f\"sync called at {time.strftime('%X')}\")\nasync def main():\n    task1 = asyncio.create_task(caller1())\n    task2 = asyncio.create_task(caller2())\n    await task1\n    await task2\nasyncio.run(main())"
  },
  {
    "url": "https://stackoverflow.com/questions/60873454/how-can-i-list-all-the-virtual-environments-created-with-venv",
    "body": "`\njvenvfindall(){  # search for Python virtual envs.  -v for verbose details\n    unset verbose\n\n    OPTIND=1\n    while getopts 'v' OPTION; do\n      case \"$OPTION\" in\n\n        v)\n          verbose=1\n          ;;\n\n        ?)\n          ;;\n      esac\n    done\n    shift \"$(($OPTIND -1))\"\n\n\n    local bup=$PWD\n    for dn in $(mdfind -name activate | egrep /bin/activate$|  xargs -o egrep -l nondestructive 2>/dev/null | xargs -L 1 dirname | xargs -L 1 dirname)\n    do\n        if [[ -z \"$verbose\" ]]; then\n            printf \"$dn\\n\"\n        else\n            printf \"\\n\\nvenv info for $dn:\\n\"\n            cd $dn\n            echo space usage, $(du -d 0 -h)\n            #requires the jq and jc utilities... to extract create and modification times\n            echo create, mod dttm: $(stat . | jc --stat | jq '.[]|{birth_time, change_time}')\n            tree -d -L 1 lib\n        fi\n    done\n    cd $bup\n}"
  },
  {
    "url": "https://stackoverflow.com/questions/51525237/how-to-set-up-httphandler-for-python-logging",
    "body": "import json\nimport logging\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\nclass CustomHttpHandler(logging.Handler):\n    def __init__(self, url: str, token: str, silent: bool = True):\n        '''\n        Initializes the custom http handler\n        Parameters:\n            url (str): The URL that the logs will be sent to\n            token (str): The Authorization token being used\n            silent (bool): If False the http response and logs will be sent\n                           to STDOUT for debug\n        '''\n        self.url = url\n        self.token = token\n        self.silent = silent\n        # sets up a session with the server\n        self.MAX_POOLSIZE = 100\n        self.session = session = requests.Session()\n        session.headers.update({\n            'Content-Type': 'application/json',\n            'Authorization': 'Bearer %s' % (self.token)\n        })\n        self.session.mount('https://', HTTPAdapter(\n            max_retries=Retry(\n                total=5,\n                backoff_factor=0.5,\n                status_forcelist=[403, 500]\n            ),\n            pool_connections=self.MAX_POOLSIZE,\n            pool_maxsize=self.MAX_POOLSIZE\n        ))\n        super().__init__()\n    def emit(self, record):\n        '''\n        This function gets called when a log event gets emitted. It recieves a\n        record, formats it and sends it to the url\n        Parameters:\n            record: a log record\n        '''\n        logEntry = self.format(record)\n        response = self.session.post(self.url, data=logEntry)\n        if not self.silent:\n            print(logEntry)\n            print(response.content)\n# create logger\nlog = logging.getLogger('')\nlog.setLevel(logging.INFO)\n# create formatter - this formats the log messages accordingly\nformatter = logging.Formatter(json.dumps({\n    'time': '%(asctime)s',\n    'pathname': '%(pathname)s',\n    'line': '%(lineno)d',\n    'logLevel': '%(levelname)s',\n    'message': '%(message)s'\n}))\n# create a custom http logger handler\nhttpHandler = CustomHttpHandler(\n    url='<YOUR_URL>',\n    token='<YOUR_TOKEN>',\n    silent=False\n)\nhttpHandler.setLevel(logging.INFO)\n# add formatter to custom http handler\nhttpHandler.setFormatter(formatter)\n# add handler to logger\nlog.addHandler(httpHandler)\nlog.info('Hello world!')"
  },
  {
    "url": "https://stackoverflow.com/questions/15667578/how-do-i-install-mezzanine-as-a-django-app",
    "body": "`\n    ...\n    \"mezzanine.core.request.CurrentRequestMiddleware\",\n\t\"mezzanine.core.middleware.RedirectFallbackMiddleware\",\n\t\"mezzanine.core.middleware.TemplateForDeviceMiddleware\",\n\t\"mezzanine.core.middleware.TemplateForHostMiddleware\",\n\t\"mezzanine.core.middleware.AdminLoginInterfaceSelectorMiddleware\",\n\t\"mezzanine.core.middleware.SitePermissionMiddleware\",\n\t# Uncomment the following if using any of the SSL settings:\n\t# \"mezzanine.core.middleware.SSLRedirectMiddleware\",\n\t\"mezzanine.pages.middleware.PageMiddleware\",\n    ...."
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    [cfati@CFATI-5510-0:E:\\Work\\Dev\\StackExchange\\StackOverflow\\q073458524]> sopr.bat\n    >    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n    >\n    >    [prompt]>\n    >    [prompt]> $_CTss = Get-ChildItem 'c:\\Install\\pc064\\Python\\Python' -Filter '_ctypes.pyd' -Recurse\n    >    [prompt]> foreach ($_CTs in $_CTss) {\n    >    >>     echo $_CTs.FullName\n    >    >>     Start-Process -Wait -FilePath 'c:\\Install\\pc064\\Depends\\DependencyWalkerPolitistTexan\\Version\\depends.exe' -ArgumentList '-c', '-ot:out.txt', $_CTs.FullName\n    >    >>     Get-Content 'out.txt' | Select-String 'FFI.*\\.DLL$'\n    >    >> }\n    >    C:\\Install\\pc064\\Python\\Python\\02.07.18\\DLLs\\_ctypes.pyd\n    >    C:\\Install\\pc064\\Python\\Python\\03.04.04\\DLLs\\_ctypes.pyd\n    >    C:\\Install\\pc064\\Python\\Python\\03.05.04\\DLLs\\_ctypes.pyd\n    >    C:\\Install\\pc064\\Python\\Python\\03.06.08\\DLLs\\_ctypes.pyd\n    >    C:\\Install\\pc064\\Python\\Python\\03.07.09\\DLLs\\_ctypes.pyd\n    >    C:\\Install\\pc064\\Python\\Python\\03.08\\DLLs\\_ctypes.pyd\n    >\n    >         [  6] c:\\install\\pc064\\python\\python\\03.08\\dlls\\LIBFFI-7.DLL\n    >    C:\\Install\\pc064\\Python\\Python\\03.09\\DLLs\\_ctypes.pyd\n    >         [  6] c:\\install\\pc064\\python\\python\\03.09\\dlls\\LIBFFI-7.DLL\n    >    C:\\Install\\pc064\\Python\\Python\\03.10\\DLLs\\_ctypes.pyd\n    >         [  6] c:\\install\\pc064\\python\\python\\03.10\\dlls\\LIBFFI-7.DLL\n    >    C:\\Install\\pc064\\Python\\Python\\03.11\\DLLs\\_ctypes.pyd\n    >         [  6] c:\\install\\pc064\\python\\python\\03.11\\dlls\\LIBFFI-8.DLL\n    >"
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    [cristian.fati@cfati-16i2019-0:~/Work/Dev/StackExchange/StackOverflow/q073458524]> ~/sopr.sh\n    >    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n    >\n    >    Running OSX\n    >    [064bit prompt]>\n    >    [064bit prompt]> uname -a\n    >    Darwin cfati-16i2019-0 22.5.0 Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:22 PDT 2023; root:xnu-8796.121.3~7/RELEASE_X86_64 x86_64\n    >    [064bit prompt]>\n    >    [064bit prompt]> for g in 7 8 9 10 11; do _CTS=$(python3.${g} -c \"import ctypes;print(_ctypes.__file__)\"); echo ${_CTS}; otool -L ${_CTS} | grep ffi; done\n    >    /usr/local/cellar/python@3.7/3.7.16/frameworks/python.framework/versions/3.7/lib/python3.7/lib-dynload/_ctypes.cpython-37m-darwin.so\n    >    /usr/local/cellar/python@3.8/3.8.17_1/frameworks/python.framework/versions/3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-darwin.so\n    >    \t/usr/lib/libffi.dylib (compatibility version 1.0.0, current version 30.0.0)\n    >    /usr/local/cellar/python@3.9/3.9.17_1/frameworks/python.framework/versions/3.9/lib/python3.9/lib-dynload/_ctypes.cpython-39-darwin.so\n    >    \t/usr/lib/libffi.dylib (compatibility version 1.0.0, current version 30.0.0)\n    >    /usr/local/cellar/python@3.10/3.10.12_1/frameworks/python.framework/versions/3.10/lib/python3.10/lib-dynload/_ctypes.cpython-310-darwin.so\n    >    \t/usr/lib/libffi.dylib (compatibility version 1.0.0, current version 30.0.0)\n    >    /usr/local/cellar/python@3.11/3.11.4_1/frameworks/python.framework/versions/3.11/lib/python3.11/lib-dynload/_ctypes.cpython-311-darwin.so\n    >    \t/usr/lib/libffi.dylib (compatibility version 1.0.0, current version 30.0.0)\n    >"
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    (qaic-env) [cfati@cfati-5510-0:/mnt/e/Work/Dev/StackExchange/StackOverflow/q073458524]> ~/sopr.sh\n    >    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n    >\n    >    [064bit prompt]>\n    >    [064bit prompt]> uname -a\n    >    Linux cfati-5510-0 6.2.0-37-generic #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n    >    [064bit prompt]>\n    >    [064bit prompt]> for g in 5 6 7 8 9 10 11 12; do _CTS=$(python3.${g} -c \"import _ctypes;print(_ctypes.__file__)\"); echo ${_CTS}; ldd ${_CTS} | grep ffi; done\n    >    /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so\n    >        libffi.so.7 => /lib/x86_64-linux-gnu/libffi.so.7 (0x00007ff1fb7d1000)\n    >    /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so\n    >        libffi.so.7 => /lib/x86_64-linux-gnu/libffi.so.7 (0x00007f0bf4ec5000)\n    >    /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so\n    >        libffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8 (0x00007fc2a3ae6000)\n    >    /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so\n    >        libffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8 (0x00007f32b6176000)\n    >    /usr/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so\n    >        libffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8 (0x00007f40e590a000)\n    >    /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so\n    >        libffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8 (0x00007f20bcbc7000)\n    >    /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so\n    >        libffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8 (0x00007f01a5555000)\n    >    /usr/lib/python3.12/lib-dynload/_ctypes.cpython-312-x86_64-linux-gnu.so\n    >        libffi.so.8 => /lib/x86_64-linux-gnu/libffi.so.8 (0x00007f4a95e2d000)\n    >"
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    [root@cfati-5510-0:/work/q073458524]> ~/sopr.sh\n    >    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n    >\n    >    [064bit prompt]>\n    >    [064bit prompt]> cat /etc/os-release | grep PRODUCT\n    >    REDHAT_SUPPORT_PRODUCT=\"centos\"\n    >    REDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n    >    [064bit prompt]>\n    >    [064bit prompt]> for g in 2.7 3.6; do _CTS=$(python${g} -c \"import _ctypes;print(_ctypes.__file__)\"); echo ${_CTS}; ldd ${_CTS} | grep ffi; done\n    >    /usr/lib64/python2.7/lib-dynload/_ctypes.so\n    >        libffi.so.6 => /lib64/libffi.so.6 (0x00007f6529d71000)\n    >    /usr/lib64/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so\n    >        libffi.so.6 => /lib64/libffi.so.6 (0x00007f1085acc000)\n    >"
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    (base) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackExchange\\StackOverflow\\q073458524]> :: ------- Anaconda Prompt -------\n>    (base) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackExchange\\StackOverflow\\q073458524]> cd /d f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\n>\n>    (base) [cfati@CFATI-5510-0:f:\\Install\\pc064\\Anaconda\\Anaconda\\Version]> sopr.bat\n>    ### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n>\n>    [prompt]> :: Reactivate base to have active environment in prompt\n>    [prompt]> conda deactivate & conda activate base\n>\n>    (base) [prompt]> :: ------- Anaconda Prompt (still) -------\n>    (base) [prompt]> :: ------- ${ANACONDA_INSTALL_PATH} is a placeholder for f:\\Install\\pc064\\Anaconda\\Anaconda\\Version -------\n>    (base) [prompt]> conda env list\n>    # conda environments:\n>    #\n>                             F:\\Install\\pc032\\Intel\\OneAPI\\Version\\intelpython\\python3.7\n>                             F:\\Install\\pc032\\Intel\\OneAPI\\Version\\intelpython\\python3.7\\envs\\2021.1.1\n>    base                  *  ${ANACONDA_INSTALL_PATH}\n>    py_pc032_030602_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc032_030602_00\n>    py_pc064_030610_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030610_00\n>    py_pc064_030704_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030704_00\n>    py_pc064_030716_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030716_00\n>    py_pc064_030800_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030800_00\n>    py_pc064_030808_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030808_00\n>    py_pc064_030817_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030817_00\n>    py_pc064_030900_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030900_00\n>    py_pc064_030917_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030917_00\n>    py_pc064_030917_01       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030917_01\n>    py_pc064_031000_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031000_00\n>    py_pc064_031006_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031006_00\n>    py_pc064_031012_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_00\n>    py_pc064_031012_01       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_01\n>    py_pc064_031104_00       ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031104_00\n>\n>\n>    (base) [prompt]>\n>    (base) [prompt]> :: Search environments for _ctypes.pyd\n>    (base) [prompt]> dir /B /S \"envs\\*_ctypes.pyd\"\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc032_030602_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030610_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030610_00\\DLLs\\instrumented\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030704_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030704_00\\DLLs\\instrumented\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030716_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030800_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030800_00\\DLLs\\instrumented\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030808_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030817_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030900_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030917_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030917_01\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031000_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031006_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_00\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_01\\DLLs\\_ctypes.pyd\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031104_00\\DLLs\\_ctypes.pyd\n>\n>    (base) [prompt]>\n>    (base) [prompt]> :: Search environments for the FFI dll\n>    (base) [prompt]> dir /B /S \"envs\\*ffi*.dll\"\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030800_00\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030808_00\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030817_00\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030817_00\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030817_00\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030817_00\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030900_00\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030917_00\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_030917_01\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031000_00\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031000_00\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031000_00\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031006_00\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031006_00\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031006_00\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_00\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_00\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_00\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_01\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_01\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031012_01\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031104_00\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031104_00\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\envs\\py_pc064_031104_00\\Library\\bin\\ffi.dll\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    (base) [prompt]> :: ------- Anaconda Prompt (still) -------\n>    (base) [prompt]> :: ------- ${ANACONDA_INSTALL_PATH} is a placeholder for f:\\Install\\pc064\\Anaconda\\Anaconda\\Version -------\n>    (base) [prompt]> dir /B /S \"pkgs\\*ffi*.dll\"\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.2-hd77b12b_4\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.2-hd77b12b_4\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.2-hd77b12b_4\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.2-hd77b12b_6\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.2-hd77b12b_6\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.2-hd77b12b_6\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.4-hd77b12b_0\\Library\\bin\\ffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.4-hd77b12b_0\\Library\\bin\\ffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\libffi-3.4.4-hd77b12b_0\\Library\\bin\\ffi.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.8.0-hff0d562_2\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.8.12-h6244533_0\\DLLs\\libffi-8.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.8.13-h6244533_0\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.8.17-h1aa4202_0\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.8.5-h5fd99cc_1\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.8.8-hdbf39b2_5\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.9.0-h6244533_2\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.9.12-h6244533_0\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.9.16-h6244533_2\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.9.17-h1aa4202_0\\DLLs\\libffi-7.dll\n>    ${ANACONDA_INSTALL_PATH}\\pkgs\\python-3.9.17-h6244533_0\\DLLs\\libffi-7.dll\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    (base) [prompt]> :: ------- Anaconda Prompt (still) -------\n    >    (base) [prompt]> :: --- PATH in active (base) environment ---\n    >    (base) [prompt]> echo off & (for %g in (\"%PATH:;=\" \"%\") do (echo %~g)) & echo on\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\Library\\mingw-w64\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\Library\\usr\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\Library\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\Scripts\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\condabin\n    >    C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\n    >    C:\\WINDOWS\\System32\n    >    C:\\WINDOWS\n    >    C:\\WINDOWS\\System32\\Wbem\n    >    C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin\n    >    C:\\Program Files\\dotnet\n    >    e:\\Work\\Dev\\Utils\\current\\Win\n    >    e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\n    >    C:\\Users\\cfati\\.dotnet\\tools\n    >    .\n    >\n    >    (base) [prompt]>\n    >    (base) [prompt]> :: --- Activate Python 3.10.12 environment ---\n    >    (base) [prompt]> conda activate py_pc064_031012_00\n    >\n    >    (py_pc064_031012_00) [prompt]> :: --- PATH in active (py_pc064_031012_00) environment ---\n    >    (py_pc064_031012_00) [prompt]> echo off & (for %g in (\"%PATH:;=\" \"%\") do (echo %~g)) & echo on\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\envs\\py_pc064_031012_00\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\envs\\py_pc064_031012_00\\Library\\mingw-w64\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\envs\\py_pc064_031012_00\\Library\\usr\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\envs\\py_pc064_031012_00\\Library\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\envs\\py_pc064_031012_00\\Scripts\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\envs\\py_pc064_031012_00\\bin\n    >    f:\\Install\\pc064\\Anaconda\\Anaconda\\Version\\condabin\n    >    C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\n    >    C:\\WINDOWS\\System32\n    >    C:\\WINDOWS\n    >    C:\\WINDOWS\\System32\\Wbem\n    >    C:\\Install\\pc064\\Docker\\Docker\\Version\\Docker\\resources\\bin\n    >    C:\\Program Files\\dotnet\n    >    e:\\Work\\Dev\\Utils\\current\\Win\n    >    e:\\Work\\Dev\\VEnvs\\py_pc064_03.10_test0\\Scripts\n    >    C:\\Users\\cfati\\.dotnet\\tools\n    >    .\n    >"
  },
  {
    "url": "https://stackoverflow.com/questions/73458524/importerror-dll-load-failed-while-importing-ctypes-the-specified-module-coul",
    "body": ">    (py_pc064_031012_00) [prompt]> :: ------- Anaconda Prompt (still) -------\n>    (py_pc064_031012_00) [prompt]> \"f:\\Install\\pc064\\LucasG\\DependencyWalkerPolitistTexan\\Version\\DependenciesGui.exe\" \"envs\\%CONDA_DEFAULT_ENV%\\DLLs\\_ctypes.pyd\"\n>\n>    (py_pc064_031012_00) [prompt]> conda deactivate & conda activate py_pc064_030817_00\n>\n>    (py_pc064_030817_00) [prompt]> \"f:\\Install\\pc064\\LucasG\\DependencyWalkerPolitistTexan\\Version\\DependenciesGui.exe\" \"envs\\%CONDA_DEFAULT_ENV%\\DLLs\\_ctypes.pyd\"\n>"
  },
  {
    "url": "https://stackoverflow.com/questions/40447290/python-unittest-and-multithreading",
    "body": "import unittest\nimport threading\nfrom concurrent import futures\nclass catch_threading_exception:\n    \"\"\"\n    https://docs.python.org/3/library/test.html#test.support.catch_threading_exception\n    Context manager catching threading.Thread exception using\n    threading.excepthook.\n    Attributes set when an exception is catched:\n    * exc_type\n    * exc_value\n    * exc_traceback\n    * thread\n    See threading.excepthook() documentation for these attributes.\n    These attributes are deleted at the context manager exit.\n    Usage:\n        with support.catch_threading_exception() as cm:\n            # code spawning a thread which raises an exception\n            ...\n            # check the thread exception, use cm attributes:\n            # exc_type, exc_value, exc_traceback, thread\n            ...\n        # exc_type, exc_value, exc_traceback, thread attributes of cm no longer\n        # exists at this point\n        # (to avoid reference cycles)\n    \"\"\"\n    def __init__(self):\n        self.exc_type = None\n        self.exc_value = None\n        self.exc_traceback = None\n        self.thread = None\n        self._old_hook = None\n    def _hook(self, args):\n        self.exc_type = args.exc_type\n        self.exc_value = args.exc_value\n        self.exc_traceback = args.exc_traceback\n        self.thread = args.thread\n    def __enter__(self):\n        self._old_hook = threading.excepthook\n        threading.excepthook = self._hook\n        return self\n    def __exit__(self, *exc_info):\n        threading.excepthook = self._old_hook\n        del self.exc_type\n        del self.exc_value\n        del self.exc_traceback\n        del self.thread\nclass MyTests(unittest.TestCase):\n    def test_tpe(self):\n        with futures.ThreadPoolExecutor() as pool:\n            pool.submit(self.fail).result()\n    def test_t_excepthook(self):\n        with catch_threading_exception() as cm:\n            t = threading.Thread(target=self.fail)\n            t.start()\n            t.join()\n            if cm.exc_value is not None:\n                raise cm.exc_value\nif __name__ == '__main__':\n    unittest.main()"
  },
  {
    "url": "https://stackoverflow.com/questions/35851782/why-does-handling-multiple-exceptions-require-a-tuple-but-not-a-list",
    "body": "case PyCmp_EXC_MATCH:\n    if (PyTuple_Check(w)) {\n        Py_ssize_t i, length;\n        length = PyTuple_Size(w);\n        for (i = 0; i < length; i += 1) {\n            PyObject *exc = PyTuple_GET_ITEM(w, i);\n            if (!PyExceptionClass_Check(exc)) {\n                _PyErr_SetString(tstate, PyExc_TypeError,\n                                 CANNOT_CATCH_MSG);\n                return NULL;\n            }\n        }\n    }\n    else {\n        if (!PyExceptionClass_Check(w)) {\n            _PyErr_SetString(tstate, PyExc_TypeError,\n                             CANNOT_CATCH_MSG);\n            return NULL;\n        }\n    }\n    res = PyErr_GivenExceptionMatches(v, w);\n    break;"
  },
  {
    "url": "https://stackoverflow.com/questions/74817656/construct-string-from-its-permutation-with-least-jumps-back",
    "body": "import itertools\nimport random\nimport numpy as np\nimport timeit\nimport matplotlib.pyplot as plt\nW=10 # Size of a word\n# Generate a random word\ndef randomWord():\n    return ''.join(np.random.choice(list(\"abcdef\"),W))\n# And a random shuffling of it\ndef shuffle(s):\n    return ''.join(random.sample(s,len(s)))\n# Quick'n'dirty implementation of the function you are using\n# It would have been better, btw, if you had provided them, as\n# needed to create a minimal reproducible example\ndef findOccurrences(s, c):\n    return [pos for pos, char in enumerate(s) if char == c]\ndef stringPop(s, i):\n    return s[:i]+s[i+1:]\n# Your function\ndef solvenaive(s1, s2, curr_ind):\n    if len(s1) == 0:\n        return 0\n    first_s1 = s1[0]\n\n    vorkommen = findOccurrences(s2, first_s1)\n    results = []\n    for i in vorkommen:\n        new_s1 = s1[1:]\n        new_s2 = stringPop(s2, i)\n        res = solvenaive(new_s1, new_s2, i)\n\n        if curr_ind > i:\n            results.append(res+1)\n        else:\n            results.append(res)\n\n    return min(results)\n# Mine. Almost the same.\n# But I add 2 parameters\n# bestScore which is what we have seen best\n# and sofar, a partial score\ndef solvecut(s1, s2, curr_ind, bestScore, sofar=0):\n    if len(s1)==0: # If we reach a \"leaf\" of recursion, then\n                   # return the score. (sofar is a real score in that case, not a partial)\n        return sofar\n    # Otherwise, we will recurse... unless it is a lost cause\n    if sofar>=bestScore: # No need to try better, we are already over our best\n        return 1e9\n    first_s1 = s1[0]\n\n    vorkommen = findOccurrences(s2, first_s1)\n    for i in vorkommen:\n        new_s1 = s1[1:] # I let that one here in the loop because I don't want to optimize anything other that my \"cuts\"\n                        # so that improvement are clearly imputable to those cuts\n                        # but, obviously that could have been done outside the loop\n        new_s2 = stringPop(s2, i)\n        if curr_ind>i:\n            res=solvecut(new_s1, new_s2, i, bestScore, sofar+1)\n        else:\n            res=solvecut(new_s1, new_s2, i, bestScore, sofar)\n        if res<bestScore:\n            bestScore=res\n    return bestScore # Sometimes we'll return that just because we did nothing best, but it doesn't matter\n# Test if result are the same on some examples\nfor i in range(20):\n    w=randomWord()\n    s=shuffle(w)\n    sc1=solvenaive(w,s,0)\n    sc2=solvecut(w, s, 0, 1e9)\n    print(w, s, sc1, sc2)\n# Timeit\n# Note that timing depends a lot of the word (letter repetition, etc.). So it is important to do them with the same words\n# Especially since this is not very fast, so we need to timit with number=1\nW=17\nw1=randomWord()\ns1=shuffle(w1)\nprint(timeit.timeit(lambda: solvenaive(w1, s1, 0), number=1))\nprint(timeit.timeit(lambda: solvecut(w1, s1, 0, 1e9), number=1))"
  },
  {
    "url": "https://stackoverflow.com/questions/74817656/construct-string-from-its-permutation-with-least-jumps-back",
    "body": "cbaaecffae aaebcffcae 2 2\needeafdffb ffdbeefaed 3 3\naedefdceba adeefcabde 4 4\naaafccaafb aafacaafcb 2 2\neaffdfafef efdffefaaf 2 2\ndedbfffbce bcdffbedfe 3 3\nfedfcbaeed bedcfaefde 4 4\nffeebfcfab feaebcffbf 3 3\nfcdaddbfbf ffddacbfbd 3 3\ndeffcdeaea eeafdadfce 4 4\ncafeeebcdb beaedfbecc 4 4\nbeefdaeabd edaefbbdea 3 3\nddccbdbdae cdbdbcdead 3 3\nbcbababdef decafbbbba 4 4\ndfaeceefea edfcefaaee 2 2\nabcdcbbdbf fbbadccbdb 4 4\nacbedbaefc cbeacaebdf 3 3\naaffacfbde cfaaeafbfd 3 3\nedceddebdc dedcecbded 2 2\necafabdfea fafaeaedbc 3 3\n4.060046362923458\n0.05187216284684837"
  },
  {
    "url": "https://stackoverflow.com/questions/74817656/construct-string-from-its-permutation-with-least-jumps-back",
    "body": "1 cut=6.040092557668686e-06\n2 cut=8.61193984746933e-06\n3 cut=1.5990110114216805e-05\n4 cut=1.5911879017949104e-05\n5 cut=2.897903323173523e-05\n6 cut=3.5561854019761086e-05\n7 cut=5.8827921748161316e-05\n8 cut=0.00018196692690253258\n9 cut=0.0001191610936075449\n10 cut=0.0002572301309555769\n11 cut=0.0008078841492533684\n12 cut=0.0009584939107298851\n13 cut=0.008062224835157394\n14 cut=0.004587493138387799\n15 cut=0.08773919194936752\n16 cut=0.02209200686775148\n17 cut=0.045466484036296606\n18 cut=0.11209587100893259\n19 cut=0.40787983988411725\n20 cut=2.2789966259151697\n21 cut=1.8927371250465512\n22 cut=1.097903796005994"
  },
  {
    "url": "https://stackoverflow.com/questions/74817656/construct-string-from-its-permutation-with-least-jumps-back",
    "body": "def lessNaive(s1, s2, curr_ind, bestScore, sofar):\n    if len(s1)==0:\n        return sofar\n    if sofar>=bestScore or (curr_ind>0 and sofar+1>=bestScore): # No need to try better, we are already over our best\n        return 1e9\n    first_s1 = s1[0]\n    new_s1 = s1[1:]\n    for i in range(len(s2)):\n        if s2[i]!=first_s1: continue\n        new_s2 = stringPop(s2, i)\n        if curr_ind>i:\n            res=lessNaive(new_s1, new_s2, i, bestScore, sofar+1)\n        else:\n            res=lessNaive(new_s1, new_s2, i, bestScore, sofar)\n        if res<bestScore:\n            bestScore=res\n    return bestScore # Sometimes we'll return that just because we did nothing best, but it doesn't matter"
  },
  {
    "url": "https://stackoverflow.com/questions/60517190/are-poetry-lock-files-os-independent",
    "body": "!toml\nwatchdog = [\n    {file = \"watchdog-2.1.6-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:9693f35162dc6208d10b10ddf0458cc09ad70c30ba689d9206e02cd836ce28a3\"},\n    {file = \"watchdog-2.1.6-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:aba5c812f8ee8a3ff3be51887ca2d55fb8e268439ed44110d3846e4229eb0e8b\"},\n    {file = \"watchdog-2.1.6-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:4ae38bf8ba6f39d5b83f78661273216e7db5b00f08be7592062cb1fc8b8ba542\"},\n    {file = \"watchdog-2.1.6-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:ad6f1796e37db2223d2a3f302f586f74c72c630b48a9872c1e7ae8e92e0ab669\"},\n    {file = \"watchdog-2.1.6-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:922a69fa533cb0c793b483becaaa0845f655151e7256ec73630a1b2e9ebcb660\"},\n    {file = \"watchdog-2.1.6-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:b2fcf9402fde2672545b139694284dc3b665fd1be660d73eca6805197ef776a3\"},\n    {file = \"watchdog-2.1.6-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:3386b367e950a11b0568062b70cc026c6f645428a698d33d39e013aaeda4cc04\"},\n    {file = \"watchdog-2.1.6-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:8f1c00aa35f504197561060ca4c21d3cc079ba29cf6dd2fe61024c70160c990b\"},\n    {file = \"watchdog-2.1.6-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:b52b88021b9541a60531142b0a451baca08d28b74a723d0c99b13c8c8d48d604\"},\n    {file = \"watchdog-2.1.6-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:8047da932432aa32c515ec1447ea79ce578d0559362ca3605f8e9568f844e3c6\"},\n    {file = \"watchdog-2.1.6-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:e92c2d33858c8f560671b448205a268096e17870dcf60a9bb3ac7bfbafb7f5f9\"},\n    {file = \"watchdog-2.1.6-pp37-pypy37_pp73-macosx_10_9_x86_64.whl\", hash = \"sha256:b7d336912853d7b77f9b2c24eeed6a5065d0a0cc0d3b6a5a45ad6d1d05fb8cd8\"},\n    {file = \"watchdog-2.1.6-py3-none-manylinux2014_aarch64.whl\", hash = \"sha256:cca7741c0fcc765568350cb139e92b7f9f3c9a08c4f32591d18ab0a6ac9e71b6\"},\n    {file = \"watchdog-2.1.6-py3-none-manylinux2014_armv7l.whl\", hash = \"sha256:25fb5240b195d17de949588628fdf93032ebf163524ef08933db0ea1f99bd685\"},\n    {file = \"watchdog-2.1.6-py3-none-manylinux2014_i686.whl\", hash = \"sha256:be9be735f827820a06340dff2ddea1fb7234561fa5e6300a62fe7f54d40546a0\"},\n    {file = \"watchdog-2.1.6-py3-none-manylinux2014_ppc64.whl\", hash = \"sha256:d0d19fb2441947b58fbf91336638c2b9f4cc98e05e1045404d7a4cb7cddc7a65\"},\n    {file = \"watchdog-2.1.6-py3-none-manylinux2014_ppc64le.whl\", hash = \"sha256:3becdb380d8916c873ad512f1701f8a92ce79ec6978ffde92919fd18d41da7fb\"},\n    {file = \"watchdog-2.1.6-py3-none-manylinux2014_s390x.whl\", hash = \"sha256:ae67501c95606072aafa865b6ed47343ac6484472a2f95490ba151f6347acfc2\"},\n    {file = \"watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl\", hash = \"sha256:e0f30db709c939cabf64a6dc5babb276e6d823fd84464ab916f9b9ba5623ca15\"},\n    {file = \"watchdog-2.1.6-py3-none-win32.whl\", hash = \"sha256:e02794ac791662a5eafc6ffeaf9bcc149035a0e48eb0a9d40a8feb4622605a3d\"},\n    {file = \"watchdog-2.1.6-py3-none-win_amd64.whl\", hash = \"sha256:bd9ba4f332cf57b2c1f698be0728c020399ef3040577cde2939f2e045b39c1e5\"},\n    {file = \"watchdog-2.1.6-py3-none-win_ia64.whl\", hash = \"sha256:a0f1c7edf116a12f7245be06120b1852275f9506a7d90227648b250755a03923\"},\n    {file = \"watchdog-2.1.6.tar.gz\", hash = \"sha256:a36e75df6c767cbf46f61a91c70b3ba71811dfa0aca4a324d9407a06a8b7a2e7\"},\n]\nweasyprint = [\n    {file = \"weasyprint-54.1-py3-none-any.whl\", hash = \"sha256:27c078ded67a43c9a05c349eda01ea327805d48e5c3ca3b704f57eb82bd78592\"},\n    {file = \"weasyprint-54.1.tar.gz\", hash = \"sha256:fa57db862e06bd01c5e7d82dad399b3b9952a39827023c17bee9b1c061ff1bbd\"},\n]"
  },
  {
    "url": "https://stackoverflow.com/questions/53237266/how-can-i-minimize-maximize-windows-in-macos-with-the-cocoa-api-from-a-python-sc",
    "body": "from AppKit import NSApplication, NSApp, NSWorkspace\nfrom Quartz import kCGWindowListOptionOnScreenOnly, kCGNullWindowID, CGWindowListCopyWindowInfo\nworkspace = NSWorkspace.sharedWorkspace()\nactiveApps = workspace.runningApplications()\nfor app in activeApps:\n    if app.isActive():\n        options = kCGWindowListOptionOnScreenOnly\n        windowList = CGWindowListCopyWindowInfo(options,\n                                                kCGNullWindowID)\n        for window in windowList:\n            if window['kCGWindowOwnerName'] == app.localizedName():\n                print(window.getKeys_)\n                break\n        break"
  },
  {
    "url": "https://stackoverflow.com/questions/54855780/how-to-create-ner-pipeline-with-multiple-models-in-spacy",
    "body": "import spacy # tested with v2.2.3\nfrom spacy.pipeline import EntityRecognizer\ntext = \"Jane lives in Boston. Jan lives in Bremen.\"\n# load the English and German models\nnlp_en = spacy.load('en_core_web_sm')  # NER tags PERSON, GPE, ...\nnlp_de = spacy.load('de_core_news_sm') # NER tags PER, LOC, ...\n# the Vocab objects are not the same\nassert nlp_en.vocab != nlp_de.vocab\n# but the vectors are identical (because neither model has vectors)\nassert nlp_en.vocab.vectors.to_bytes() == nlp_de.vocab.vectors.to_bytes()\n# original English output\ndoc1 = nlp_en(text)\nprint([(ent.text, ent.label_) for ent in doc1.ents])\n# [('Jane', 'PERSON'), ('Boston', 'GPE'), ('Bremen', 'GPE')]\n# original German output (the German model makes weird predictions for English text)\ndoc2 = nlp_de(text)\nprint([(ent.text, ent.label_) for ent in doc2.ents])\n# [('Jane lives', 'PER'), ('Boston', 'LOC'), ('Jan lives', 'PER'), ('Bremen', 'LOC')]\n# initialize a new NER component with the vocab from the English pipeline\nner_de = EntityRecognizer(nlp_en.vocab)\n# reload the NER component from the German model by serializing\n# without the vocab and deserializing using the new NER component\nner_de.from_bytes(nlp_de.get_pipe(\"ner\").to_bytes(exclude=[\"vocab\"]))\n# add the German NER component to the end of the English pipeline\nnlp_en.add_pipe(ner_de, name=\"ner_de\")\n# check that they have the same vocab\nassert nlp_en.vocab == ner_de.vocab\n# combined output (English NER runs first, German second)\ndoc3 = nlp_en(text)\nprint([(ent.text, ent.label_) for ent in doc3.ents])\n# [('Jane', 'PERSON'), ('Boston', 'GPE'), ('Jan lives', 'PER'), ('Bremen', 'GPE')]"
  },
  {
    "url": "https://stackoverflow.com/questions/47125723/keras-lstm-for-text-generation-keeps-repeating-a-line-or-a-sequence",
    "body": "for epoch in range(1, 60):\n    print('epoch', epoch)\n    # Fit the model for 1 epoch on the available training data\n    model.fit(x, y,\n              batch_size=128,\n              epochs=1)\n    # Select a text seed at random\n    start_index = random.randint(0, len(text) - maxlen - 1)\n    generated_text = text[start_index: start_index + maxlen]\n    print('--- Generating with seed: \"' + generated_text + '\"')\n    for temperature in [0.2, 0.5, 1.0, 1.2]:\n        print('------ temperature:', temperature)\n        sys.stdout.write(generated_text)\n        # We generate 400 characters\n        for i in range(400):\n            sampled = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(generated_text):\n                sampled[0, t, char_indices[char]] = 1.\n            preds = model.predict(sampled, verbose=0)[0]\n            next_index = sample(preds, temperature)\n            next_char = chars[next_index]\n            generated_text += next_char\n            generated_text = generated_text[1:]\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()"
  },
  {
    "url": "https://stackoverflow.com/questions/58657285/how-does-loggings-extra-argument-work",
    "body": "import logging\nimport sys\nclass NewLogger(logging.Logger):\n    # override the makeRecord method\n    def makeRecord(self, *args, **kwargs):\n        rv = super(NewLogger, self).makeRecord(*args, **kwargs)\n        rv.__dict__[\"City\"] = rv.__dict__.get(\"City\", \"Khazad-dum\")\n        return rv\nlog = NewLogger(\"foobar\")\nlog.propagate = False\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(\"INFO\")\nformatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s | %(City)s')\nhandler.setFormatter(formatter)\nlog.addHandler(handler)\nlog.error(\"you shall not pass\")  # should log the default value\nlog.error(\"fly you foos!\", extra={\"City\": \"Mordor\"})  # should log \"Mordor\""
  },
  {
    "url": "https://stackoverflow.com/questions/58657285/how-does-loggings-extra-argument-work",
    "body": "import logging\nimport sys\nlog = logging.getLogger(\"foobar\")\nlog.propagate = False\nhandler = logging.StreamHandler(sys.stdout)\nhandler.setLevel(\"INFO\")\nformatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s | %(City)s')\nhandler.setFormatter(formatter)\nlog.addHandler(handler)\nold_factory = logging.getLogRecordFactory()\ndef record_factory(*args, **kwargs):\n    record = old_factory(*args, **kwargs)  # get the unmodified record\n    record.City = \"Khazad-dum\"\n    return record\nlogging.setLogRecordFactory(record_factory)\nlog.error(\"you shall not pass\")\nlog.error(\"fly you foos!\", extra={\"City\": \"Mordor\"})"
  },
  {
    "url": "https://stackoverflow.com/questions/58657285/how-does-loggings-extra-argument-work",
    "body": "2023-02-07 14:11:19,722 | foobar | ERROR | you shall not pass | Khazad-dum\nTraceback (most recent call last):\n  File \"main.py\", line 82, in <module>\n    log.error(\"fly you foos!\", extra={\"City\": \"Mordor\"})\n  File \"/home/vvvvv/.pyenv/versions/3.8.12/lib/python3.8/logging/__init__.py\", line 1475, in error\n    self._log(ERROR, msg, args, **kwargs)\n  File \"/home/vvvvv/.pyenv/versions/3.8.12/lib/python3.8/logging/__init__.py\", line 1587, in _log\n    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n  File \"/home/vvvvv/.pyenv/versions/3.8.12/lib/python3.8/logging/__init__.py\", line 1561, in makeRecord\n    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\nKeyError: \"Attempt to overwrite 'City' in LogRecord\""
  },
  {
    "url": "https://stackoverflow.com/questions/7492855/getting-an-embedded-python-runtime-to-use-the-current-active-virtualenv",
    "body": "std::vector<std::string> paths;\nstd::string pathEnv = getenv(\"PATH\");\nboost::split(paths, pathEnv, boost::is_any_of(\";:\"));\nfor (std::string path : paths)\n{\n  boost::filesystem::path pythonPath = boost::filesystem::path(path) / \"python\";\n  std::cout << pythonPath << std::endl;\n  if (boost::filesystem::exists(pythonPath))\n  {\n    pythonProgramName_ = pythonPath.string(); // remember path, because Py_SetProgramName doesn't save it anywhere\n    Py_SetProgramName(&pythonProgramName_[0]);\n    break;\n  }\n}\nPy_Initialize();"
  },
  {
    "url": "https://stackoverflow.com/questions/31703037/how-can-i-change-modify-replace-text-in-a-pdf-using-python",
    "body": "#!chapter_007/src/snippet_012.py\nfrom borb.pdf import Document\nfrom borb.pdf import Page\nfrom borb.pdf import PageLayout, SingleColumnLayout\nfrom borb.pdf import Table, FixedColumnWidthTable\nfrom borb.pdf import Paragraph\nfrom borb.pdf import PDF\nfrom decimal import Decimal\ndef main():\n    # create empty Document\n    doc: Document = Document()\n    # add new Page\n    pge: Page = Page()\n    doc.add_page(pge)\n    # set PageLayout\n    lay: PageLayout = SingleColumnLayout(pge)\n    # add Table\n    tab: Table = FixedColumnWidthTable(number_of_columns=2, number_of_rows=3)\n    tab.add(Paragraph(\"Name:\", font=\"Helvetica-Bold\"))\n    tab.add(Paragraph(\"Schellekens\"))\n    tab.add(Paragraph(\"Firstname:\", font=\"Helvetica-Bold\"))\n    tab.add(Paragraph(\"Jots\"))\n    tab.add(Paragraph(\"Title:\", font=\"Helvetica-Bold\"))\n    tab.add(Paragraph(\"CEO borb\"))\n    tab.set_padding_on_all_cells(Decimal(5), Decimal(5), Decimal(5), Decimal(5))\n    lay.add(tab)\n    # store\n    with open(\"output.pdf\", 'wb') as pdf_file_handle:\n        PDF.dumps(pdf_file_handle, doc)\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/31703037/how-can-i-change-modify-replace-text-in-a-pdf-using-python",
    "body": "#!chapter_007/src/snippet_013.py\nfrom borb.pdf import Document\nfrom borb.pdf import PDF\nfrom borb.toolkit import SimpleFindReplace\nimport typing\ndef main():\n    # attempt to read a PDF\n    doc: typing.Optional[Document] = None\n    with open(\"output.pdf\", \"rb\") as pdf_file_handle:\n        doc = PDF.loads(pdf_file_handle)\n    # check whether we actually read a PDF\n    assert doc is not None\n    # find/replace\n    doc = SimpleFindReplace.sub(\"Jots\", \"Joris\", doc)\n    # store\n    with open(\"output2.pdf\", \"wb\") as pdf_file_handle:\n        PDF.dumps(pdf_file_handle, doc)\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "url": "https://stackoverflow.com/questions/15952733/sqlalchemy-logging-of-changes-with-date-and-user",
    "body": "from flask import has_request_context  # How to check if in a Flask session\nfrom sqlalchemy import inspect\nfrom sqlalchemy.orm import class_mapper\nfrom sqlalchemy.orm.attributes import get_history\nfrom sqlalchemy.event import listen\nfrom YOUR_SESSION_MANAGER import get_user  # This would be something in Pyramid\nfrom my_project import models  # Where your models are defined\ndef get_object_changes(obj):\n    \"\"\" Given a model instance, returns dict of pending\n    changes waiting for database flush/commit.\n    e.g. {\n        'some_field': {\n            'before': *SOME-VALUE*,\n            'after': *SOME-VALUE*\n        },\n        ...\n    }\n    \"\"\"\n    inspection = inspect(obj)\n    changes = {}\n    for attr in class_mapper(obj.__class__).column_attrs:\n        if getattr(inspection.attrs, attr.key).history.has_changes():\n            if get_history(obj, attr.key)[2]:\n                before = get_history(obj, attr.key)[2].pop()\n                after = getattr(obj, attr.key)\n                if before != after:\n                    if before or after:\n                        changes[attr.key] = {'before': before, 'after': after}\n    return changes\ndef my_model_change_listener(mapper, connection, target):\n    changes = get_object_changes(target)\n    changes.pop(\"modify_ts\", None)  # remove fields you don't want to track\n    user_id = None\n    if has_request_context():\n        # Call your function to get active user and extract id\n        user_id = getattr(get_user(), 'id', None)\n    if user_id is None:\n        # What do you want to do if user can't be determined\n        pass\n    # You now have the model instance (target), the user_id who is logged in,\n    # and a dictionary of changes.\n    # Either do somthing \"quick\" with it here or call an async task (e.g.\n    # Celery) to do something with the information that may take longer\n    # than you want the request to take.\n# Add the listener\nlisten(models.MyModel, 'after_update', my_model_change_listener)"
  },
  {
    "url": "https://stackoverflow.com/questions/52757556/using-line-profiler-with-multiprocessing",
    "body": "import multiprocessing as mp\nimport line_profiler\nclass Worker(mp.Process):\n    def run(self):\n        prof = line_profiler.LineProfiler()\n        # Wrap all functions that you want to be profiled in this process\n        # These can be global functions or any class methods\n        # Make sure to replace instance methods on a class level, not the bound methods self.run2\n        Worker.run2 = prof(Worker.run2)\n        ...\n        # run the main\n        self.run2()\n        # store stats in separate file for each process\n        prof.dump_stats('worker.lprof')\n    def run2(self):\n        # real run method renamed\n        ..."
  },
  {
    "url": "https://stackoverflow.com/questions/68624314/do-asynchronous-context-managers-need-to-protect-their-cleanup-code-from-cancell",
    "body": "async def release_db_connection(conn):\n    \"\"\"\n    Cancellation safe variant of `release_db_connection`\n    Internally protects against cancellation by delaying it until cleanup.\n    \"\"\"\n    # cleanup is run in separate task so that it\n    # cannot be cancelled from the outside.\n    shielded_release = asyncio.create_task(asyncio.sleep(1))\n    # Wait for cleanup completion – unlike `asyncio.shield`,\n    # delay any cancellation until we are done.\n    try:\n        await shielded_release\n    except asyncio.CancelledError:\n        await shielded_release\n        # propagate cancellation when we are done\n        raise\n    finally:\n        print(\"Released database connection.\")"
  },
  {
    "url": "https://stackoverflow.com/questions/48702706/why-does-sqlalchemy-not-update-relations-after-object-deletion",
    "body": "class HideDeletedRelationshipWithoutFlushMixin:\n    def __getattribute__(self, item):\n        value = super().__getattribute__(item)\n        if value is not None and hasattr(value, DEFAULT_STATE_ATTR):\n            state = instance_state(value)\n            if (\n                    state.deleted or                                               # After flush\n                    state.session is not None and state in state.session._deleted  # Before flush\n            ):\n                return None\n        return value\nclass HideDeletedRelationshipAfterFlushMixin:\n    def __getattribute__(self, item):\n        value = super().__getattribute__(item)\n        if value is not None and hasattr(value, DEFAULT_STATE_ATTR) and instance_state(value).deleted:\n            return None\n        return value"
  },
  {
    "url": "https://stackoverflow.com/questions/33983860/hide-chromedriver-console-in-python",
    "body": "from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService # Similar thing for firefox also!\nfrom subprocess import CREATE_NO_WINDOW # This flag will only be available in windows\n# Define your own service object with the `CREATE_NO_WINDOW ` flag\n# If chromedriver.exe is not in PATH, then use:\n# ChromeService('/path/to/chromedriver')\nchrome_service = ChromeService('chromedriver')\n# Use `chrome_service.creationflags` for selenium < 4.6\nchrome_service.creation_flags = CREATE_NO_WINDOW\ndriver = webdriver.Chrome(service=chrome_service)"
  },
  {
    "url": "https://stackoverflow.com/questions/70641660/how-do-you-get-and-use-a-refresh-token-for-the-dropbox-api-python-3-x",
    "body": "import base64\nimport requests\nimport json\nAPP_KEY = '<APP_KEY>'\nAPP_SECRET = '<APP_SECRET>'\nACCESS_CODE_GENERATED = '<ACCESS_CODE_GENERATED>'\nBASIC_AUTH = base64.b64encode(f'{APP_KEY}:{APP_SECRET}'.encode())\nheaders = {\n    'Authorization': f\"Basic {BASIC_AUTH}\",\n    'Content-Type': 'application/x-www-form-urlencoded',\n}\ndata = f'code={ACCESS_CODE_GENERATED}&grant_type=authorization_code'\nresponse = requests.post('https://api.dropboxapi.com/oauth2/token',\n                         data=data,\n                         auth=(APP_KEY, APP_SECRET))\nprint(json.dumps(json.loads(response.text), indent=2))"
  },
  {
    "url": "https://stackoverflow.com/questions/56734576/find-input-shape-from-onnx-file",
    "body": "import onnx\nmodel = onnx.load(r\"model.onnx\")\n# The model is represented as a protobuf structure and it can be accessed\n# using the standard python-for-protobuf methods\n# iterate through inputs of the graph\nfor input in model.graph.input:\n    print (input.name, end=\": \")\n    # get type of input tensor\n    tensor_type = input.type.tensor_type\n    # check if it has a shape:\n    if (tensor_type.HasField(\"shape\")):\n        # iterate through dimensions of the shape:\n        for d in tensor_type.shape.dim:\n            # the dimension may have a definite (integer) value or a symbolic identifier or neither:\n            if (d.HasField(\"dim_value\")):\n                print (d.dim_value, end=\", \")  # known dimension\n            elif (d.HasField(\"dim_param\")):\n                print (d.dim_param, end=\", \")  # unknown dimension with symbolic name\n            else:\n                print (\"?\", end=\", \")  # unknown dimension with no name\n    else:\n        print (\"unknown rank\", end=\"\")\n    print()"
  },
  {
    "url": "https://stackoverflow.com/questions/55324449/how-to-specify-a-minimum-or-maximum-float-value-with-argparse",
    "body": "def range_limited_float_type(arg):\n    \"\"\" Type function for argparse - a float within some predefined bounds \"\"\"\n    try:\n        f = float(arg)\n    except ValueError:\n        raise argparse.ArgumentTypeError(\"Must be a floating point number\")\n    if f < MIN_VAL or f > MAX_VAL:\n        raise argparse.ArgumentTypeError(\"Argument must be < \" + str(MAX_VAL) + \"and > \" + str(MIN_VAL))\n    return f\nparser.add_argument(\n    '-f',\n    '--float',\n    type=range_limited_float_type,\n    help='Your argument description'\n)"
  },
  {
    "url": "https://stackoverflow.com/questions/57368870/simply-using-parsec-in-python",
    "body": "from parsec import *\nspaces = regex(r'\\s*', re.MULTILINE)\nname = regex(r'[_a-zA-Z][_a-zA-Z0-9]*')\ntag_start = spaces >> string('<') >> name << string('>') << spaces\ntag_stop = spaces >> string('</') >> name << string('>') << spaces\n@generate\ndef header_kv():\n    key = yield spaces >> name << spaces\n    yield string(':')\n    value = yield spaces >> regex('[^\\n]+')\n    return {key: value}\n@generate\ndef header():\n    tag_name = yield tag_start\n    values = yield sepBy(header_kv, string('\\n'))\n    tag_name_end = yield tag_stop\n    assert tag_name == tag_name_end\n    return {\n        'type': 'tag',\n        'name': tag_name,\n        'values': values\n    }\n@generate\ndef body():\n    tag_name = yield tag_start\n    values = yield sepBy(sepBy1(regex(r'[^\\n<,]+'), string(',')), string('\\n'))\n    tag_name_end = yield tag_stop\n    assert tag_name == tag_name_end\n    return {\n        'type': 'tag',\n        'name': tag_name,\n        'values': values\n    }\nparser = header + body"
  },
  {
    "url": "https://stackoverflow.com/questions/69776414/pytzusagewarning-the-zone-attribute-is-specific-to-pytzs-interface-please-mig",
    "body": "\"\"\"Basic Flask Example from flask-apscheduler examples, file jobs.py\"\"\"\nfrom flask import Flask\nfrom flask_apscheduler import APScheduler\nclass Config:\n    \"\"\"App configuration.\"\"\"\n    JOBS = [\n        {\n            \"id\": \"job1\",\n            \"func\": \"jobs:job1\",\n            \"args\": (1, 2),\n            \"trigger\": \"interval\",\n            \"seconds\": 10,\n        }\n    ]\n    SCHEDULER_API_ENABLED = True\n    SCHEDULER_TIMEZONE = \"Europe/Berlin\"  # <========== add here\ndef job1(var_one, var_two):\n    print(str(var_one) + \" \" + str(var_two))\nif __name__ == \"__main__\":\n    app = Flask(__name__)\n    app.config.from_object(Config())\n    scheduler = APScheduler()\n    scheduler.init_app(app)\n    scheduler.start()\n    app.run()"
  },
  {
    "url": "https://stackoverflow.com/questions/60778279/fastapi-middleware-peeking-into-responses",
    "body": "from starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nimport json\nfrom .async_iterator_wrapper import async_iterator_wrapper as aiwrap\nclass some_middleware(BaseHTTPMiddleware):\n   async def dispatch(self, request:Request, call_next:RequestResponseEndpoint):\n      # --------------------------\n      # DO WHATEVER YOU TO DO HERE\n      #---------------------------\n\n      response = await call_next(request)\n      # Consuming FastAPI response and grabbing body here\n      resp_body = [section async for section in response.__dict__['body_iterator']]\n      # Repairing FastAPI response\n      response.__setattr__('body_iterator', aiwrap(resp_body)\n      # Formatting response body for logging\n      try:\n         resp_body = json.loads(resp_body[0].decode())\n      except:\n         resp_body = str(resp_body)"
  },
  {
    "url": "https://stackoverflow.com/questions/12007406/divide-dataframe-by-first-row",
    "body": "In [1]: df\nOut[1]:\n                   SPY      Google        Gold        Xom\nDate\n2008-12-31   90.239998  153.250580   86.519997  79.830002\n2009-01-02   92.959999  160.060059   86.230003  81.639999\n2009-01-05   92.849998  163.412491   84.480003  81.629997\n2009-01-06   93.470001  166.406265   85.129997  80.300003\n2009-01-07   90.669998  160.403763   82.750000  78.250000\n...                ...         ...         ...        ...\n2012-12-24  142.350006  353.425262  160.619995  86.919998\n2012-12-26  141.750000  353.111450  160.779999  87.070000\n2012-12-27  141.559998  351.826263  161.160004  86.860001\n2012-12-28  140.029999  348.697998  160.539993  85.099998\n2012-12-31  142.410004  352.369232  162.020004  86.550003\n[1007 rows x 4 columns]\nIn [2]: df.iloc[0]\nOut[2]:\nSPY        90.239998\nGoogle    153.250580\nGold       86.519997\nXom        79.830002\nName: 2008-12-31 00:00:00, dtype: float64\nIn [3]: df.div(df.iloc[0])\nOut[3]:\n                 SPY    Google      Gold       Xom\nDate\n2008-12-31  1.000000  1.000000  1.000000  1.000000\n2009-01-02  1.030142  1.044434  0.996648  1.022673\n2009-01-05  1.028923  1.066309  0.976422  1.022548\n2009-01-06  1.035793  1.085844  0.983934  1.005888\n2009-01-07  1.004765  1.046676  0.956426  0.980208\n...              ...       ...       ...       ...\n2012-12-24  1.577460  2.306192  1.856449  1.088814\n2012-12-26  1.570811  2.304144  1.858299  1.090693\n2012-12-27  1.568706  2.295758  1.862691  1.088062\n2012-12-28  1.551751  2.275345  1.855525  1.066015\n2012-12-31  1.578125  2.299301  1.872631  1.084179\n[1007 rows x 4 columns]"
  },
  {
    "url": "https://stackoverflow.com/questions/57596086/final-annotation-and-decorator-in-python3-8",
    "body": "$ cat demo.py\nfrom typing import final, Final\n# FOO is marked final, can't assign another value to it\nFOO: Final[int] = 42\nclass Foo:\n    @final\n    def spam(self) -> int:\n        \"\"\"A final method can't be overridden in a subclass\"\"\"\n        return 42\n@final\nclass Bar:\n    \"\"\"A final class can't be subclassed\"\"\"\n# Rule breaking section\nFOO = 81\nclass Spam(Foo, Bar):\n    def spam(self) -> int:\n        return 17\nif __name__ == '__main__':\n    print(\"FOO:\", FOO)\n    print(\"Spam().spam():\", Spam().spam())\n$ python3.8 demo.py   # Python will not throw errors here\nFOO: 81\nSpam().spam(): 17\n$ mypy demo.py        # only a type checker will\ndemo.py:17: error: Cannot assign to final name \"FOO\"\ndemo.py:19: error: Cannot inherit from final class \"Bar\"\ndemo.py:20: error: Cannot override final attribute \"spam\" (previously declared in base class \"Foo\")"
  },
  {
    "url": "https://stackoverflow.com/questions/56719138/how-can-i-save-a-librosa-spectrogram-plot-as-a-specific-sized-image",
    "body": "import librosa\nimport numpy\nimport skimage.io\ndef scale_minmax(X, min=0.0, max=1.0):\n    X_std = (X - X.min()) / (X.max() - X.min())\n    X_scaled = X_std * (max - min) + min\n    return X_scaled\ndef spectrogram_image(y, sr, out, hop_length, n_mels):\n    # use log-melspectrogram\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n                                            n_fft=hop_length*2, hop_length=hop_length)\n    mels = numpy.log(mels + 1e-9) # add small number to avoid log(0)\n    # min-max scale to fit inside 8-bit range\n    img = scale_minmax(mels, 0, 255).astype(numpy.uint8)\n    img = numpy.flip(img, axis=0) # put low frequencies at the bottom in image\n    img = 255-img # invert. make black==more energy\n    # save as PNG\n    skimage.io.imsave(out, img)\nif __name__ == '__main__':\n    # settings\n    hop_length = 512 # number of samples per time-step in spectrogram\n    n_mels = 128 # number of bins in spectrogram. Height of image\n    time_steps = 384 # number of time-steps. Width of image\n    # load audio. Using example from librosa\n    path = librosa.util.example_audio_file()\n    y, sr = librosa.load(path, offset=1.0, duration=10.0, sr=22050)\n    out = 'out.png'\n    # extract a fixed length window\n    start_sample = 0 # starting at beginning\n    length_samples = time_steps*hop_length\n    window = y[start_sample:start_sample+length_samples]\n\n    # convert to PNG\n    spectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)\n    print('wrote file', out)"
  },
  {
    "url": "https://stackoverflow.com/questions/61451279/how-do-setcolumnstretch-and-setrowstretch-work",
    "body": "    import random\n    import sys\n\n    from PyQt5 import QtGui, QtWidgets\n\n    if __name__ == \"__main__\":\n        app = QtWidgets.QApplication(sys.argv)\n        w = QtWidgets.QWidget()\n        glay = QtWidgets.QGridLayout(w)\n        elements = (\n            (0, 0, 1, 1),  # Position: 0x0 1 rowspan 1 colspan\n            (1, 0, 1, 1),  # Position: 1x0 1 rowspan 1 colspan\n            (0, 1, 2, 1),  # Position: 0x1 2 rowspan 1 colspan\n            (2, 0, 1, 2),  # Position: 2x0 1 rowspan 2 colspan\n        )\n        for i, (row, col, row_span, col_span) in enumerate(elements):\n            label = QtWidgets.QLabel(\"{}\".format(i))\n            color = QtGui.QColor(*random.sample(range(255), 3))\n            label.setStyleSheet(\"background-color: {}\".format(color.name()))\n            glay.addWidget(label, row, col, row_span, col_span)\n        w.resize(640, 480)\n        w.show()\n        sys.exit(app.exec_())"
  },
  {
    "url": "https://stackoverflow.com/questions/61451279/how-do-setcolumnstretch-and-setrowstretch-work",
    "body": "    import random\n    import sys\n\n    from PyQt5 import QtGui, QtWidgets\n\n    if __name__ == \"__main__\":\n        app = QtWidgets.QApplication(sys.argv)\n        w = QtWidgets.QWidget()\n        glay = QtWidgets.QGridLayout(w)\n        for i in range(3):\n            for j in range(3):\n                label = QtWidgets.QLabel(\"{}x{}\".format(i, j))\n                color = QtGui.QColor(*random.sample(range(255), 3))\n                label.setStyleSheet(\"background-color: {}\".format(color.name()))\n                glay.addWidget(label, i, j)\n        glay.setRowStretch(0, 1)\n        glay.setRowStretch(1, 2)\n        glay.setRowStretch(2, 3)\n        w.resize(640, 480)\n        w.show()\n        sys.exit(app.exec_())"
  },
  {
    "url": "https://stackoverflow.com/questions/56153726/plot-k-nearest-neighbor-graph-with-8-features",
    "body": "import pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\ndef plot_correlation(data):\n    '''\n    plot correlation's matrix to explore dependency between features\n    '''\n    # init figure size\n    rcParams['figure.figsize'] = 15, 20\n    fig = plt.figure()\n    sns.heatmap(data.corr(), annot=True, fmt=\".2f\")\n    plt.show()\n    fig.savefig('corr.png')\n# load your data\ndata  = pd.read_csv('diabetes.csv')\n# plot correlation & densities\nplot_correlation(data)"
  },
  {
    "url": "https://stackoverflow.com/questions/56153726/plot-k-nearest-neighbor-graph-with-8-features",
    "body": "import pandas as pd\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\ndef plot_densities(data):\n    '''\n    Plot features densities depending on the outcome values\n    '''\n    # change fig size to fit all subplots beautifully\n    rcParams['figure.figsize'] = 15, 20\n    # separate data based on outcome values\n    outcome_0 = data[data['Outcome'] == 0]\n    outcome_1 = data[data['Outcome'] == 1]\n\n    # init figure\n    fig, axs = plt.subplots(8, 1)\n    fig.suptitle('Features densities for different outcomes 0/1')\n    plt.subplots_adjust(left = 0.25, right = 0.9, bottom = 0.1, top = 0.95,\n                        wspace = 0.2, hspace = 0.9)\n\n    # plot densities for outcomes\n    for column_name in names[:-1]:\n        ax = axs[names.index(column_name)]\n        #plt.subplot(4, 2, names.index(column_name) + 1)\n        outcome_0[column_name].plot(kind='density', ax=ax, subplots=True,\n                                    sharex=False, color=\"red\", legend=True,\n                                    label=column_name + ' for Outcome = 0')\n        outcome_1[column_name].plot(kind='density', ax=ax, subplots=True,\n                                     sharex=False, color=\"green\", legend=True,\n                                     label=column_name + ' for Outcome = 1')\n        ax.set_xlabel(column_name + ' values')\n        ax.set_title(column_name + ' density')\n        ax.grid('on')\n    plt.show()\n    fig.savefig('densities.png')\n# load your data\ndata  = pd.read_csv('diabetes.csv')\nnames = list(data.columns)\n# plot correlation & densities\nplot_densities(data)"
  },
  {
    "url": "https://stackoverflow.com/questions/56153726/plot-k-nearest-neighbor-graph-with-8-features",
    "body": "import warnings\nimport numpy as np\nimport pandas as pd\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n# filter warnings\nwarnings.filterwarnings(\"ignore\")\ndef accuracy(k, X_train, y_train, X_test, y_test):\n    '''\n    compute accuracy of the classification based on k values\n    '''\n    # instantiate learning model and fit data\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n\n    # predict the response\n    pred = knn.predict(X_test)\n\n    # evaluate and return  accuracy\n    return accuracy_score(y_test, pred)\ndef classify_and_plot(X, y):\n    '''\n    split data, fit, classify, plot and evaluate results\n    '''\n    # split data into training and testing set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 41)\n    # init vars\n    n_neighbors = 5\n    h           = .02  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold  = ListedColormap(['#FF0000', '#0000FF'])\n\n    rcParams['figure.figsize'] = 5, 5\n    for weights in ['uniform', 'distance']:\n        # we create an instance of Neighbours Classifier and fit the data.\n        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n        clf.fit(X_train, y_train)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                             np.arange(y_min, y_max, h))\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        fig = plt.figure()\n        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n        # Plot also the training points, x-axis = 'Glucose', y-axis = \"BMI\"\n        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n        plt.xlim(xx.min(), xx.max())\n        plt.ylim(yy.min(), yy.max())\n        plt.title(\"0/1 outcome classification (k = %i, weights = '%s')\" % (n_neighbors, weights))\n        plt.show()\n        fig.savefig(weights +'.png')\n\n        # evaluate\n        y_expected  = y_test\n        y_predicted = clf.predict(X_test)\n\n        # print results\n        print('----------------------------------------------------------------------')\n        print('Classification report')\n        print('----------------------------------------------------------------------')\n        print('\\n', classification_report(y_expected, y_predicted))\n        print('----------------------------------------------------------------------')\n        print('Accuracy = %5s' % round(accuracy(n_neighbors, X_train, y_train, X_test, y_test), 3))\n        print('----------------------------------------------------------------------')\n# load your data\ndata  = pd.read_csv('diabetes.csv')\nnames = list(data.columns)\n# we only take the best two features and prepare them for the KNN classifier\nrows_nbr = 30 # data.shape[0]\nX_prime  = np.array(data.iloc[:rows_nbr, [1,5]])\nX        = X_prime # preprocessing.scale(X_prime)\ny        = np.array(data.iloc[:rows_nbr, 8])\n# classify, evaluate and plot results\nclassify_and_plot(X, y)"
  },
  {
    "url": "https://stackoverflow.com/questions/76038966/type-hinting-pandas-dataframe-content-and-columns",
    "body": "import pandas as pd\nimport pandera as pa\nfrom pandera.typing import DataFrame\nclass MySchema(pa.DataFrameModel):\n    a: int\n    b: float\n    c: str = pa.Field(nullable=True)  # For example, allow None values\n    d: float    # US dollars\nclass OtherSchema(pa.DataFrameModel):\n    year: int = pa.Field(ge=1900, le=2050)\ndef generate_data() -> DataFrame[MySchema]:\n    df = pd.DataFrame({\n        \"a\": [1, 2, 3],\n        \"b\": [10.0, 20.0, 30.0],\n        \"c\": [\"A\", \"B\", \"C\"],\n        \"d\": [0.1, 0.2, 0.3],\n    })\n    # Runtime verification here, throws on schema mismatch\n    strongly_typed_df = DataFrame[MySchema](df)\n    return strongly_typed_df\ndef transform(input: DataFrame[MySchema]) -> DataFrame[OtherSchema]:\n    # This demonstrates that you can use strongly\n    # typed column names from the schema\n    df = input.filter(items=[MySchema.a]).rename(\n            columns={MySchema.a: OtherSchema.year}\n    )\n    return DataFrame[OtherSchema](df) # This will throw on range validation!\ndf1 = generate_data()\ndf2 = transform(df1)\ntransform(df2)   # mypy prints error here - incompatible type!"
  },
  {
    "url": "https://stackoverflow.com/questions/61711710/runtimeerror-class-not-set-defining-abstractbaseuser-as-class-django-co",
    "body": "class ModelBase(type):\n    \"\"\"\n    Metaclass for all models.\n    \"\"\"\n    def __new__(cls, name, bases, attrs):\n        super_new = super(ModelBase, cls).__new__\n        # Also ensure initialization is only performed for subclasses of Model\n        # (excluding Model class itself).\n        parents = [b for b in bases if isinstance(b, ModelBase)]\n        if not parents:\n            return super_new(cls, name, bases, attrs)\n        # Create the class.\n        module = attrs.pop('__module__')\n        new_class = super_new(cls, name, bases, {'__module__': module})\n\n        # <========== THE CODE BELLOW SHOULD BE ADDED ONLY ======>>\n        new_attrs = {'__module__': module}\n        classcell = attrs.pop('__classcell__', None)\n        if classcell is not None:\n            new_attrs['__classcell__'] = classcell\n        new_class = super_new(cls, name, bases, new_attrs)\n        # <========== THE CODE ABOVE SHOULD BE ADDED ONLY ======>>\n        # the rest of the class ....."
  },
  {
    "url": "https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn",
    "body": "import matplotlib.pyplot as plt\nimport numpy as np\nmean_1 = np.array([10, 20, 30, 25, 32, 43])\nstd_1 = np.array([2.2, 2.3, 1.2, 2.2, 1.8, 3.5])\nmean_2 = np.array([12, 22, 30, 13, 33, 39])\nstd_2 = np.array([2.4, 1.3, 2.2, 1.2, 1.9, 3.5])\nx = np.arange(len(mean_1))\nplt.plot(x, mean_1, 'b-', label='mean_1')\nplt.fill_between(x, mean_1 - std_1, mean_1 + std_1, color='b', alpha=0.2)\nplt.plot(x, mean_2, 'r-', label='mean_2')\nplt.fill_between(x, mean_2 - std_2, mean_2 + std_2, color='r', alpha=0.2)\nplt.legend()\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn",
    "body": "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set()\nN = 100\nx = np.arange(N)\nmean_1 = 25 + np.random.normal(0.1, 1, N).cumsum()\nstd_1 = 3 + np.random.normal(0, .08, N).cumsum()\nmean_2 = 15 + np.random.normal(0.2, 1, N).cumsum()\nstd_2 = 4 + np.random.normal(0, .1, N).cumsum()\nplt.plot(x, mean_1, 'b-', label='mean_1')\nplt.fill_between(x, mean_1 - std_1, mean_1 + std_1, color='b', alpha=0.2)\nplt.plot(x, mean_2, 'r--', label='mean_2')\nplt.fill_between(x, mean_2 - std_2, mean_2 + std_2, color='r', alpha=0.2)\nplt.legend(title='title')\nplt.show()"
  },
  {
    "url": "https://stackoverflow.com/questions/37972029/regex-to-match-pep440-compliant-version-strings",
    "body": "VERSION_PATTERN = r\"\"\"\n    v?\n    (?:\n        (?:(?P<epoch>[0-9]+)!)?                           # epoch\n        (?P<release>[0-9]+(?:\\.[0-9]+)*)                  # release segment\n        (?P<pre>                                          # pre-release\n            [-_\\.]?\n            (?P<pre_l>(a|b|c|rc|alpha|beta|pre|preview))\n            [-_\\.]?\n            (?P<pre_n>[0-9]+)?\n        )?\n        (?P<post>                                         # post release\n            (?:-(?P<post_n1>[0-9]+))\n            |\n            (?:\n                [-_\\.]?\n                (?P<post_l>post|rev|r)\n                [-_\\.]?\n                (?P<post_n2>[0-9]+)?\n            )\n        )?\n        (?P<dev>                                          # dev release\n            [-_\\.]?\n            (?P<dev_l>dev)\n            [-_\\.]?\n            (?P<dev_n>[0-9]+)?\n        )?\n    )\n    (?:\\+(?P<local>[a-z0-9]+(?:[-_\\.][a-z0-9]+)*))?       # local version\n\"\"\""
  },
  {
    "url": "https://stackoverflow.com/questions/3044455/sqlalchemy-how-to-group-by-two-fields-and-filter-by-date",
    "body": "qry = (session.query(\n         table.c.field1,\n         table.c.field2,\n        # #strftime* for year-month works on sqlite;\n\n        # @todo: find proper function for mysql (as in the question)\n        # Also it is not clear if only MONTH part is enough, so that\n        # May-2001 and May-2009 can be joined, or YEAR-MONTH must be used\n        func.strftime('%Y-%m', table.c.datestamp),\n        func.count(),\n    )\n    # optionally check only last 2 month data (could have partial months)\n    .filter(table.c.datestamp < datetime.date.today() - datetime.timedelta(60))\n    .group_by(\n            table.c.field1,\n            table.c.field2,\n            func.strftime('%Y-%m', table.c.datestamp),\n            )\n    # comment this line out to see all the groups\n    .having(func.count()>1)\n  )"
  },
  {
    "url": "https://stackoverflow.com/questions/60054076/is-dataclass-a-good-fit-to-replace-a-dictionary",
    "body": "from typing import ClassVar\n@dataclass\nclass Lamp:\n    valid_sockets: ClassVar[set] = { 'edison_screw', 'bayonet' }\n    valid_min_wattage: ClassVar[int] = 40\n    valid_max_wattage: ClassVar[int] = 200\n    height_cm: int\n    socket: str\n    wattage: int\n\n    def __post_init__(self) -> None:\n        assert self._is_valid_wattage(), f'Lamp requires {self.valid_min_wattage}-{self.valid_max_wattage}W bulb'\n        assert self._is_valid_socket(), f'Bulb must be one of {self.valid_sockets}'\n\n    def _is_valid_socket(self) -> bool:\n        return self.socket.lower() in self.valid_sockets\n    def _is_valid_wattage(self) -> bool:\n        return (self.wattage > self.valid_min_wattage) and ( self.wattage < self.valid_max_wattage)\nIn [27]: l = Lamp(50, 'bayonet', 80)\nIn [28]: print(repr(l))\nLamp(height_cm=50, socket='bayonet', wattage=80)\nIn [29]: l = Lamp(50, 'bayonet', 300)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In [29], line 1\n----> 1 l = Lamp(50, 'bayonet', 300)\nFile <string>:6, in __init__(self, height_cm, socket, wattage)\nCell In [25], line 11, in Lamp.__post_init__(self)\n     10 def __post_init__(self) -> None:\n---> 11     assert self._is_valid_wattage(), f'Lamp requires {self.valid_min_wattage}-{self.valid_max_wattage}W bulb'\n     12     assert self._is_valid_socket(), f'Bulb must be one of {self.valid_sockets}'\nAssertionError: Lamp requires 40-200W bulb"
  },
  {
    "url": "https://stackoverflow.com/questions/58061225/read-a-parquet-bytes-object-in-python",
    "body": "import io\nimport pandas as pd\npq_bytes = b'PAR1\\x15\\x02\\x19\\x1c5\\x00\\x18\\x06schema\\x15\\x00\\x00\\x16\\x00\\x19\\x1c\\x19\\x0c\\x16\\x00\\x16\\x00&\\x00\\x16\\x00\\x14\\x00\\x00\\x19,\\x18\\x06pandas\\x18\\x8c\\x01{\"index_columns\": [], \"column_indexes\": [], \"columns\": [], \"creator\": {\"library\": \"pyarrow\", \"version\": \"1.0.1\"}, \"pandas_version\": \"1.1.3\"}\\x00\\x18\\x0cARROW:schema\\x18\\xd8\\x02//////gAAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABBAAQAAAAAAAKAAwAAAAEAAgACgAAAMQAAAAEAAAAAQAAAAwAAAAIAAwABAAIAAgAAACcAAAABAAAAIwAAAB7ImluZGV4X2NvbHVtbnMiOiBbXSwgImNvbHVtbl9pbmRleGVzIjogW10sICJjb2x1bW5zIjogW10sICJjcmVhdG9yIjogeyJsaWJyYXJ5IjogInB5YXJyb3ciLCAidmVyc2lvbiI6ICIxLjAuMSJ9LCAicGFuZGFzX3ZlcnNpb24iOiAiMS4xLjMifQAAAAAGAAAAcGFuZGFzAAAAAAAAAAAAAA==\\x00\\x18\"parquet-cpp version 1.5.1-SNAPSHOT\\x19\\x0c\\x00M\\x02\\x00\\x00PAR1'\npq_file = io.BytesIO(pq_bytes)\ndf = pd.read_parquet(pq_file)"
  },
  {
    "url": "https://stackoverflow.com/questions/58061225/read-a-parquet-bytes-object-in-python",
    "body": "import pandas as pd\ndf = pd.DataFrame()\ndf.to_parquet()\nb'PAR1\\x15\\x04\\x15\\x00\\x15\\x02L\\x15\\x00\\x15\\x04\\x12\\x00\\x00\\x00&&\\x1c\\x15\\x02\\x195\\x04\\x00\\x06\\x19\\x18\\x11__index_level_0__\\x15\\x02\\x16\\x00\\x16\\x1c\\x16\\x1e&\\x00&\\x08)\\x1c\\x15\\x04\\x15\\x04\\x15\\x02\\x00\\x00\\x00\\x15\\x02\\x19,5\\x00\\x18\\x06schema\\x15\\x02\\x00\\x15\\x02%\\x02\\x18\\x11__index_level_0__l\\xbc\\x00\\x00\\x00\\x16\\x00\\x19\\x1c\\x19\\x1c&&\\x1c\\x15\\x02\\x195\\x04\\x00\\x06\\x19\\x18\\x11__index_level_0__\\x15\\x02\\x16\\x00\\x16\\x1c\\x16\\x1e&\\x00&\\x08)\\x1c\\x15\\x04\\x15\\x04\\x15\\x02\\x00\\x00\\x00\\x16\\x1e\\x16\\x00&&\\x16\\x1e\\x14\\x00\\x00\\x19,\\x18\\x06pandas\\x18\\xf6\\x02{\"index_columns\": [\"__index_level_0__\"], \"column_indexes\": [{\"name\": null, \"field_name\": null, \"pandas_type\": \"empty\", \"numpy_type\": \"object\", \"metadata\": null}], \"columns\": [{\"name\": null, \"field_name\": \"__index_level_0__\", \"pandas_type\": \"empty\", \"numpy_type\": \"object\", \"metadata\": null}], \"creator\": {\"library\": \"pyarrow\", \"version\": \"3.0.0\"}, \"pandas_version\": \"1.2.3\"}\\x00\\x18\\x0cARROW:schema\\x18\\xec\\x05/////ygCAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABBAAQAAAAAAAKAAwAAAAEAAgACgAAAKwBAAAEAAAAAQAAAAwAAAAIAAwABAAIAAgAAACEAQAABAAAAHYBAAB7ImluZGV4X2NvbHVtbnMiOiBbIl9faW5kZXhfbGV2ZWxfMF9fIl0sICJjb2x1bW5faW5kZXhlcyI6IFt7Im5hbWUiOiBudWxsLCAiZmllbGRfbmFtZSI6IG51bGwsICJwYW5kYXNfdHlwZSI6ICJlbXB0eSIsICJudW1weV90eXBlIjogIm9iamVjdCIsICJtZXRhZGF0YSI6IG51bGx9XSwgImNvbHVtbnMiOiBbeyJuYW1lIjogbnVsbCwgImZpZWxkX25hbWUiOiAiX19pbmRleF9sZXZlbF8wX18iLCAicGFuZGFzX3R5cGUiOiAiZW1wdHkiLCAibnVtcHlfdHlwZSI6ICJvYmplY3QiLCAibWV0YWRhdGEiOiBudWxsfV0sICJjcmVhdG9yIjogeyJsaWJyYXJ5IjogInB5YXJyb3ciLCAidmVyc2lvbiI6ICIzLjAuMCJ9LCAicGFuZGFzX3ZlcnNpb24iOiAiMS4yLjMifQAABgAAAHBhbmRhcwAAAQAAABQAAAAQABQACAAGAAcADAAAABAAEAAAAAAAAQEQAAAAKAAAAAQAAAAAAAAAEQAAAF9faW5kZXhfbGV2ZWxfMF9fAAAABAAEAAQAAAA=\\x00\\x18\"parquet-cpp version 1.5.1-SNAPSHOT\\x19\\x1c\\x1c\\x00\\x00\\x00\\x1f\\x05\\x00\\x00PAR1'"
  }
]